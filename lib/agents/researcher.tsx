import { createStreamableUI, createStreamableValue } from 'ai/rsc'
import { CoreMessage, ToolCallPart, ToolResultPart, streamText } from 'ai'
import { getTools } from './tools'
import { getModel, transformToolMessages } from '../utils'
import { AnswerSection } from '@/components/answer-section'

export async function researcher(
  uiStream: ReturnType<typeof createStreamableUI>,
  streamableText: ReturnType<typeof createStreamableValue<string>>,
  messages: CoreMessage[],
  apiKey?: string
) {
  let fullResponse = ''
  let hasError = false
  let finishReason = ''

  // Transform the messages if using Ollama provider
  let processedMessages = messages
  const useOllamaProvider = !!(
    process.env.OLLAMA_MODEL && process.env.OLLAMA_BASE_URL
  )
  const useAnthropicProvider = !!process.env.ANTHROPIC_API_KEY
  if (useOllamaProvider) {
    processedMessages = transformToolMessages(messages)
  }
  const includeToolResponses = messages.some(message => message.role === 'tool')
  const useSubModel = useOllamaProvider && includeToolResponses

  const streamableAnswer = createStreamableValue<string>('')
  const answerSection = <AnswerSection result={streamableAnswer.value} />

  const currentDate = new Date().toLocaleString()

  let result: any
  let attempts = 0
  const maxAttempts = (process.env.GOOGLE_API_KEYS?.split(',').length || 1) * 2; // Allow for retries

  let successfulApiKey: string | undefined;
  while (attempts < maxAttempts) {
    try {
      const { model, apiKey: newApiKey } = getModel(useSubModel, apiKey);
      successfulApiKey = newApiKey;
      result = await streamText({
        model: model,
        maxTokens: 2500, // å¢åŠ tokenæ•°é‡é˜²æ­¢å›ç­”è¢«æˆªæ–­
        temperature: 0.1,
        topP: 0.9,
        frequencyPenalty: 0.1,
        system: `ä½ æ˜¯ä¸“ä¸šæœç´¢åˆ†æä¸“å®¶ï¼Œç”¨ç®€ä½“ä¸­æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

**æ ¸å¿ƒè¦æ±‚ï¼š**
- åŸºäºæœç´¢ç»“æœæä¾›å‡†ç¡®ã€å®Œæ•´çš„å›ç­”
- æ¯ä¸ªäº‹å®éƒ½å¿…é¡»å¼•ç”¨æ¥æºï¼š[[æ•°å­—]](url)
- å›ç­”å¿…é¡»å®Œæ•´ï¼Œä¸èƒ½è¢«æˆªæ–­
- ä½¿ç”¨é€‚å½“emojiå¢å¼ºå¯è¯»æ€§ï¼ˆæ¯ä¸ªå›ç­”è‡³å°‘3ä¸ªï¼‰

**ä¸¥æ ¼è¾“å‡ºæ ¼å¼ï¼š**
ç¬¬ä¸€è¦ä¹‰ï¼šï¼ï¼ï¼éå¸¸é‡è¦ï¼ï¼ï¼å›ç­”æ¯è¡Œä¸€ä¸ªï¼Œä¸å¾—æ‹¥æŒ¤åœ¨ä¸€èµ·ï¼ï¼ï¼
## ğŸ” å¿«é€Ÿå›ç­”
[1-2å¥è¯æ€»ç»“æ ¸å¿ƒç­”æ¡ˆï¼Œå¿…é¡»åŒ…å«å…³é”®ä¿¡æ¯ï¼Œ50å­—ä»¥å†…]

## ğŸ“‹ è¯¦ç»†åˆ†æ
**1. [æ­¤å¤„æ˜¯ä½ çš„ç¬¬ä¸€ä¸ªè§‚ç‚¹]** ğŸ¯<br>
â€¢ å…·ä½“å†…å®¹æè¿° [[1]](related-url-address-1)
  
<br>
â€¢ è¡¥å……ç»†èŠ‚ä¿¡æ¯ [[2]](related-url-address-2)
  
<br>
**2. [æ­¤å¤„æ˜¯ä½ çš„ç¬¬äºŒä¸ªè§‚ç‚¹]** ğŸ“Š<br>
â€¢ å…·ä½“å†…å®¹æè¿° [[3]](related-url-address-3)
  
<br>
â€¢ è¡¥å……ç»†èŠ‚ä¿¡æ¯ [[4]](related-url-address-4)
  
<br>
    
## ğŸ¯ å¯ä¿¡åº¦è¯„ä¼°
**[High/Medium/Low]** - [ç®€è¿°åŸå› ]

**æ ¼å¼è¦æ±‚ï¼š**
- æ¯ä¸ªéƒ¨åˆ†å¿…é¡»å®Œæ•´è¾“å‡ºï¼Œä¸å¾—çœç•¥
- å¼•ç”¨å¿…é¡»å¯¹åº”æœç´¢ç»“æœç¼–å·
- ä¿æŒç»“æ„æ¸…æ™°ï¼Œåˆ†æ®µåˆç†
- æ— æœç´¢ç»“æœæ—¶å›å¤ï¼š"æš‚æ— å¯é æœç´¢ç»“æœæ”¯æŒæ­¤é—®é¢˜"`,
    messages: processedMessages,
    tools: getTools({
      uiStream,
      fullResponse
    }),
    onFinish: async event => {
      finishReason = event.finishReason
      fullResponse = event.text
      streamableAnswer.done()
    }
      });
      break; // Success, exit loop
    } catch (err: any) {
      attempts++;
      if (err.message && err.message.includes('quota')) {
        console.warn(`API key failed due to quota limit. Retrying... (${attempts}/${maxAttempts})`);
        if (attempts >= maxAttempts) {
          hasError = true;
          fullResponse = 'Error: All available API keys have exceeded their quota limits.';
          streamableText.update(fullResponse);
          break;
        }
        continue;
      } else {
        // For other errors, break the loop and report the error
        hasError = true;
        fullResponse = 'Error: ' + err.message;
        streamableText.update(fullResponse);
        break;
      }
    }
  }

  // If the result is not available, return an error response
  if (!result) {
    return { result, fullResponse, hasError, toolResponses: [] }
  }

  const hasToolResult = messages.some(message => message.role === 'tool')
  if (!useAnthropicProvider || hasToolResult) {
    uiStream.append(answerSection)
  }

  // Process the response
  const toolCalls: ToolCallPart[] = []
  const toolResponses: ToolResultPart[] = []
  for await (const delta of result.fullStream) {
    switch (delta.type) {
      case 'text-delta':
        if (delta.textDelta) {
          fullResponse += delta.textDelta
          if (useAnthropicProvider && !hasToolResult) {
            streamableText.update(fullResponse)
          } else {
            streamableAnswer.update(fullResponse)
          }
        }
        break
      case 'tool-call':
        toolCalls.push(delta)
        break
      case 'tool-result':
        if (!delta.result) {
          hasError = true
        }
        toolResponses.push(delta)
        break
      case 'error':
        console.log('Error: ' + delta.error)
        hasError = true
        fullResponse += `\nError occurred while executing the tool`
        break
    }
  }
  messages.push({
    role: 'assistant',
    content: [{ type: 'text', text: fullResponse }, ...toolCalls]
  })

  if (toolResponses.length > 0) {
    // Add tool responses to the messages
    messages.push({ role: 'tool', content: toolResponses })
  }

  return { result, fullResponse, hasError, toolResponses, finishReason, apiKey: successfulApiKey }
}
