{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 5, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/node_modules/ai/rsc/dist/rsc-shared.mjs/proxy.js"],"sourcesContent":["import { registerClientReference } from \"react-server-dom-turbopack/server.edge\";\nexport const InternalAIProvider = registerClientReference(\n    function() { throw new Error(\"Attempted to call InternalAIProvider() from the server but InternalAIProvider is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/node_modules/ai/rsc/dist/rsc-shared.mjs\",\n    \"InternalAIProvider\",\n);\nexport const readStreamableValue = registerClientReference(\n    function() { throw new Error(\"Attempted to call readStreamableValue() from the server but readStreamableValue is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/node_modules/ai/rsc/dist/rsc-shared.mjs\",\n    \"readStreamableValue\",\n);\nexport const useAIState = registerClientReference(\n    function() { throw new Error(\"Attempted to call useAIState() from the server but useAIState is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/node_modules/ai/rsc/dist/rsc-shared.mjs\",\n    \"useAIState\",\n);\nexport const useActions = registerClientReference(\n    function() { throw new Error(\"Attempted to call useActions() from the server but useActions is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/node_modules/ai/rsc/dist/rsc-shared.mjs\",\n    \"useActions\",\n);\nexport const useStreamableValue = registerClientReference(\n    function() { throw new Error(\"Attempted to call useStreamableValue() from the server but useStreamableValue is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/node_modules/ai/rsc/dist/rsc-shared.mjs\",\n    \"useStreamableValue\",\n);\nexport const useSyncUIState = registerClientReference(\n    function() { throw new Error(\"Attempted to call useSyncUIState() from the server but useSyncUIState is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/node_modules/ai/rsc/dist/rsc-shared.mjs\",\n    \"useSyncUIState\",\n);\nexport const useUIState = registerClientReference(\n    function() { throw new Error(\"Attempted to call useUIState() from the server but useUIState is on the client. It's not possible to invoke a client function from the server, it can only be rendered as a Component or passed to props of a Client Component.\"); },\n    \"[project]/node_modules/ai/rsc/dist/rsc-shared.mjs\",\n    \"useUIState\",\n);\n"],"names":[],"mappings":";;;;;;;;;;;;AACO,MAAM,qBAAqB,CAAA,GAAA,+PAAA,CAAA,0BAAuB,AAAD,EACpD;IAAa,MAAM,IAAI,MAAM;AAAoP,GACjR,qDACA;AAEG,MAAM,sBAAsB,CAAA,GAAA,+PAAA,CAAA,0BAAuB,AAAD,EACrD;IAAa,MAAM,IAAI,MAAM;AAAsP,GACnR,qDACA;AAEG,MAAM,aAAa,CAAA,GAAA,+PAAA,CAAA,0BAAuB,AAAD,EAC5C;IAAa,MAAM,IAAI,MAAM;AAAoO,GACjQ,qDACA;AAEG,MAAM,aAAa,CAAA,GAAA,+PAAA,CAAA,0BAAuB,AAAD,EAC5C;IAAa,MAAM,IAAI,MAAM;AAAoO,GACjQ,qDACA;AAEG,MAAM,qBAAqB,CAAA,GAAA,+PAAA,CAAA,0BAAuB,AAAD,EACpD;IAAa,MAAM,IAAI,MAAM;AAAoP,GACjR,qDACA;AAEG,MAAM,iBAAiB,CAAA,GAAA,+PAAA,CAAA,0BAAuB,AAAD,EAChD;IAAa,MAAM,IAAI,MAAM;AAA4O,GACzQ,qDACA;AAEG,MAAM,aAAa,CAAA,GAAA,+PAAA,CAAA,0BAAuB,AAAD,EAC5C;IAAa,MAAM,IAAI,MAAM;AAAoO,GACjQ,qDACA"}},
    {"offset": {"line": 38, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 43, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/node_modules/ai/rsc/shared-client/streamable.tsx","/turbopack/[project]/node_modules/ai/rsc/constants.ts","/turbopack/[project]/node_modules/ai/rsc/shared-client/context.tsx","/turbopack/[project]/node_modules/ai/rsc/utils.tsx"],"sourcesContent":["import { startTransition, useLayoutEffect, useState } from 'react';\nimport { STREAMABLE_VALUE_TYPE } from '../constants';\nimport type { StreamableValue } from '../types';\n\nfunction hasReadableValueSignature(value: unknown): value is StreamableValue {\n  return !!(\n    value &&\n    typeof value === 'object' &&\n    'type' in value &&\n    value.type === STREAMABLE_VALUE_TYPE\n  );\n}\n\nfunction assertStreamableValue(\n  value: unknown,\n): asserts value is StreamableValue {\n  if (!hasReadableValueSignature(value)) {\n    throw new Error(\n      'Invalid value: this hook only accepts values created via `createStreamableValue`.',\n    );\n  }\n}\n\nfunction isStreamableValue(value: unknown): value is StreamableValue {\n  const hasSignature = hasReadableValueSignature(value);\n\n  if (!hasSignature && typeof value !== 'undefined') {\n    throw new Error(\n      'Invalid value: this hook only accepts values created via `createStreamableValue`.',\n    );\n  }\n\n  return hasSignature;\n}\n\n/**\n * `readStreamableValue` takes a streamable value created via the `createStreamableValue().value` API,\n * and returns an async iterator.\n *\n * ```js\n * // Inside your AI action:\n *\n * async function action() {\n *   'use server'\n *   const streamable = createStreamableValue();\n *\n *   streamable.update(1);\n *   streamable.update(2);\n *   streamable.done(3);\n *   // ...\n *   return streamable.value;\n * }\n * ```\n *\n * And to read the value:\n *\n * ```js\n * const streamableValue = await action()\n * for await (const v of readStreamableValue(streamableValue)) {\n *   console.log(v)\n * }\n * ```\n *\n * This logs out 1, 2, 3 on console.\n */\nexport function readStreamableValue<T = unknown>(\n  streamableValue: StreamableValue<T>,\n): AsyncIterable<T | undefined> {\n  assertStreamableValue(streamableValue);\n\n  return {\n    [Symbol.asyncIterator]() {\n      let row: StreamableValue<T> | Promise<StreamableValue<T>> =\n        streamableValue;\n      let curr = row.curr;\n      let done = false;\n      let initial = true;\n\n      return {\n        async next() {\n          if (done) return { value: curr, done: true };\n\n          row = await row;\n\n          if (typeof row.error !== 'undefined') {\n            throw row.error;\n          }\n          if ('curr' in row || row.diff) {\n            if (row.diff) {\n              switch (row.diff[0]) {\n                case 0:\n                  if (typeof curr !== 'string') {\n                    throw new Error(\n                      'Invalid patch: can only append to string types. This is a bug in the AI SDK.',\n                    );\n                  } else {\n                    (curr as string) = curr + row.diff[1];\n                  }\n                  break;\n              }\n            } else {\n              curr = row.curr;\n            }\n\n            // The last emitted { done: true } won't be used as the value\n            // by the for await...of syntax.\n            if (!row.next) {\n              done = true;\n              return {\n                value: curr,\n                done: false,\n              };\n            }\n          }\n\n          if (!row.next) {\n            return {\n              value: curr,\n              done: true,\n            };\n          }\n\n          row = row.next;\n          if (initial) {\n            initial = false;\n            if (typeof curr === 'undefined') {\n              // This is the initial chunk and there isn't an initial value yet.\n              // Let's skip this one.\n              return this.next();\n            }\n          }\n\n          return {\n            value: curr,\n            done: false,\n          };\n        },\n      };\n    },\n  };\n}\n\n/**\n * `useStreamableValue` is a React hook that takes a streamable value created via the `createStreamableValue().value` API,\n * and returns the current value, error, and pending state.\n *\n * This is useful for consuming streamable values received from a component's props. For example:\n *\n * ```js\n * function MyComponent({ streamableValue }) {\n *   const [data, error, pending] = useStreamableValue(streamableValue);\n *\n *   if (pending) return <div>Loading...</div>;\n *   if (error) return <div>Error: {error.message}</div>;\n *\n *   return <div>Data: {data}</div>;\n * }\n * ```\n */\nexport function useStreamableValue<T = unknown, Error = unknown>(\n  streamableValue?: StreamableValue<T>,\n): [data: T | undefined, error: Error | undefined, pending: boolean] {\n  const [curr, setCurr] = useState<T | undefined>(\n    isStreamableValue(streamableValue) ? streamableValue.curr : undefined,\n  );\n  const [error, setError] = useState<Error | undefined>(\n    isStreamableValue(streamableValue) ? streamableValue.error : undefined,\n  );\n  const [pending, setPending] = useState<boolean>(\n    isStreamableValue(streamableValue) ? !!streamableValue.next : false,\n  );\n\n  useLayoutEffect(() => {\n    if (!isStreamableValue(streamableValue)) return;\n\n    let cancelled = false;\n\n    const iterator = readStreamableValue(streamableValue);\n    if (streamableValue.next) {\n      startTransition(() => {\n        if (cancelled) return;\n        setPending(true);\n      });\n    }\n\n    (async () => {\n      try {\n        for await (const value of iterator) {\n          if (cancelled) return;\n          startTransition(() => {\n            if (cancelled) return;\n            setCurr(value);\n          });\n        }\n      } catch (e) {\n        if (cancelled) return;\n        startTransition(() => {\n          if (cancelled) return;\n          setError(e as Error);\n        });\n      } finally {\n        if (cancelled) return;\n        startTransition(() => {\n          if (cancelled) return;\n          setPending(false);\n        });\n      }\n    })();\n\n    return () => {\n      cancelled = true;\n    };\n  }, [streamableValue]);\n\n  return [curr, error, pending];\n}\n","export const STREAMABLE_VALUE_TYPE = Symbol.for('ui.streamable.value');\nexport const DEV_DEFAULT_STREAMABLE_WARNING_TIME = 15 * 1000;\n","/* eslint-disable react-hooks/exhaustive-deps */\n'use client';\n\nimport * as React from 'react';\n\nimport * as jsondiffpatch from 'jsondiffpatch';\nimport type {\n  InternalAIProviderProps,\n  AIProvider,\n  InferAIState,\n  ValueOrUpdater,\n  InferActions,\n  InferUIState,\n} from '../types';\nimport { isFunction } from '../utils';\n\nconst InternalUIStateProvider = React.createContext<null | any>(null);\nconst InternalAIStateProvider = React.createContext<undefined | any>(undefined);\nconst InternalActionProvider = React.createContext<null | any>(null);\nconst InternalSyncUIStateProvider = React.createContext<null | any>(null);\n\nexport function InternalAIProvider({\n  children,\n  initialUIState,\n  initialAIState,\n  initialAIStatePatch,\n  wrappedActions,\n  wrappedSyncUIState,\n}: InternalAIProviderProps) {\n  if (!('use' in React)) {\n    throw new Error('Unsupported React version.');\n  }\n\n  const uiState = React.useState(initialUIState);\n  const setUIState = uiState[1];\n\n  const resolvedInitialAIStatePatch = initialAIStatePatch\n    ? (React as any).use(initialAIStatePatch)\n    : undefined;\n  initialAIState = React.useMemo(() => {\n    if (resolvedInitialAIStatePatch) {\n      return jsondiffpatch.patch(\n        jsondiffpatch.clone(initialAIState),\n        resolvedInitialAIStatePatch,\n      );\n    }\n    return initialAIState;\n  }, [initialAIState, resolvedInitialAIStatePatch]);\n\n  const aiState = React.useState(initialAIState);\n  const setAIState = aiState[1];\n  const aiStateRef = React.useRef(aiState[0]);\n\n  React.useEffect(() => {\n    aiStateRef.current = aiState[0];\n  }, [aiState[0]]);\n\n  const clientWrappedActions = React.useMemo(\n    () =>\n      Object.fromEntries(\n        Object.entries(wrappedActions).map(([key, action]) => [\n          key,\n          async (...args: any) => {\n            const aiStateSnapshot = aiStateRef.current;\n            const [aiStateDelta, result] = await action(\n              aiStateSnapshot,\n              ...args,\n            );\n            (async () => {\n              const delta = await aiStateDelta;\n              if (delta !== undefined) {\n                aiState[1](\n                  jsondiffpatch.patch(\n                    jsondiffpatch.clone(aiStateSnapshot),\n                    delta,\n                  ),\n                );\n              }\n            })();\n            return result;\n          },\n        ]),\n      ),\n    [wrappedActions],\n  );\n\n  const clientWrappedSyncUIStateAction = React.useMemo(() => {\n    if (!wrappedSyncUIState) {\n      return () => {};\n    }\n\n    return async () => {\n      const aiStateSnapshot = aiStateRef.current;\n      const [aiStateDelta, uiState] = await wrappedSyncUIState!(\n        aiStateSnapshot,\n      );\n\n      if (uiState !== undefined) {\n        setUIState(uiState);\n      }\n\n      const delta = await aiStateDelta;\n      if (delta !== undefined) {\n        const patchedAiState = jsondiffpatch.patch(\n          jsondiffpatch.clone(aiStateSnapshot),\n          delta,\n        );\n        setAIState(patchedAiState);\n      }\n    };\n  }, [wrappedSyncUIState]);\n\n  return (\n    <InternalAIStateProvider.Provider value={aiState}>\n      <InternalUIStateProvider.Provider value={uiState}>\n        <InternalActionProvider.Provider value={clientWrappedActions}>\n          <InternalSyncUIStateProvider.Provider\n            value={clientWrappedSyncUIStateAction}\n          >\n            {children}\n          </InternalSyncUIStateProvider.Provider>\n        </InternalActionProvider.Provider>\n      </InternalUIStateProvider.Provider>\n    </InternalAIStateProvider.Provider>\n  );\n}\n\nexport function useUIState<AI extends AIProvider = any>() {\n  type T = InferUIState<AI, any>;\n\n  const state = React.useContext<\n    [T, (v: T | ((v_: T) => T)) => void] | null | undefined\n  >(InternalUIStateProvider);\n  if (state === null) {\n    throw new Error('`useUIState` must be used inside an <AI> provider.');\n  }\n  if (!Array.isArray(state)) {\n    throw new Error('Invalid state');\n  }\n  if (state[0] === undefined) {\n    throw new Error(\n      '`initialUIState` must be provided to `createAI` or `<AI>`',\n    );\n  }\n  return state;\n}\n\n// TODO: How do we avoid causing a re-render when the AI state changes but you\n// are only listening to a specific key? We need useSES perhaps?\nfunction useAIState<AI extends AIProvider = any>(): [\n  InferAIState<AI, any>,\n  (newState: ValueOrUpdater<InferAIState<AI, any>>) => void,\n];\nfunction useAIState<AI extends AIProvider = any>(\n  key: keyof InferAIState<AI, any>,\n): [\n  InferAIState<AI, any>[typeof key],\n  (newState: ValueOrUpdater<InferAIState<AI, any>[typeof key]>) => void,\n];\nfunction useAIState<AI extends AIProvider = any>(\n  ...args: [] | [keyof InferAIState<AI, any>]\n) {\n  type T = InferAIState<AI, any>;\n\n  const state = React.useContext<\n    [T, (newState: ValueOrUpdater<T>) => void] | null | undefined\n  >(InternalAIStateProvider);\n  if (state === null) {\n    throw new Error('`useAIState` must be used inside an <AI> provider.');\n  }\n  if (!Array.isArray(state)) {\n    throw new Error('Invalid state');\n  }\n  if (state[0] === undefined) {\n    throw new Error(\n      '`initialAIState` must be provided to `createAI` or `<AI>`',\n    );\n  }\n  if (args.length >= 1 && typeof state[0] !== 'object') {\n    throw new Error(\n      'When using `useAIState` with a key, the AI state must be an object.',\n    );\n  }\n\n  const key = args[0];\n  const setter = React.useCallback(\n    typeof key === 'undefined'\n      ? state[1]\n      : (newState: ValueOrUpdater<T>) => {\n          if (isFunction(newState)) {\n            return state[1](s => {\n              return { ...s, [key]: newState(s[key]) };\n            });\n          } else {\n            return state[1]({ ...state[0], [key]: newState });\n          }\n        },\n    [key],\n  );\n\n  if (args.length === 0) {\n    return state;\n  } else {\n    return [state[0][args[0]], setter];\n  }\n}\n\nexport function useActions<AI extends AIProvider = any>() {\n  type T = InferActions<AI, any>;\n\n  const actions = React.useContext<T>(InternalActionProvider);\n  return actions;\n}\n\nexport function useSyncUIState() {\n  const syncUIState = React.useContext<() => Promise<void>>(\n    InternalSyncUIStateProvider,\n  );\n\n  if (syncUIState === null) {\n    throw new Error('`useSyncUIState` must be used inside an <AI> provider.');\n  }\n\n  return syncUIState;\n}\n\nexport { useAIState };\n","import React, { Suspense } from 'react';\n\nexport function createResolvablePromise<T = any>() {\n  let resolve: (value: T) => void, reject: (error: unknown) => void;\n  const promise = new Promise<T>((res, rej) => {\n    resolve = res;\n    reject = rej;\n  });\n  return {\n    promise,\n    resolve: resolve!,\n    reject: reject!,\n  };\n}\n\n// Use the name `R` for `Row` as it will be shorter in the RSC payload.\nconst R = [\n  (async ({\n    c, // current\n    n, // next\n  }: {\n    c: React.ReactNode;\n    n: Promise<any>;\n  }) => {\n    const chunk = await n;\n    if (chunk.done) {\n      return chunk.value;\n    }\n\n    if (chunk.append) {\n      return (\n        <>\n          {c}\n          <Suspense fallback={chunk.value}>\n            <R c={chunk.value} n={chunk.next} />\n          </Suspense>\n        </>\n      );\n    }\n\n    return (\n      <Suspense fallback={chunk.value}>\n        <R c={chunk.value} n={chunk.next} />\n      </Suspense>\n    );\n  }) as unknown as React.FC<{\n    c: React.ReactNode;\n    n: Promise<any>;\n  }>,\n][0];\n\nexport function createSuspensedChunk(initialValue: React.ReactNode) {\n  const { promise, resolve, reject } = createResolvablePromise();\n\n  return {\n    row: (\n      <Suspense fallback={initialValue}>\n        <R c={initialValue} n={promise} />\n      </Suspense>\n    ) as React.ReactNode,\n    resolve,\n    reject,\n  };\n}\n\nexport const isFunction = (x: unknown): x is Function =>\n  typeof x === 'function';\n\nexport const consumeStream = async (stream: ReadableStream) => {\n  const reader = stream.getReader();\n  while (true) {\n    const { done } = await reader.read();\n    if (done) break;\n  }\n};\n"],"names":[],"mappings":""}},
    {"offset": {"line": 48, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 53, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/node_modules/ai/rsc/ai-state.tsx","/turbopack/[project]/node_modules/ai/rsc/utils.tsx","/turbopack/[project]/node_modules/ai/rsc/streamable.tsx","/turbopack/[project]/node_modules/ai/core/util/retry-with-exponential-backoff.ts","/turbopack/[project]/node_modules/ai/core/util/delay.ts","/turbopack/[project]/node_modules/ai/core/util/detect-image-mimetype.ts","/turbopack/[project]/node_modules/ai/core/prompt/data-content.ts","/turbopack/[project]/node_modules/ai/core/prompt/invalid-message-role-error.ts","/turbopack/[project]/node_modules/ai/core/prompt/convert-to-language-model-prompt.ts","/turbopack/[project]/node_modules/ai/core/prompt/get-validated-prompt.ts","/turbopack/[project]/node_modules/ai/core/prompt/prepare-call-settings.ts","/turbopack/[project]/node_modules/ai/core/types/token-usage.ts","/turbopack/[project]/node_modules/ai/core/util/convert-zod-to-json-schema.ts","/turbopack/[project]/node_modules/ai/core/util/is-non-empty-object.ts","/turbopack/[project]/node_modules/ai/core/prompt/prepare-tools-and-tool-choice.ts","/turbopack/[project]/node_modules/ai/streams/ai-stream.ts","/turbopack/[project]/node_modules/ai/streams/stream-data.ts","/turbopack/[project]/node_modules/ai/streams/openai-stream.ts","/turbopack/[project]/node_modules/ai/rsc/constants.ts","/turbopack/[project]/node_modules/ai/rsc/stream-ui/stream-ui.tsx","/turbopack/[project]/node_modules/ai/rsc/provider.tsx"],"sourcesContent":["import { AsyncLocalStorage } from 'async_hooks';\nimport * as jsondiffpatch from 'jsondiffpatch';\nimport { createResolvablePromise, isFunction } from './utils';\nimport type {\n  AIProvider,\n  InternalAIStateStorageOptions,\n  InferAIState,\n  MutableAIState,\n  ValueOrUpdater,\n} from './types';\n\n// It is possible that multiple AI requests get in concurrently, for different\n// AI instances. So ALS is necessary here for a simpler API.\nconst asyncAIStateStorage = new AsyncLocalStorage<{\n  currentState: any;\n  originalState: any;\n  sealed: boolean;\n  options: InternalAIStateStorageOptions;\n  mutationDeltaPromise?: Promise<any>;\n  mutationDeltaResolve?: (v: any) => void;\n}>();\n\nfunction getAIStateStoreOrThrow(message: string) {\n  const store = asyncAIStateStorage.getStore();\n  if (!store) {\n    throw new Error(message);\n  }\n  return store;\n}\n\nexport function withAIState<S, T>(\n  { state, options }: { state: S; options: InternalAIStateStorageOptions },\n  fn: () => T,\n): T {\n  return asyncAIStateStorage.run(\n    {\n      currentState: state,\n      originalState: state,\n      sealed: false,\n      options,\n    },\n    fn,\n  );\n}\n\nexport function getAIStateDeltaPromise() {\n  const store = getAIStateStoreOrThrow('Internal error occurred.');\n  return store.mutationDeltaPromise;\n}\n\n// Internal method. This will be called after the AI Action has been returned\n// and you can no longer call `getMutableAIState()` inside any async callbacks\n// created by that Action.\nexport function sealMutableAIState() {\n  const store = getAIStateStoreOrThrow('Internal error occurred.');\n  store.sealed = true;\n}\n\n/**\n * Get the current AI state.\n * If `key` is provided, it will return the value of the specified key in the\n * AI state, if it's an object. If it's not an object, it will throw an error.\n *\n * @example const state = getAIState() // Get the entire AI state\n * @example const field = getAIState('key') // Get the value of the key\n */\nfunction getAIState<AI extends AIProvider = any>(): Readonly<\n  InferAIState<AI, any>\n>;\nfunction getAIState<AI extends AIProvider = any>(\n  key: keyof InferAIState<AI, any>,\n): Readonly<InferAIState<AI, any>[typeof key]>;\nfunction getAIState<AI extends AIProvider = any>(\n  ...args: [] | [key: keyof InferAIState<AI, any>]\n) {\n  const store = getAIStateStoreOrThrow(\n    '`getAIState` must be called within an AI Action.',\n  );\n\n  if (args.length > 0) {\n    const key = args[0];\n    if (typeof store.currentState !== 'object') {\n      throw new Error(\n        `You can't get the \"${String(\n          key,\n        )}\" field from the AI state because it's not an object.`,\n      );\n    }\n    return store.currentState[key as keyof typeof store.currentState];\n  }\n\n  return store.currentState;\n}\n\n/**\n * Get the mutable AI state. Note that you must call `.done()` when finishing\n * updating the AI state.\n *\n * @example\n * ```tsx\n * const state = getMutableAIState()\n * state.update({ ...state.get(), key: 'value' })\n * state.update((currentState) => ({ ...currentState, key: 'value' }))\n * state.done()\n * ```\n *\n * @example\n * ```tsx\n * const state = getMutableAIState()\n * state.done({ ...state.get(), key: 'value' }) // Done with a new state\n * ```\n */\nfunction getMutableAIState<AI extends AIProvider = any>(): MutableAIState<\n  InferAIState<AI, any>\n>;\nfunction getMutableAIState<AI extends AIProvider = any>(\n  key: keyof InferAIState<AI, any>,\n): MutableAIState<InferAIState<AI, any>[typeof key]>;\nfunction getMutableAIState<AI extends AIProvider = any>(\n  ...args: [] | [key: keyof InferAIState<AI, any>]\n) {\n  type AIState = InferAIState<AI, any>;\n  type AIStateWithKey = typeof args extends [key: keyof AIState]\n    ? AIState[(typeof args)[0]]\n    : AIState;\n  type NewStateOrUpdater = ValueOrUpdater<AIStateWithKey>;\n\n  const store = getAIStateStoreOrThrow(\n    '`getMutableAIState` must be called within an AI Action.',\n  );\n\n  if (store.sealed) {\n    throw new Error(\n      \"`getMutableAIState` must be called before returning from an AI Action. Please move it to the top level of the Action's function body.\",\n    );\n  }\n\n  if (!store.mutationDeltaPromise) {\n    const { promise, resolve } = createResolvablePromise();\n    store.mutationDeltaPromise = promise;\n    store.mutationDeltaResolve = resolve;\n  }\n\n  function doUpdate(newState: NewStateOrUpdater, done: boolean) {\n    if (args.length > 0) {\n      if (typeof store.currentState !== 'object') {\n        const key = args[0];\n        throw new Error(\n          `You can't modify the \"${String(\n            key,\n          )}\" field of the AI state because it's not an object.`,\n        );\n      }\n    }\n\n    if (isFunction(newState)) {\n      if (args.length > 0) {\n        store.currentState[args[0]] = newState(store.currentState[args[0]]);\n      } else {\n        store.currentState = newState(store.currentState);\n      }\n    } else {\n      if (args.length > 0) {\n        store.currentState[args[0]] = newState;\n      } else {\n        store.currentState = newState;\n      }\n    }\n\n    store.options.onSetAIState?.({\n      key: args.length > 0 ? args[0] : undefined,\n      state: store.currentState,\n      done,\n    });\n  }\n\n  const mutableState = {\n    get: () => {\n      if (args.length > 0) {\n        const key = args[0];\n        if (typeof store.currentState !== 'object') {\n          throw new Error(\n            `You can't get the \"${String(\n              key,\n            )}\" field from the AI state because it's not an object.`,\n          );\n        }\n        return store.currentState[key] as Readonly<AIStateWithKey>;\n      }\n\n      return store.currentState as Readonly<AIState>;\n    },\n    update: function update(newAIState: NewStateOrUpdater) {\n      doUpdate(newAIState, false);\n    },\n    done: function done(...doneArgs: [] | [NewStateOrUpdater]) {\n      if (doneArgs.length > 0) {\n        doUpdate(doneArgs[0] as NewStateOrUpdater, true);\n      }\n\n      const delta = jsondiffpatch.diff(store.originalState, store.currentState);\n      store.mutationDeltaResolve!(delta);\n    },\n  };\n\n  return mutableState;\n}\n\nexport { getAIState, getMutableAIState };\n","import React, { Suspense } from 'react';\n\nexport function createResolvablePromise<T = any>() {\n  let resolve: (value: T) => void, reject: (error: unknown) => void;\n  const promise = new Promise<T>((res, rej) => {\n    resolve = res;\n    reject = rej;\n  });\n  return {\n    promise,\n    resolve: resolve!,\n    reject: reject!,\n  };\n}\n\n// Use the name `R` for `Row` as it will be shorter in the RSC payload.\nconst R = [\n  (async ({\n    c, // current\n    n, // next\n  }: {\n    c: React.ReactNode;\n    n: Promise<any>;\n  }) => {\n    const chunk = await n;\n    if (chunk.done) {\n      return chunk.value;\n    }\n\n    if (chunk.append) {\n      return (\n        <>\n          {c}\n          <Suspense fallback={chunk.value}>\n            <R c={chunk.value} n={chunk.next} />\n          </Suspense>\n        </>\n      );\n    }\n\n    return (\n      <Suspense fallback={chunk.value}>\n        <R c={chunk.value} n={chunk.next} />\n      </Suspense>\n    );\n  }) as unknown as React.FC<{\n    c: React.ReactNode;\n    n: Promise<any>;\n  }>,\n][0];\n\nexport function createSuspensedChunk(initialValue: React.ReactNode) {\n  const { promise, resolve, reject } = createResolvablePromise();\n\n  return {\n    row: (\n      <Suspense fallback={initialValue}>\n        <R c={initialValue} n={promise} />\n      </Suspense>\n    ) as React.ReactNode,\n    resolve,\n    reject,\n  };\n}\n\nexport const isFunction = (x: unknown): x is Function =>\n  typeof x === 'function';\n\nexport const consumeStream = async (stream: ReadableStream) => {\n  const reader = stream.getReader();\n  while (true) {\n    const { done } = await reader.read();\n    if (done) break;\n  }\n};\n","import type { ReactNode } from 'react';\nimport type OpenAI from 'openai';\nimport { z } from 'zod';\nimport zodToJsonSchema from 'zod-to-json-schema';\n\n// TODO: This needs to be externalized.\nimport { OpenAIStream } from '../streams';\n\nimport {\n  STREAMABLE_VALUE_TYPE,\n  DEV_DEFAULT_STREAMABLE_WARNING_TIME,\n} from './constants';\nimport {\n  createResolvablePromise,\n  createSuspensedChunk,\n  consumeStream,\n} from './utils';\nimport type { StreamablePatch, StreamableValue } from './types';\n\n// It's necessary to define the type manually here, otherwise TypeScript compiler\n// will not be able to infer the correct return type as it's circular.\ntype StreamableUIWrapper = {\n  /**\n   * The value of the streamable UI. This can be returned from a Server Action and received by the client.\n   */\n  readonly value: React.ReactNode;\n\n  /**\n   * This method updates the current UI node. It takes a new UI node and replaces the old one.\n   */\n  update(value: React.ReactNode): StreamableUIWrapper;\n\n  /**\n   * This method is used to append a new UI node to the end of the old one.\n   * Once appended a new UI node, the previous UI node cannot be updated anymore.\n   *\n   * @example\n   * ```jsx\n   * const ui = createStreamableUI(<div>hello</div>)\n   * ui.append(<div>world</div>)\n   *\n   * // The UI node will be:\n   * // <>\n   * //   <div>hello</div>\n   * //   <div>world</div>\n   * // </>\n   * ```\n   */\n  append(value: React.ReactNode): StreamableUIWrapper;\n\n  /**\n   * This method is used to signal that there is an error in the UI stream.\n   * It will be thrown on the client side and caught by the nearest error boundary component.\n   */\n  error(error: any): StreamableUIWrapper;\n\n  /**\n   * This method marks the UI node as finalized. You can either call it without any parameters or with a new UI node as the final state.\n   * Once called, the UI node cannot be updated or appended anymore.\n   *\n   * This method is always **required** to be called, otherwise the response will be stuck in a loading state.\n   */\n  done(...args: [React.ReactNode] | []): StreamableUIWrapper;\n};\n\n/**\n * Create a piece of changable UI that can be streamed to the client.\n * On the client side, it can be rendered as a normal React node.\n */\nfunction createStreamableUI(initialValue?: React.ReactNode) {\n  let currentValue = initialValue;\n  let closed = false;\n  let { row, resolve, reject } = createSuspensedChunk(initialValue);\n\n  function assertStream(method: string) {\n    if (closed) {\n      throw new Error(method + ': UI stream is already closed.');\n    }\n  }\n\n  let warningTimeout: NodeJS.Timeout | undefined;\n  function warnUnclosedStream() {\n    if (process.env.NODE_ENV === 'development') {\n      if (warningTimeout) {\n        clearTimeout(warningTimeout);\n      }\n      warningTimeout = setTimeout(() => {\n        console.warn(\n          'The streamable UI has been slow to update. This may be a bug or a performance issue or you forgot to call `.done()`.',\n        );\n      }, DEV_DEFAULT_STREAMABLE_WARNING_TIME);\n    }\n  }\n  warnUnclosedStream();\n\n  const streamable: StreamableUIWrapper = {\n    value: row,\n    update(value: React.ReactNode) {\n      assertStream('.update()');\n\n      // There is no need to update the value if it's referentially equal.\n      if (value === currentValue) {\n        warnUnclosedStream();\n        return streamable;\n      }\n\n      const resolvable = createResolvablePromise();\n      currentValue = value;\n\n      resolve({ value: currentValue, done: false, next: resolvable.promise });\n      resolve = resolvable.resolve;\n      reject = resolvable.reject;\n\n      warnUnclosedStream();\n\n      return streamable;\n    },\n    append(value: React.ReactNode) {\n      assertStream('.append()');\n\n      const resolvable = createResolvablePromise();\n      currentValue = value;\n\n      resolve({ value, done: false, append: true, next: resolvable.promise });\n      resolve = resolvable.resolve;\n      reject = resolvable.reject;\n\n      warnUnclosedStream();\n\n      return streamable;\n    },\n    error(error: any) {\n      assertStream('.error()');\n\n      if (warningTimeout) {\n        clearTimeout(warningTimeout);\n      }\n      closed = true;\n      reject(error);\n\n      return streamable;\n    },\n    done(...args: [] | [React.ReactNode]) {\n      assertStream('.done()');\n\n      if (warningTimeout) {\n        clearTimeout(warningTimeout);\n      }\n      closed = true;\n      if (args.length) {\n        resolve({ value: args[0], done: true });\n        return streamable;\n      }\n      resolve({ value: currentValue, done: true });\n\n      return streamable;\n    },\n  };\n\n  return streamable;\n}\n\nconst STREAMABLE_VALUE_INTERNAL_LOCK = Symbol('streamable.value.lock');\n\n/**\n * Create a wrapped, changable value that can be streamed to the client.\n * On the client side, the value can be accessed via the readStreamableValue() API.\n */\nfunction createStreamableValue<T = any, E = any>(\n  initialValue?: T | ReadableStream<T>,\n) {\n  const isReadableStream =\n    initialValue instanceof ReadableStream ||\n    (typeof initialValue === 'object' &&\n      initialValue !== null &&\n      'getReader' in initialValue &&\n      typeof initialValue.getReader === 'function' &&\n      'locked' in initialValue &&\n      typeof initialValue.locked === 'boolean');\n\n  if (!isReadableStream) {\n    return createStreamableValueImpl<T, E>(initialValue);\n  }\n\n  const streamableValue = createStreamableValueImpl<T, E>();\n\n  // Since the streamable value will be from a readable stream, it's not allowed\n  // to update the value manually as that introduces race conditions and\n  // unexpected behavior.\n  // We lock the value to prevent any updates from the user.\n  streamableValue[STREAMABLE_VALUE_INTERNAL_LOCK] = true;\n\n  (async () => {\n    try {\n      // Consume the readable stream and update the value.\n      const reader = initialValue.getReader();\n\n      while (true) {\n        const { value, done } = await reader.read();\n        if (done) {\n          break;\n        }\n\n        // Unlock the value to allow updates.\n        streamableValue[STREAMABLE_VALUE_INTERNAL_LOCK] = false;\n        if (typeof value === 'string') {\n          streamableValue.append(value);\n        } else {\n          streamableValue.update(value);\n        }\n        // Lock the value again.\n        streamableValue[STREAMABLE_VALUE_INTERNAL_LOCK] = true;\n      }\n\n      streamableValue[STREAMABLE_VALUE_INTERNAL_LOCK] = false;\n      streamableValue.done();\n    } catch (e) {\n      streamableValue[STREAMABLE_VALUE_INTERNAL_LOCK] = false;\n      streamableValue.error(e);\n    }\n  })();\n\n  return streamableValue;\n}\n\n// It's necessary to define the type manually here, otherwise TypeScript compiler\n// will not be able to infer the correct return type as it's circular.\ntype StreamableValueWrapper<T, E> = {\n  /**\n   * The value of the streamable. This can be returned from a Server Action and\n   * received by the client. To read the streamed values, use the\n   * `readStreamableValue` or `useStreamableValue` APIs.\n   */\n  readonly value: StreamableValue<T, E>;\n\n  /**\n   * This method updates the current value with a new one.\n   */\n  update(value: T): StreamableValueWrapper<T, E>;\n\n  /**\n   * This method is used to append a delta string to the current value. It\n   * requires the current value of the streamable to be a string.\n   *\n   * @example\n   * ```jsx\n   * const streamable = createStreamableValue('hello');\n   * streamable.append(' world');\n   *\n   * // The value will be 'hello world'\n   * ```\n   */\n  append(value: T): StreamableValueWrapper<T, E>;\n\n  /**\n   * This method is used to signal that there is an error in the value stream.\n   * It will be thrown on the client side when consumed via\n   * `readStreamableValue` or `useStreamableValue`.\n   */\n  error(error: any): StreamableValueWrapper<T, E>;\n\n  /**\n   * This method marks the value as finalized. You can either call it without\n   * any parameters or with a new value as the final state.\n   * Once called, the value cannot be updated or appended anymore.\n   *\n   * This method is always **required** to be called, otherwise the response\n   * will be stuck in a loading state.\n   */\n  done(...args: [T] | []): StreamableValueWrapper<T, E>;\n\n  /**\n   * @internal This is an internal lock to prevent the value from being\n   * updated by the user.\n   */\n  [STREAMABLE_VALUE_INTERNAL_LOCK]: boolean;\n};\n\nfunction createStreamableValueImpl<T = any, E = any>(initialValue?: T) {\n  let closed = false;\n  let locked = false;\n  let resolvable = createResolvablePromise<StreamableValue<T, E>>();\n\n  let currentValue = initialValue;\n  let currentError: E | undefined;\n  let currentPromise: typeof resolvable.promise | undefined =\n    resolvable.promise;\n  let currentPatchValue: StreamablePatch;\n\n  function assertStream(method: string) {\n    if (closed) {\n      throw new Error(method + ': Value stream is already closed.');\n    }\n    if (locked) {\n      throw new Error(\n        method + ': Value stream is locked and cannot be updated.',\n      );\n    }\n  }\n\n  let warningTimeout: NodeJS.Timeout | undefined;\n  function warnUnclosedStream() {\n    if (process.env.NODE_ENV === 'development') {\n      if (warningTimeout) {\n        clearTimeout(warningTimeout);\n      }\n      warningTimeout = setTimeout(() => {\n        console.warn(\n          'The streamable value has been slow to update. This may be a bug or a performance issue or you forgot to call `.done()`.',\n        );\n      }, DEV_DEFAULT_STREAMABLE_WARNING_TIME);\n    }\n  }\n  warnUnclosedStream();\n\n  function createWrapped(initialChunk?: boolean): StreamableValue<T, E> {\n    // This makes the payload much smaller if there're mutative updates before the first read.\n    let init: Partial<StreamableValue<T, E>>;\n\n    if (currentError !== undefined) {\n      init = { error: currentError };\n    } else {\n      if (currentPatchValue && !initialChunk) {\n        init = { diff: currentPatchValue };\n      } else {\n        init = { curr: currentValue };\n      }\n    }\n\n    if (currentPromise) {\n      init.next = currentPromise;\n    }\n\n    if (initialChunk) {\n      init.type = STREAMABLE_VALUE_TYPE;\n    }\n\n    return init;\n  }\n\n  // Update the internal `currentValue` and `currentPatchValue` if needed.\n  function updateValueStates(value: T) {\n    // If we can only send a patch over the wire, it's better to do so.\n    currentPatchValue = undefined;\n    if (typeof value === 'string') {\n      if (typeof currentValue === 'string') {\n        if (value.startsWith(currentValue)) {\n          currentPatchValue = [0, value.slice(currentValue.length)];\n        }\n      }\n    }\n\n    currentValue = value;\n  }\n\n  const streamable: StreamableValueWrapper<T, E> = {\n    set [STREAMABLE_VALUE_INTERNAL_LOCK](state: boolean) {\n      locked = state;\n    },\n    get value() {\n      return createWrapped(true);\n    },\n    update(value: T) {\n      assertStream('.update()');\n\n      const resolvePrevious = resolvable.resolve;\n      resolvable = createResolvablePromise();\n\n      updateValueStates(value);\n      currentPromise = resolvable.promise;\n      resolvePrevious(createWrapped());\n\n      warnUnclosedStream();\n\n      return streamable;\n    },\n    append(value: T) {\n      assertStream('.append()');\n\n      if (\n        typeof currentValue !== 'string' &&\n        typeof currentValue !== 'undefined'\n      ) {\n        throw new Error(\n          `.append(): The current value is not a string. Received: ${typeof currentValue}`,\n        );\n      }\n      if (typeof value !== 'string') {\n        throw new Error(\n          `.append(): The value is not a string. Received: ${typeof value}`,\n        );\n      }\n\n      const resolvePrevious = resolvable.resolve;\n      resolvable = createResolvablePromise();\n\n      if (typeof currentValue === 'string') {\n        currentPatchValue = [0, value];\n        (currentValue as string) = currentValue + value;\n      } else {\n        currentPatchValue = undefined;\n        currentValue = value;\n      }\n\n      currentPromise = resolvable.promise;\n      resolvePrevious(createWrapped());\n\n      warnUnclosedStream();\n\n      return streamable;\n    },\n    error(error: any) {\n      assertStream('.error()');\n\n      if (warningTimeout) {\n        clearTimeout(warningTimeout);\n      }\n      closed = true;\n      currentError = error;\n      currentPromise = undefined;\n\n      resolvable.resolve({ error });\n\n      return streamable;\n    },\n    done(...args: [] | [T]) {\n      assertStream('.done()');\n\n      if (warningTimeout) {\n        clearTimeout(warningTimeout);\n      }\n      closed = true;\n      currentPromise = undefined;\n\n      if (args.length) {\n        updateValueStates(args[0]);\n        resolvable.resolve(createWrapped());\n        return streamable;\n      }\n\n      resolvable.resolve({});\n\n      return streamable;\n    },\n  };\n\n  return streamable;\n}\n\nexport { createStreamableUI, createStreamableValue };\n\ntype Streamable = ReactNode | Promise<ReactNode>;\ntype Renderer<T> = (\n  props: T,\n) =>\n  | Streamable\n  | Generator<Streamable, Streamable, void>\n  | AsyncGenerator<Streamable, Streamable, void>;\n\n/**\n * `render` is a helper function to create a streamable UI from some LLMs.\n * This API only supports OpenAI's GPT models with Function Calling and Assistants Tools,\n * please use `streamUI` for compatibility with other providers.\n *\n * @deprecated It's recommended to use the `streamUI` API for compatibility with AI SDK Core APIs\n * and future features. This API will be removed in a future release.\n */\nexport function render<\n  TS extends {\n    [name: string]: z.Schema;\n  } = {},\n  FS extends {\n    [name: string]: z.Schema;\n  } = {},\n>(options: {\n  /**\n   * The model name to use. Must be OpenAI SDK compatible. Tools and Functions are only supported\n   * GPT models (3.5/4), OpenAI Assistants, Mistral small and large, and Fireworks firefunction-v1.\n   *\n   * @example \"gpt-3.5-turbo\"\n   */\n  model: string;\n  /**\n   * The provider instance to use. Currently the only provider available is OpenAI.\n   * This needs to match the model name.\n   */\n  provider: OpenAI;\n  messages: Parameters<\n    typeof OpenAI.prototype.chat.completions.create\n  >[0]['messages'];\n  text?: Renderer<{\n    /**\n     * The full text content from the model so far.\n     */\n    content: string;\n    /**\n     * The new appended text content from the model since the last `text` call.\n     */\n    delta: string;\n    /**\n     * Whether the model is done generating text.\n     * If `true`, the `content` will be the final output and this call will be the last.\n     */\n    done: boolean;\n  }>;\n  tools?: {\n    [name in keyof TS]: {\n      description?: string;\n      parameters: TS[name];\n      render: Renderer<z.infer<TS[name]>>;\n    };\n  };\n  functions?: {\n    [name in keyof FS]: {\n      description?: string;\n      parameters: FS[name];\n      render: Renderer<z.infer<FS[name]>>;\n    };\n  };\n  initial?: ReactNode;\n  temperature?: number;\n}): ReactNode {\n  const ui = createStreamableUI(options.initial);\n\n  // The default text renderer just returns the content as string.\n  const text = options.text\n    ? options.text\n    : ({ content }: { content: string }) => content;\n\n  const functions = options.functions\n    ? Object.entries(options.functions).map(\n        ([name, { description, parameters }]) => {\n          return {\n            name,\n            description,\n            parameters: zodToJsonSchema(parameters) as Record<string, unknown>,\n          };\n        },\n      )\n    : undefined;\n\n  const tools = options.tools\n    ? Object.entries(options.tools).map(\n        ([name, { description, parameters }]) => {\n          return {\n            type: 'function' as const,\n            function: {\n              name,\n              description,\n              parameters: zodToJsonSchema(parameters) as Record<\n                string,\n                unknown\n              >,\n            },\n          };\n        },\n      )\n    : undefined;\n\n  if (functions && tools) {\n    throw new Error(\n      \"You can't have both functions and tools defined. Please choose one or the other.\",\n    );\n  }\n\n  let finished: Promise<void> | undefined;\n\n  async function handleRender(\n    args: any,\n    renderer: undefined | Renderer<any>,\n    res: ReturnType<typeof createStreamableUI>,\n  ) {\n    if (!renderer) return;\n\n    const resolvable = createResolvablePromise<void>();\n\n    if (finished) {\n      finished = finished.then(() => resolvable.promise);\n    } else {\n      finished = resolvable.promise;\n    }\n\n    const value = renderer(args);\n    if (\n      value instanceof Promise ||\n      (value &&\n        typeof value === 'object' &&\n        'then' in value &&\n        typeof value.then === 'function')\n    ) {\n      const node = await (value as Promise<React.ReactNode>);\n      res.update(node);\n      resolvable.resolve(void 0);\n    } else if (\n      value &&\n      typeof value === 'object' &&\n      Symbol.asyncIterator in value\n    ) {\n      const it = value as AsyncGenerator<\n        React.ReactNode,\n        React.ReactNode,\n        void\n      >;\n      while (true) {\n        const { done, value } = await it.next();\n        res.update(value);\n        if (done) break;\n      }\n      resolvable.resolve(void 0);\n    } else if (value && typeof value === 'object' && Symbol.iterator in value) {\n      const it = value as Generator<React.ReactNode, React.ReactNode, void>;\n      while (true) {\n        const { done, value } = it.next();\n        res.update(value);\n        if (done) break;\n      }\n      resolvable.resolve(void 0);\n    } else {\n      res.update(value);\n      resolvable.resolve(void 0);\n    }\n  }\n\n  (async () => {\n    let hasFunction = false;\n    let content = '';\n\n    consumeStream(\n      OpenAIStream(\n        (await options.provider.chat.completions.create({\n          model: options.model,\n          messages: options.messages,\n          temperature: options.temperature,\n          stream: true,\n          ...(functions\n            ? {\n                functions,\n              }\n            : {}),\n          ...(tools\n            ? {\n                tools,\n              }\n            : {}),\n        })) as any,\n        {\n          ...(functions\n            ? {\n                async experimental_onFunctionCall(functionCallPayload) {\n                  hasFunction = true;\n                  handleRender(\n                    functionCallPayload.arguments,\n                    options.functions?.[functionCallPayload.name as any]\n                      ?.render,\n                    ui,\n                  );\n                },\n              }\n            : {}),\n          ...(tools\n            ? {\n                async experimental_onToolCall(toolCallPayload: any) {\n                  hasFunction = true;\n\n                  // TODO: We might need Promise.all here?\n                  for (const tool of toolCallPayload.tools) {\n                    handleRender(\n                      tool.func.arguments,\n                      options.tools?.[tool.func.name as any]?.render,\n                      ui,\n                    );\n                  }\n                },\n              }\n            : {}),\n          onText(chunk) {\n            content += chunk;\n            handleRender({ content, done: false, delta: chunk }, text, ui);\n          },\n          async onFinal() {\n            if (hasFunction) {\n              await finished;\n              ui.done();\n              return;\n            }\n\n            handleRender({ content, done: true }, text, ui);\n            await finished;\n            ui.done();\n          },\n        },\n      ),\n    );\n  })();\n\n  return ui.value;\n}\n","import { APICallError, RetryError } from '@ai-sdk/provider';\nimport { getErrorMessage, isAbortError } from '@ai-sdk/provider-utils';\nimport { delay } from './delay';\n\nexport type RetryFunction = <OUTPUT>(\n  fn: () => PromiseLike<OUTPUT>,\n) => PromiseLike<OUTPUT>;\n\n/**\nThe `retryWithExponentialBackoff` strategy retries a failed API call with an exponential backoff.\nYou can configure the maximum number of retries, the initial delay, and the backoff factor.\n */\nexport const retryWithExponentialBackoff =\n  ({\n    maxRetries = 2,\n    initialDelayInMs = 2000,\n    backoffFactor = 2,\n  } = {}): RetryFunction =>\n  async <OUTPUT>(f: () => PromiseLike<OUTPUT>) =>\n    _retryWithExponentialBackoff(f, {\n      maxRetries,\n      delayInMs: initialDelayInMs,\n      backoffFactor,\n    });\n\nasync function _retryWithExponentialBackoff<OUTPUT>(\n  f: () => PromiseLike<OUTPUT>,\n  {\n    maxRetries,\n    delayInMs,\n    backoffFactor,\n  }: { maxRetries: number; delayInMs: number; backoffFactor: number },\n  errors: unknown[] = [],\n): Promise<OUTPUT> {\n  try {\n    return await f();\n  } catch (error) {\n    if (isAbortError(error)) {\n      throw error; // don't retry when the request was aborted\n    }\n\n    if (maxRetries === 0) {\n      throw error; // don't wrap the error when retries are disabled\n    }\n\n    const errorMessage = getErrorMessage(error);\n    const newErrors = [...errors, error];\n    const tryNumber = newErrors.length;\n\n    if (tryNumber > maxRetries) {\n      throw new RetryError({\n        message: `Failed after ${tryNumber} attempts. Last error: ${errorMessage}`,\n        reason: 'maxRetriesExceeded',\n        errors: newErrors,\n      });\n    }\n\n    if (\n      error instanceof Error &&\n      APICallError.isAPICallError(error) &&\n      error.isRetryable === true &&\n      tryNumber <= maxRetries\n    ) {\n      await delay(delayInMs);\n      return _retryWithExponentialBackoff(\n        f,\n        { maxRetries, delayInMs: backoffFactor * delayInMs, backoffFactor },\n        newErrors,\n      );\n    }\n\n    if (tryNumber === 1) {\n      throw error; // don't wrap the error when a non-retryable error occurs on the first try\n    }\n\n    throw new RetryError({\n      message: `Failed after ${tryNumber} attempts with non-retryable error: '${errorMessage}'`,\n      reason: 'errorNotRetryable',\n      errors: newErrors,\n    });\n  }\n}\n","export async function delay(delayInMs: number): Promise<void> {\n  return new Promise(resolve => setTimeout(resolve, delayInMs));\n}\n","const mimeTypeSignatures = [\n  { mimeType: 'image/gif' as const, bytes: [0x47, 0x49, 0x46] },\n  { mimeType: 'image/png' as const, bytes: [0x89, 0x50, 0x4e, 0x47] },\n  { mimeType: 'image/jpeg' as const, bytes: [0xff, 0xd8] },\n  { mimeType: 'image/webp' as const, bytes: [0x52, 0x49, 0x46, 0x46] },\n];\n\nexport function detectImageMimeType(\n  image: Uint8Array,\n): 'image/jpeg' | 'image/png' | 'image/gif' | 'image/webp' | undefined {\n  for (const { bytes, mimeType } of mimeTypeSignatures) {\n    if (\n      image.length >= bytes.length &&\n      bytes.every((byte, index) => image[index] === byte)\n    ) {\n      return mimeType;\n    }\n  }\n\n  return undefined;\n}\n","import { InvalidDataContentError } from '@ai-sdk/provider';\nimport {\n  convertBase64ToUint8Array,\n  convertUint8ArrayToBase64,\n} from '@ai-sdk/provider-utils';\n\n/**\nData content. Can either be a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer.\n */\nexport type DataContent = string | Uint8Array | ArrayBuffer | Buffer;\n\n/**\nConverts data content to a base64-encoded string.\n\n@param content - Data content to convert.\n@returns Base64-encoded string.\n*/\nexport function convertDataContentToBase64String(content: DataContent): string {\n  if (typeof content === 'string') {\n    return content;\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return convertUint8ArrayToBase64(new Uint8Array(content));\n  }\n\n  return convertUint8ArrayToBase64(content);\n}\n\n/**\nConverts data content to a Uint8Array.\n\n@param content - Data content to convert.\n@returns Uint8Array.\n */\nexport function convertDataContentToUint8Array(\n  content: DataContent,\n): Uint8Array {\n  if (content instanceof Uint8Array) {\n    return content;\n  }\n\n  if (typeof content === 'string') {\n    try {\n      return convertBase64ToUint8Array(content);\n    } catch (error) {\n      throw new InvalidDataContentError({\n        message:\n          'Invalid data content. Content string is not a base64-encoded image.',\n        content,\n        cause: error,\n      });\n    }\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return new Uint8Array(content);\n  }\n\n  throw new InvalidDataContentError({ content });\n}\n","export class InvalidMessageRoleError extends Error {\n  readonly role: string;\n\n  constructor({\n    role,\n    message = `Invalid message role: '${role}'. Must be one of: \"system\", \"user\", \"assistant\", \"tool\".`,\n  }: {\n    role: string;\n    message?: string;\n  }) {\n    super(message);\n\n    this.name = 'AI_InvalidMessageRoleError';\n\n    this.role = role;\n  }\n\n  static isInvalidMessageRoleError(\n    error: unknown,\n  ): error is InvalidMessageRoleError {\n    return (\n      error instanceof Error &&\n      error.name === 'AI_InvalidMessageRoleError' &&\n      typeof (error as InvalidMessageRoleError).role === 'string'\n    );\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      role: this.role,\n    };\n  }\n}\n","import {\n  LanguageModelV1ImagePart,\n  LanguageModelV1Message,\n  LanguageModelV1Prompt,\n  LanguageModelV1TextPart,\n} from '@ai-sdk/provider';\nimport { CoreMessage } from '../prompt/message';\nimport { detectImageMimeType } from '../util/detect-image-mimetype';\nimport { convertDataContentToUint8Array } from './data-content';\nimport { ValidatedPrompt } from './get-validated-prompt';\nimport { InvalidMessageRoleError } from './invalid-message-role-error';\nimport { getErrorMessage } from '@ai-sdk/provider-utils';\n\nexport function convertToLanguageModelPrompt(\n  prompt: ValidatedPrompt,\n): LanguageModelV1Prompt {\n  const languageModelMessages: LanguageModelV1Prompt = [];\n\n  if (prompt.system != null) {\n    languageModelMessages.push({ role: 'system', content: prompt.system });\n  }\n\n  const promptType = prompt.type;\n  switch (promptType) {\n    case 'prompt': {\n      languageModelMessages.push({\n        role: 'user',\n        content: [{ type: 'text', text: prompt.prompt }],\n      });\n      break;\n    }\n\n    case 'messages': {\n      languageModelMessages.push(\n        ...prompt.messages.map(convertToLanguageModelMessage),\n      );\n      break;\n    }\n\n    default: {\n      const _exhaustiveCheck: never = promptType;\n      throw new Error(`Unsupported prompt type: ${_exhaustiveCheck}`);\n    }\n  }\n\n  return languageModelMessages;\n}\n\nexport function convertToLanguageModelMessage(\n  message: CoreMessage,\n): LanguageModelV1Message {\n  const role = message.role;\n  switch (role) {\n    case 'system': {\n      return { role: 'system', content: message.content };\n    }\n\n    case 'user': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'user',\n          content: [{ type: 'text', text: message.content }],\n        };\n      }\n\n      return {\n        role: 'user',\n        content: message.content.map(\n          (part): LanguageModelV1TextPart | LanguageModelV1ImagePart => {\n            switch (part.type) {\n              case 'text': {\n                return part;\n              }\n\n              case 'image': {\n                if (part.image instanceof URL) {\n                  return {\n                    type: 'image',\n                    image: part.image,\n                    mimeType: part.mimeType,\n                  };\n                }\n\n                // try to convert string image parts to urls\n                if (typeof part.image === 'string') {\n                  try {\n                    const url = new URL(part.image);\n\n                    switch (url.protocol) {\n                      case 'http:':\n                      case 'https:': {\n                        return {\n                          type: 'image',\n                          image: url,\n                          mimeType: part.mimeType,\n                        };\n                      }\n                      case 'data:': {\n                        try {\n                          const [header, base64Content] = part.image.split(',');\n                          const mimeType = header.split(';')[0].split(':')[1];\n\n                          if (mimeType == null || base64Content == null) {\n                            throw new Error('Invalid data URL format');\n                          }\n\n                          return {\n                            type: 'image',\n                            image:\n                              convertDataContentToUint8Array(base64Content),\n                            mimeType,\n                          };\n                        } catch (error) {\n                          throw new Error(\n                            `Error processing data URL: ${getErrorMessage(\n                              message,\n                            )}`,\n                          );\n                        }\n                      }\n                      default: {\n                        throw new Error(\n                          `Unsupported URL protocol: ${url.protocol}`,\n                        );\n                      }\n                    }\n                  } catch (_ignored) {\n                    // not a URL\n                  }\n                }\n\n                const imageUint8 = convertDataContentToUint8Array(part.image);\n\n                return {\n                  type: 'image',\n                  image: imageUint8,\n                  mimeType: part.mimeType ?? detectImageMimeType(imageUint8),\n                };\n              }\n            }\n          },\n        ),\n      };\n    }\n\n    case 'assistant': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'assistant',\n          content: [{ type: 'text', text: message.content }],\n        };\n      }\n\n      return {\n        role: 'assistant',\n        content: message.content.filter(\n          // remove empty text parts:\n          part => part.type !== 'text' || part.text !== '',\n        ),\n      };\n    }\n\n    case 'tool': {\n      return message;\n    }\n\n    default: {\n      const _exhaustiveCheck: never = role;\n      throw new InvalidMessageRoleError({ role: _exhaustiveCheck });\n    }\n  }\n}\n","import { InvalidPromptError } from '@ai-sdk/provider';\nimport { CoreMessage } from './message';\nimport { Prompt } from './prompt';\n\nexport type ValidatedPrompt =\n  | {\n      type: 'prompt';\n      prompt: string;\n      messages: undefined;\n      system?: string;\n    }\n  | {\n      type: 'messages';\n      prompt: undefined;\n      messages: CoreMessage[];\n      system?: string;\n    };\n\nexport function getValidatedPrompt(prompt: Prompt): ValidatedPrompt {\n  if (prompt.prompt == null && prompt.messages == null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt or messages must be defined',\n    });\n  }\n\n  if (prompt.prompt != null && prompt.messages != null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt and messages cannot be defined at the same time',\n    });\n  }\n\n  if (prompt.messages != null) {\n    for (const message of prompt.messages) {\n      if (message.role === 'system' && typeof message.content !== 'string') {\n        throw new InvalidPromptError({\n          prompt,\n          message: 'system message content must be a string',\n        });\n      }\n    }\n  }\n\n  return prompt.prompt != null\n    ? {\n        type: 'prompt',\n        prompt: prompt.prompt,\n        messages: undefined,\n        system: prompt.system,\n      }\n    : {\n        type: 'messages',\n        prompt: undefined,\n        messages: prompt.messages!, // only possible case bc of checks above\n        system: prompt.system,\n      };\n}\n","import { InvalidArgumentError } from '@ai-sdk/provider';\nimport { CallSettings } from './call-settings';\n\n/**\n * Validates call settings and sets default values.\n */\nexport function prepareCallSettings({\n  maxTokens,\n  temperature,\n  topP,\n  presencePenalty,\n  frequencyPenalty,\n  seed,\n  maxRetries,\n}: CallSettings): CallSettings {\n  if (maxTokens != null) {\n    if (!Number.isInteger(maxTokens)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxTokens',\n        value: maxTokens,\n        message: 'maxTokens must be an integer',\n      });\n    }\n\n    if (maxTokens < 1) {\n      throw new InvalidArgumentError({\n        parameter: 'maxTokens',\n        value: maxTokens,\n        message: 'maxTokens must be >= 1',\n      });\n    }\n  }\n\n  if (temperature != null) {\n    if (typeof temperature !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'temperature',\n        value: temperature,\n        message: 'temperature must be a number',\n      });\n    }\n  }\n\n  if (topP != null) {\n    if (typeof topP !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'topP',\n        value: topP,\n        message: 'topP must be a number',\n      });\n    }\n  }\n\n  if (presencePenalty != null) {\n    if (typeof presencePenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'presencePenalty',\n        value: presencePenalty,\n        message: 'presencePenalty must be a number',\n      });\n    }\n  }\n\n  if (frequencyPenalty != null) {\n    if (typeof frequencyPenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'frequencyPenalty',\n        value: frequencyPenalty,\n        message: 'frequencyPenalty must be a number',\n      });\n    }\n  }\n\n  if (seed != null) {\n    if (!Number.isInteger(seed)) {\n      throw new InvalidArgumentError({\n        parameter: 'seed',\n        value: seed,\n        message: 'seed must be an integer',\n      });\n    }\n  }\n\n  if (maxRetries != null) {\n    if (!Number.isInteger(maxRetries)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be an integer',\n      });\n    }\n\n    if (maxRetries < 0) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be >= 0',\n      });\n    }\n  }\n\n  return {\n    maxTokens,\n    temperature: temperature ?? 0,\n    topP,\n    presencePenalty,\n    frequencyPenalty,\n    seed,\n    maxRetries: maxRetries ?? 2,\n  };\n}\n","/**\nRepresents the number of tokens used in a prompt and completion.\n */\nexport type CompletionTokenUsage = {\n  /**\nThe number of tokens used in the prompt\n   */\n  promptTokens: number;\n\n  /**\nThe number of tokens used in the completion.\n */\n  completionTokens: number;\n\n  /**\nThe total number of tokens used (promptTokens + completionTokens).\n   */\n  totalTokens: number;\n};\n\n/**\nRepresents the number of tokens used in an embedding.\n */\nexport type EmbeddingTokenUsage = {\n  /**\nThe number of tokens used in the embedding.\n   */\n  tokens: number;\n};\n\nexport function calculateCompletionTokenUsage(usage: {\n  promptTokens: number;\n  completionTokens: number;\n}): CompletionTokenUsage {\n  return {\n    promptTokens: usage.promptTokens,\n    completionTokens: usage.completionTokens,\n    totalTokens: usage.promptTokens + usage.completionTokens,\n  };\n}\n","import { JSONSchema7 } from 'json-schema';\nimport * as z from 'zod';\nimport zodToJsonSchema from 'zod-to-json-schema';\n\nexport function convertZodToJSONSchema(\n  zodSchema: z.Schema<unknown>,\n): JSONSchema7 {\n  // we assume that zodToJsonSchema will return a valid JSONSchema7\n  return zodToJsonSchema(zodSchema) as JSONSchema7;\n}\n","export function isNonEmptyObject(\n  object: Record<string, unknown> | undefined | null,\n): object is Record<string, unknown> {\n  return object != null && Object.keys(object).length > 0;\n}\n","import {\n  LanguageModelV1FunctionTool,\n  LanguageModelV1ToolChoice,\n} from '@ai-sdk/provider';\nimport { CoreTool } from '../tool/tool';\nimport { CoreToolChoice } from '../types/language-model';\nimport { convertZodToJSONSchema } from '../util/convert-zod-to-json-schema';\nimport { isNonEmptyObject } from '../util/is-non-empty-object';\n\nexport function prepareToolsAndToolChoice<\n  TOOLS extends Record<string, CoreTool>,\n>({\n  tools,\n  toolChoice,\n}: {\n  tools: TOOLS | undefined;\n  toolChoice: CoreToolChoice<TOOLS> | undefined;\n}): {\n  tools: LanguageModelV1FunctionTool[] | undefined;\n  toolChoice: LanguageModelV1ToolChoice | undefined;\n} {\n  if (!isNonEmptyObject(tools)) {\n    return {\n      tools: undefined,\n      toolChoice: undefined,\n    };\n  }\n\n  return {\n    tools: Object.entries(tools).map(([name, tool]) => ({\n      type: 'function' as const,\n      name,\n      description: tool.description,\n      parameters: convertZodToJSONSchema(tool.parameters),\n    })),\n    toolChoice:\n      toolChoice == null\n        ? { type: 'auto' }\n        : typeof toolChoice === 'string'\n        ? { type: toolChoice }\n        : { type: 'tool' as const, toolName: toolChoice.toolName as string },\n  };\n}\n","import {\n  createParser,\n  type EventSourceParser,\n  type ParsedEvent,\n  type ReconnectInterval,\n} from 'eventsource-parser';\nimport { OpenAIStreamCallbacks } from './openai-stream';\n\nexport interface FunctionCallPayload {\n  name: string;\n  arguments: Record<string, unknown>;\n}\nexport interface ToolCallPayload {\n  tools: {\n    id: string;\n    type: 'function';\n    func: {\n      name: string;\n      arguments: Record<string, unknown>;\n    };\n  }[];\n}\n\n/**\n * Configuration options and helper callback methods for AIStream stream lifecycle events.\n * @interface\n */\nexport interface AIStreamCallbacksAndOptions {\n  /** `onStart`: Called once when the stream is initialized. */\n  onStart?: () => Promise<void> | void;\n  /** `onCompletion`: Called for each tokenized message. */\n  onCompletion?: (completion: string) => Promise<void> | void;\n  /** `onFinal`: Called once when the stream is closed with the final completion message. */\n  onFinal?: (completion: string) => Promise<void> | void;\n  /** `onToken`: Called for each tokenized message. */\n  onToken?: (token: string) => Promise<void> | void;\n  /** `onText`: Called for each text chunk. */\n  onText?: (text: string) => Promise<void> | void;\n  /**\n   * @deprecated This flag is no longer used and only retained for backwards compatibility.\n   * You can remove it from your code.\n   */\n  experimental_streamData?: boolean;\n}\n\n/**\n * Options for the AIStreamParser.\n * @interface\n * @property {string} event - The event (type) from the server side event stream.\n */\nexport interface AIStreamParserOptions {\n  event?: string;\n}\n\n/**\n * Custom parser for AIStream data.\n * @interface\n * @param {string} data - The data to be parsed.\n * @param {AIStreamParserOptions} options - The options for the parser.\n * @returns {string | void} The parsed data or void.\n */\nexport interface AIStreamParser {\n  (data: string, options: AIStreamParserOptions):\n    | string\n    | void\n    | { isText: false; content: string };\n}\n\n/**\n * Creates a TransformStream that parses events from an EventSource stream using a custom parser.\n * @param {AIStreamParser} customParser - Function to handle event data.\n * @returns {TransformStream<Uint8Array, string>} TransformStream parsing events.\n */\nexport function createEventStreamTransformer(\n  customParser?: AIStreamParser,\n): TransformStream<Uint8Array, string | { isText: false; content: string }> {\n  const textDecoder = new TextDecoder();\n  let eventSourceParser: EventSourceParser;\n\n  return new TransformStream({\n    async start(controller): Promise<void> {\n      eventSourceParser = createParser(\n        (event: ParsedEvent | ReconnectInterval) => {\n          if (\n            ('data' in event &&\n              event.type === 'event' &&\n              event.data === '[DONE]') ||\n            // Replicate doesn't send [DONE] but does send a 'done' event\n            // @see https://replicate.com/docs/streaming\n            (event as any).event === 'done'\n          ) {\n            controller.terminate();\n            return;\n          }\n\n          if ('data' in event) {\n            const parsedMessage = customParser\n              ? customParser(event.data, {\n                  event: event.event,\n                })\n              : event.data;\n            if (parsedMessage) controller.enqueue(parsedMessage);\n          }\n        },\n      );\n    },\n\n    transform(chunk) {\n      eventSourceParser.feed(textDecoder.decode(chunk));\n    },\n  });\n}\n\n/**\n * Creates a transform stream that encodes input messages and invokes optional callback functions.\n * The transform stream uses the provided callbacks to execute custom logic at different stages of the stream's lifecycle.\n * - `onStart`: Called once when the stream is initialized.\n * - `onToken`: Called for each tokenized message.\n * - `onCompletion`: Called every time an AIStream completion message is received. This can occur multiple times when using e.g. OpenAI functions\n * - `onFinal`: Called once when the stream is closed with the final completion message.\n *\n * This function is useful when you want to process a stream of messages and perform specific actions during the stream's lifecycle.\n *\n * @param {AIStreamCallbacksAndOptions} [callbacks] - An object containing the callback functions.\n * @return {TransformStream<string, Uint8Array>} A transform stream that encodes input messages as Uint8Array and allows the execution of custom logic through callbacks.\n *\n * @example\n * const callbacks = {\n *   onStart: async () => console.log('Stream started'),\n *   onToken: async (token) => console.log(`Token: ${token}`),\n *   onCompletion: async (completion) => console.log(`Completion: ${completion}`)\n *   onFinal: async () => data.close()\n * };\n * const transformer = createCallbacksTransformer(callbacks);\n */\nexport function createCallbacksTransformer(\n  cb: AIStreamCallbacksAndOptions | OpenAIStreamCallbacks | undefined,\n): TransformStream<string | { isText: false; content: string }, Uint8Array> {\n  const textEncoder = new TextEncoder();\n  let aggregatedResponse = '';\n  const callbacks = cb || {};\n\n  return new TransformStream({\n    async start(): Promise<void> {\n      if (callbacks.onStart) await callbacks.onStart();\n    },\n\n    async transform(message, controller): Promise<void> {\n      const content = typeof message === 'string' ? message : message.content;\n\n      controller.enqueue(textEncoder.encode(content));\n\n      aggregatedResponse += content;\n\n      if (callbacks.onToken) await callbacks.onToken(content);\n      if (callbacks.onText && typeof message === 'string') {\n        await callbacks.onText(message);\n      }\n    },\n\n    async flush(): Promise<void> {\n      const isOpenAICallbacks = isOfTypeOpenAIStreamCallbacks(callbacks);\n      // If it's OpenAICallbacks, it has an experimental_onFunctionCall which means that the createFunctionCallTransformer\n      // will handle calling onComplete.\n      if (callbacks.onCompletion) {\n        await callbacks.onCompletion(aggregatedResponse);\n      }\n\n      if (callbacks.onFinal && !isOpenAICallbacks) {\n        await callbacks.onFinal(aggregatedResponse);\n      }\n    },\n  });\n}\n\nfunction isOfTypeOpenAIStreamCallbacks(\n  callbacks: AIStreamCallbacksAndOptions | OpenAIStreamCallbacks,\n): callbacks is OpenAIStreamCallbacks {\n  return 'experimental_onFunctionCall' in callbacks;\n}\n/**\n * Returns a stateful function that, when invoked, trims leading whitespace\n * from the input text. The trimming only occurs on the first invocation, ensuring that\n * subsequent calls do not alter the input text. This is particularly useful in scenarios\n * where a text stream is being processed and only the initial whitespace should be removed.\n *\n * @return {function(string): string} A function that takes a string as input and returns a string\n * with leading whitespace removed if it is the first invocation; otherwise, it returns the input unchanged.\n *\n * @example\n * const trimStart = trimStartOfStreamHelper();\n * const output1 = trimStart(\"   text\"); // \"text\"\n * const output2 = trimStart(\"   text\"); // \"   text\"\n *\n */\nexport function trimStartOfStreamHelper(): (text: string) => string {\n  let isStreamStart = true;\n\n  return (text: string): string => {\n    if (isStreamStart) {\n      text = text.trimStart();\n      if (text) isStreamStart = false;\n    }\n    return text;\n  };\n}\n\n/**\n * Returns a ReadableStream created from the response, parsed and handled with custom logic.\n * The stream goes through two transformation stages, first parsing the events and then\n * invoking the provided callbacks.\n *\n * For 2xx HTTP responses:\n * - The function continues with standard stream processing.\n *\n * For non-2xx HTTP responses:\n * - If the response body is defined, it asynchronously extracts and decodes the response body.\n * - It then creates a custom ReadableStream to propagate a detailed error message.\n *\n * @param {Response} response - The response.\n * @param {AIStreamParser} customParser - The custom parser function.\n * @param {AIStreamCallbacksAndOptions} callbacks - The callbacks.\n * @return {ReadableStream} The AIStream.\n * @throws Will throw an error if the response is not OK.\n */\nexport function AIStream(\n  response: Response,\n  customParser?: AIStreamParser,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream<Uint8Array> {\n  if (!response.ok) {\n    if (response.body) {\n      const reader = response.body.getReader();\n      return new ReadableStream({\n        async start(controller) {\n          const { done, value } = await reader.read();\n          if (!done) {\n            const errorText = new TextDecoder().decode(value);\n            controller.error(new Error(`Response error: ${errorText}`));\n          }\n        },\n      });\n    } else {\n      return new ReadableStream({\n        start(controller) {\n          controller.error(new Error('Response error: No response body'));\n        },\n      });\n    }\n  }\n\n  const responseBodyStream = response.body || createEmptyReadableStream();\n\n  return responseBodyStream\n    .pipeThrough(createEventStreamTransformer(customParser))\n    .pipeThrough(createCallbacksTransformer(callbacks));\n}\n\n// outputs lines like\n// 0: chunk\n// 0: more chunk\n// 1: a fct call\n// z: added data from Data\n\n/**\n * Creates an empty ReadableStream that immediately closes upon creation.\n * This function is used as a fallback for creating a ReadableStream when the response body is null or undefined,\n * ensuring that the subsequent pipeline processing doesn't fail due to a lack of a stream.\n *\n * @returns {ReadableStream} An empty and closed ReadableStream instance.\n */\nfunction createEmptyReadableStream(): ReadableStream {\n  return new ReadableStream({\n    start(controller) {\n      controller.close();\n    },\n  });\n}\n\n/**\n * Implements ReadableStream.from(asyncIterable), which isn't documented in MDN and isn't implemented in node.\n * https://github.com/whatwg/streams/commit/8d7a0bf26eb2cc23e884ddbaac7c1da4b91cf2bc\n */\nexport function readableFromAsyncIterable<T>(iterable: AsyncIterable<T>) {\n  let it = iterable[Symbol.asyncIterator]();\n  return new ReadableStream<T>({\n    async pull(controller) {\n      const { done, value } = await it.next();\n      if (done) controller.close();\n      else controller.enqueue(value);\n    },\n\n    async cancel(reason) {\n      await it.return?.(reason);\n    },\n  });\n}\n","import { JSONValue, formatStreamPart } from '@ai-sdk/ui-utils';\n\n/**\n * A stream wrapper to send custom JSON-encoded data back to the client.\n */\nexport class StreamData {\n  private encoder = new TextEncoder();\n\n  private controller: ReadableStreamController<Uint8Array> | null = null;\n  public stream: ReadableStream<Uint8Array>;\n\n  private isClosed: boolean = false;\n  private warningTimeout: NodeJS.Timeout | null = null;\n\n  constructor() {\n    const self = this;\n\n    this.stream = new ReadableStream({\n      start: async controller => {\n        self.controller = controller;\n\n        // Set a timeout to show a warning if the stream is not closed within 3 seconds\n        if (process.env.NODE_ENV === 'development') {\n          self.warningTimeout = setTimeout(() => {\n            console.warn(\n              'The data stream is hanging. Did you forget to close it with `data.close()`?',\n            );\n          }, 3000);\n        }\n      },\n      pull: controller => {\n        // No-op: we don't need to do anything special on pull\n      },\n      cancel: reason => {\n        this.isClosed = true;\n      },\n    });\n  }\n\n  async close(): Promise<void> {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.close();\n    this.isClosed = true;\n\n    // Clear the warning timeout if the stream is closed\n    if (this.warningTimeout) {\n      clearTimeout(this.warningTimeout);\n    }\n  }\n\n  append(value: JSONValue): void {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.enqueue(\n      this.encoder.encode(formatStreamPart('data', [value])),\n    );\n  }\n\n  appendMessageAnnotation(value: JSONValue): void {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.enqueue(\n      this.encoder.encode(formatStreamPart('message_annotations', [value])),\n    );\n  }\n}\n\n/**\n * A TransformStream for LLMs that do not have their own transform stream handlers managing encoding (e.g. OpenAIStream has one for function call handling).\n * This assumes every chunk is a 'text' chunk.\n */\nexport function createStreamDataTransformer() {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n  return new TransformStream({\n    transform: async (chunk, controller) => {\n      const message = decoder.decode(chunk);\n      controller.enqueue(encoder.encode(formatStreamPart('text', message)));\n    },\n  });\n}\n\n/**\n@deprecated Use `StreamData` instead.\n */\nexport class experimental_StreamData extends StreamData {}\n","import {\n  CreateMessage,\n  FunctionCall,\n  JSONValue,\n  ToolCall,\n  createChunkDecoder,\n  formatStreamPart,\n} from '@ai-sdk/ui-utils';\nimport {\n  AIStream,\n  FunctionCallPayload,\n  ToolCallPayload,\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n  trimStartOfStreamHelper,\n  type AIStreamCallbacksAndOptions,\n} from './ai-stream';\nimport { AzureChatCompletions } from './azure-openai-types';\nimport { createStreamDataTransformer } from './stream-data';\n\nexport type OpenAIStreamCallbacks = AIStreamCallbacksAndOptions & {\n  /**\n   * @example\n   * ```js\n   * const response = await openai.chat.completions.create({\n   *   model: 'gpt-3.5-turbo-0613',\n   *   stream: true,\n   *   messages,\n   *   functions,\n   * })\n   *\n   * const stream = OpenAIStream(response, {\n   *   experimental_onFunctionCall: async (functionCallPayload, createFunctionCallMessages) => {\n   *     // ... run your custom logic here\n   *     const result = await myFunction(functionCallPayload)\n   *\n   *     // Ask for another completion, or return a string to send to the client as an assistant message.\n   *     return await openai.chat.completions.create({\n   *       model: 'gpt-3.5-turbo-0613',\n   *       stream: true,\n   *       // Append the relevant \"assistant\" and \"function\" call messages\n   *       messages: [...messages, ...createFunctionCallMessages(result)],\n   *       functions,\n   *     })\n   *   }\n   * })\n   * ```\n   */\n  experimental_onFunctionCall?: (\n    functionCallPayload: FunctionCallPayload,\n    createFunctionCallMessages: (\n      functionCallResult: JSONValue,\n    ) => CreateMessage[],\n  ) => Promise<\n    Response | undefined | void | string | AsyncIterableOpenAIStreamReturnTypes\n  >;\n  /**\n   * @example\n   * ```js\n   * const response = await openai.chat.completions.create({\n   *   model: 'gpt-3.5-turbo-1106', // or gpt-4-1106-preview\n   *   stream: true,\n   *   messages,\n   *   tools,\n   *   tool_choice: \"auto\", // auto is default, but we'll be explicit\n   * })\n   *\n   * const stream = OpenAIStream(response, {\n   *   experimental_onToolCall: async (toolCallPayload, appendToolCallMessages) => {\n   *    let messages: CreateMessage[] = []\n   *    //   There might be multiple tool calls, so we need to iterate through them\n   *    for (const tool of toolCallPayload.tools) {\n   *     // ... run your custom logic here\n   *     const result = await myFunction(tool.function)\n   *    // Append the relevant \"assistant\" and \"tool\" call messages\n   *     appendToolCallMessage({tool_call_id:tool.id, function_name:tool.function.name, tool_call_result:result})\n   *    }\n   *     // Ask for another completion, or return a string to send to the client as an assistant message.\n   *     return await openai.chat.completions.create({\n   *       model: 'gpt-3.5-turbo-1106', // or gpt-4-1106-preview\n   *       stream: true,\n   *       // Append the results messages, calling appendToolCallMessage without\n   *       // any arguments will jsut return the accumulated messages\n   *       messages: [...messages, ...appendToolCallMessage()],\n   *       tools,\n   *        tool_choice: \"auto\", // auto is default, but we'll be explicit\n   *     })\n   *   }\n   * })\n   * ```\n   */\n  experimental_onToolCall?: (\n    toolCallPayload: ToolCallPayload,\n    appendToolCallMessage: (result?: {\n      tool_call_id: string;\n      function_name: string;\n      tool_call_result: JSONValue;\n    }) => CreateMessage[],\n  ) => Promise<\n    Response | undefined | void | string | AsyncIterableOpenAIStreamReturnTypes\n  >;\n};\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L28-L40\ninterface ChatCompletionChunk {\n  id: string;\n  choices: Array<ChatCompletionChunkChoice>;\n  created: number;\n  model: string;\n  object: string;\n}\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L43-L49\n// Updated for https://github.com/openai/openai-node/commit/f10c757d831d90407ba47b4659d9cd34b1a35b1d\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ChatCompletionChunkChoice {\n  delta: ChoiceDelta;\n  finish_reason:\n    | 'stop'\n    | 'length'\n    | 'tool_calls'\n    | 'content_filter'\n    | 'function_call'\n    | null;\n  index: number;\n}\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L123-L139\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ChoiceDelta {\n  /**\n   * The contents of the chunk message.\n   */\n  content?: string | null;\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  function_call?: FunctionCall;\n\n  /**\n   * The role of the author of this message.\n   */\n  role?: 'system' | 'user' | 'assistant' | 'tool';\n\n  tool_calls?: Array<DeltaToolCall>;\n}\n\n// From https://github.com/openai/openai-node/blob/master/src/resources/chat/completions.ts\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface DeltaToolCall {\n  index: number;\n\n  /**\n   * The ID of the tool call.\n   */\n  id?: string;\n\n  /**\n   * The function that the model called.\n   */\n  function?: ToolCallFunction;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type?: 'function';\n}\n\n// From https://github.com/openai/openai-node/blob/master/src/resources/chat/completions.ts\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ToolCallFunction {\n  /**\n   * The arguments to call the function with, as generated by the model in JSON\n   * format. Note that the model does not always generate valid JSON, and may\n   * hallucinate parameters not defined by your function schema. Validate the\n   * arguments in your code before calling your function.\n   */\n  arguments?: string;\n\n  /**\n   * The name of the function to call.\n   */\n  name?: string;\n}\n\n/**\n * https://github.com/openai/openai-node/blob/3ec43ee790a2eb6a0ccdd5f25faa23251b0f9b8e/src/resources/completions.ts#L28C1-L64C1\n * Completions API. Streamed and non-streamed responses are the same.\n */\ninterface Completion {\n  /**\n   * A unique identifier for the completion.\n   */\n  id: string;\n\n  /**\n   * The list of completion choices the model generated for the input prompt.\n   */\n  choices: Array<CompletionChoice>;\n\n  /**\n   * The Unix timestamp of when the completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"text_completion\"\n   */\n  object: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionUsage;\n}\n\ninterface CompletionChoice {\n  /**\n   * The reason the model stopped generating tokens. This will be `stop` if the model\n   * hit a natural stop point or a provided stop sequence, or `length` if the maximum\n   * number of tokens specified in the request was reached.\n   */\n  finish_reason: 'stop' | 'length' | 'content_filter';\n\n  index: number;\n\n  // edited: Removed CompletionChoice.logProbs and replaced with any\n  logprobs: any | null;\n\n  text: string;\n}\n\nexport interface CompletionUsage {\n  /**\n   * Usage statistics for the completion request.\n   */\n\n  /**\n   * Number of tokens in the generated completion.\n   */\n  completion_tokens: number;\n\n  /**\n   * Number of tokens in the prompt.\n   */\n  prompt_tokens: number;\n\n  /**\n   * Total number of tokens used in the request (prompt + completion).\n   */\n  total_tokens: number;\n}\n\n/**\n * Creates a parser function for processing the OpenAI stream data.\n * The parser extracts and trims text content from the JSON data. This parser\n * can handle data for chat or completion models.\n *\n * @return {(data: string) => string | void| { isText: false; content: string }}\n * A parser function that takes a JSON string as input and returns the extracted text content,\n * a complex object with isText: false for function/tool calls, or nothing.\n */\nfunction parseOpenAIStream(): (\n  data: string,\n) => string | void | { isText: false; content: string } {\n  const extract = chunkToText();\n  return data => extract(JSON.parse(data) as OpenAIStreamReturnTypes);\n}\n\n/**\n * Reads chunks from OpenAI's new Streamable interface, which is essentially\n * the same as the old Response body interface with an included SSE parser\n * doing the parsing for us.\n */\nasync function* streamable(stream: AsyncIterableOpenAIStreamReturnTypes) {\n  const extract = chunkToText();\n\n  for await (let chunk of stream) {\n    // convert chunk if it is an Azure chat completion. Azure does not expose all\n    // properties in the interfaces, and also uses camelCase instead of snake_case\n    if ('promptFilterResults' in chunk) {\n      chunk = {\n        id: chunk.id,\n        created: chunk.created.getDate(),\n        object: (chunk as any).object, // not exposed by Azure API\n        model: (chunk as any).model, // not exposed by Azure API\n        choices: chunk.choices.map(choice => ({\n          delta: {\n            content: choice.delta?.content,\n            function_call: choice.delta?.functionCall,\n            role: choice.delta?.role as any,\n            tool_calls: choice.delta?.toolCalls?.length\n              ? choice.delta?.toolCalls?.map((toolCall, index) => ({\n                  index,\n                  id: toolCall.id,\n                  function: toolCall.function,\n                  type: toolCall.type,\n                }))\n              : undefined,\n          },\n          finish_reason: choice.finishReason as any,\n          index: choice.index,\n        })),\n      } satisfies ChatCompletionChunk;\n    }\n\n    const text = extract(chunk);\n\n    if (text) yield text;\n  }\n}\n\nfunction chunkToText(): (\n  chunk: OpenAIStreamReturnTypes,\n) => string | { isText: false; content: string } | void {\n  const trimStartOfStream = trimStartOfStreamHelper();\n  let isFunctionStreamingIn: boolean;\n  return json => {\n    if (isChatCompletionChunk(json)) {\n      const delta = json.choices[0]?.delta;\n      if (delta.function_call?.name) {\n        isFunctionStreamingIn = true;\n        return {\n          isText: false,\n          content: `{\"function_call\": {\"name\": \"${delta.function_call.name}\", \"arguments\": \"`,\n        };\n      } else if (delta.tool_calls?.[0]?.function?.name) {\n        isFunctionStreamingIn = true;\n        const toolCall = delta.tool_calls[0];\n        if (toolCall.index === 0) {\n          return {\n            isText: false,\n            content: `{\"tool_calls\":[ {\"id\": \"${toolCall.id}\", \"type\": \"function\", \"function\": {\"name\": \"${toolCall.function?.name}\", \"arguments\": \"`,\n          };\n        } else {\n          return {\n            isText: false,\n            content: `\"}}, {\"id\": \"${toolCall.id}\", \"type\": \"function\", \"function\": {\"name\": \"${toolCall.function?.name}\", \"arguments\": \"`,\n          };\n        }\n      } else if (delta.function_call?.arguments) {\n        return {\n          isText: false,\n          content: cleanupArguments(delta.function_call?.arguments),\n        };\n      } else if (delta.tool_calls?.[0]?.function?.arguments) {\n        return {\n          isText: false,\n          content: cleanupArguments(delta.tool_calls?.[0]?.function?.arguments),\n        };\n      } else if (\n        isFunctionStreamingIn &&\n        (json.choices[0]?.finish_reason === 'function_call' ||\n          json.choices[0]?.finish_reason === 'stop')\n      ) {\n        isFunctionStreamingIn = false; // Reset the flag\n        return {\n          isText: false,\n          content: '\"}}',\n        };\n      } else if (\n        isFunctionStreamingIn &&\n        json.choices[0]?.finish_reason === 'tool_calls'\n      ) {\n        isFunctionStreamingIn = false; // Reset the flag\n        return {\n          isText: false,\n          content: '\"}}]}',\n        };\n      }\n    }\n\n    const text = trimStartOfStream(\n      isChatCompletionChunk(json) && json.choices[0].delta.content\n        ? json.choices[0].delta.content\n        : isCompletion(json)\n        ? json.choices[0].text\n        : '',\n    );\n\n    return text;\n  };\n\n  function cleanupArguments(argumentChunk: string) {\n    let escapedPartialJson = argumentChunk\n      .replace(/\\\\/g, '\\\\\\\\') // Replace backslashes first to prevent double escaping\n      .replace(/\\//g, '\\\\/') // Escape slashes\n      .replace(/\"/g, '\\\\\"') // Escape double quotes\n      .replace(/\\n/g, '\\\\n') // Escape new lines\n      .replace(/\\r/g, '\\\\r') // Escape carriage returns\n      .replace(/\\t/g, '\\\\t') // Escape tabs\n      .replace(/\\f/g, '\\\\f'); // Escape form feeds\n\n    return `${escapedPartialJson}`;\n  }\n}\n\nconst __internal__OpenAIFnMessagesSymbol = Symbol(\n  'internal_openai_fn_messages',\n);\n\ntype AsyncIterableOpenAIStreamReturnTypes =\n  | AsyncIterable<ChatCompletionChunk>\n  | AsyncIterable<Completion>\n  | AsyncIterable<AzureChatCompletions>;\n\ntype ExtractType<T> = T extends AsyncIterable<infer U> ? U : never;\n\ntype OpenAIStreamReturnTypes =\n  ExtractType<AsyncIterableOpenAIStreamReturnTypes>;\n\nfunction isChatCompletionChunk(\n  data: OpenAIStreamReturnTypes,\n): data is ChatCompletionChunk {\n  return (\n    'choices' in data &&\n    data.choices &&\n    data.choices[0] &&\n    'delta' in data.choices[0]\n  );\n}\n\nfunction isCompletion(data: OpenAIStreamReturnTypes): data is Completion {\n  return (\n    'choices' in data &&\n    data.choices &&\n    data.choices[0] &&\n    'text' in data.choices[0]\n  );\n}\n\n/**\n * @deprecated Use the [OpenAI provider](https://sdk.vercel.ai/providers/ai-sdk-providers/openai) instead.\n */\nexport function OpenAIStream(\n  res: Response | AsyncIterableOpenAIStreamReturnTypes,\n  callbacks?: OpenAIStreamCallbacks,\n): ReadableStream {\n  // Annotate the internal `messages` property for recursive function calls\n  const cb:\n    | undefined\n    | (OpenAIStreamCallbacks & {\n        [__internal__OpenAIFnMessagesSymbol]?: CreateMessage[];\n      }) = callbacks;\n\n  let stream: ReadableStream<Uint8Array>;\n  if (Symbol.asyncIterator in res) {\n    stream = readableFromAsyncIterable(streamable(res)).pipeThrough(\n      createCallbacksTransformer(\n        cb?.experimental_onFunctionCall || cb?.experimental_onToolCall\n          ? {\n              ...cb,\n              onFinal: undefined,\n            }\n          : {\n              ...cb,\n            },\n      ),\n    );\n  } else {\n    stream = AIStream(\n      res,\n      parseOpenAIStream(),\n      cb?.experimental_onFunctionCall || cb?.experimental_onToolCall\n        ? {\n            ...cb,\n            onFinal: undefined,\n          }\n        : {\n            ...cb,\n          },\n    );\n  }\n\n  if (cb && (cb.experimental_onFunctionCall || cb.experimental_onToolCall)) {\n    const functionCallTransformer = createFunctionCallTransformer(cb);\n    return stream.pipeThrough(functionCallTransformer);\n  } else {\n    return stream.pipeThrough(createStreamDataTransformer());\n  }\n}\n\nfunction createFunctionCallTransformer(\n  callbacks: OpenAIStreamCallbacks & {\n    [__internal__OpenAIFnMessagesSymbol]?: CreateMessage[];\n  },\n): TransformStream<Uint8Array, Uint8Array> {\n  const textEncoder = new TextEncoder();\n  let isFirstChunk = true;\n  let aggregatedResponse = '';\n  let aggregatedFinalCompletionResponse = '';\n  let isFunctionStreamingIn = false;\n\n  let functionCallMessages: CreateMessage[] =\n    callbacks[__internal__OpenAIFnMessagesSymbol] || [];\n\n  const decode = createChunkDecoder();\n\n  return new TransformStream({\n    async transform(chunk, controller): Promise<void> {\n      const message = decode(chunk);\n      aggregatedFinalCompletionResponse += message;\n\n      const shouldHandleAsFunction =\n        isFirstChunk &&\n        (message.startsWith('{\"function_call\":') ||\n          message.startsWith('{\"tool_calls\":'));\n\n      if (shouldHandleAsFunction) {\n        isFunctionStreamingIn = true;\n        aggregatedResponse += message;\n        isFirstChunk = false;\n        return;\n      }\n\n      // Stream as normal\n      if (!isFunctionStreamingIn) {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('text', message)),\n        );\n        return;\n      } else {\n        aggregatedResponse += message;\n      }\n    },\n    async flush(controller): Promise<void> {\n      try {\n        if (\n          !isFirstChunk &&\n          isFunctionStreamingIn &&\n          (callbacks.experimental_onFunctionCall ||\n            callbacks.experimental_onToolCall)\n        ) {\n          isFunctionStreamingIn = false;\n          const payload = JSON.parse(aggregatedResponse);\n          // Append the function call message to the list\n          let newFunctionCallMessages: CreateMessage[] = [\n            ...functionCallMessages,\n          ];\n\n          let functionResponse:\n            | Response\n            | undefined\n            | void\n            | string\n            | AsyncIterableOpenAIStreamReturnTypes\n            | undefined = undefined;\n          // This callbacks.experimental_onFunctionCall check should not be necessary but TS complains\n          if (callbacks.experimental_onFunctionCall) {\n            // If the user is using the experimental_onFunctionCall callback, they should not be using tools\n            // if payload.function_call is not defined by time we get here we must have gotten a tool response\n            // and the user had defined experimental_onToolCall\n            if (payload.function_call === undefined) {\n              console.warn(\n                'experimental_onFunctionCall should not be defined when using tools',\n              );\n            }\n\n            const argumentsPayload = JSON.parse(\n              payload.function_call.arguments,\n            );\n\n            functionResponse = await callbacks.experimental_onFunctionCall(\n              {\n                name: payload.function_call.name,\n                arguments: argumentsPayload,\n              },\n              result => {\n                // Append the function call request and result messages to the list\n                newFunctionCallMessages = [\n                  ...functionCallMessages,\n                  {\n                    role: 'assistant',\n                    content: '',\n                    function_call: payload.function_call,\n                  },\n                  {\n                    role: 'function',\n                    name: payload.function_call.name,\n                    content: JSON.stringify(result),\n                  },\n                ];\n                // Return it to the user\n                return newFunctionCallMessages;\n              },\n            );\n          }\n          if (callbacks.experimental_onToolCall) {\n            const toolCalls: ToolCallPayload = {\n              tools: [],\n            };\n            for (const tool of payload.tool_calls) {\n              toolCalls.tools.push({\n                id: tool.id,\n                type: 'function',\n                func: {\n                  name: tool.function.name,\n                  arguments: JSON.parse(tool.function.arguments),\n                },\n              });\n            }\n            let responseIndex = 0;\n            try {\n              functionResponse = await callbacks.experimental_onToolCall(\n                toolCalls,\n                result => {\n                  if (result) {\n                    const { tool_call_id, function_name, tool_call_result } =\n                      result;\n                    // Append the function call request and result messages to the list\n                    newFunctionCallMessages = [\n                      ...newFunctionCallMessages,\n                      // Only append the assistant message if it's the first response\n                      ...(responseIndex === 0\n                        ? [\n                            {\n                              role: 'assistant' as const,\n                              content: '',\n                              tool_calls: payload.tool_calls.map(\n                                (tc: ToolCall) => ({\n                                  id: tc.id,\n                                  type: 'function',\n                                  function: {\n                                    name: tc.function.name,\n                                    // we send the arguments an object to the user, but as the API expects a string, we need to stringify it\n                                    arguments: JSON.stringify(\n                                      tc.function.arguments,\n                                    ),\n                                  },\n                                }),\n                              ),\n                            },\n                          ]\n                        : []),\n                      // Append the function call result message\n                      {\n                        role: 'tool',\n                        tool_call_id,\n                        name: function_name,\n                        content: JSON.stringify(tool_call_result),\n                      },\n                    ];\n                    responseIndex++;\n                  }\n                  // Return it to the user\n                  return newFunctionCallMessages;\n                },\n              );\n            } catch (e) {\n              console.error('Error calling experimental_onToolCall:', e);\n            }\n          }\n\n          if (!functionResponse) {\n            // The user didn't do anything with the function call on the server and wants\n            // to either do nothing or run it on the client\n            // so we just return the function call as a message\n            controller.enqueue(\n              textEncoder.encode(\n                formatStreamPart(\n                  payload.function_call ? 'function_call' : 'tool_calls',\n                  // parse to prevent double-encoding:\n                  JSON.parse(aggregatedResponse),\n                ),\n              ),\n            );\n            return;\n          } else if (typeof functionResponse === 'string') {\n            // The user returned a string, so we just return it as a message\n            controller.enqueue(\n              textEncoder.encode(formatStreamPart('text', functionResponse)),\n            );\n            aggregatedFinalCompletionResponse = functionResponse;\n            return;\n          }\n\n          // Recursively:\n\n          // We don't want to trigger onStart or onComplete recursively\n          // so we remove them from the callbacks\n          // see https://github.com/vercel/ai/issues/351\n          const filteredCallbacks: OpenAIStreamCallbacks = {\n            ...callbacks,\n            onStart: undefined,\n          };\n          // We only want onFinal to be called the _last_ time\n          callbacks.onFinal = undefined;\n\n          const openAIStream = OpenAIStream(functionResponse, {\n            ...filteredCallbacks,\n            [__internal__OpenAIFnMessagesSymbol]: newFunctionCallMessages,\n          } as AIStreamCallbacksAndOptions);\n\n          const reader = openAIStream.getReader();\n\n          while (true) {\n            const { done, value } = await reader.read();\n            if (done) {\n              break;\n            }\n            controller.enqueue(value);\n          }\n        }\n      } finally {\n        if (callbacks.onFinal && aggregatedFinalCompletionResponse) {\n          await callbacks.onFinal(aggregatedFinalCompletionResponse);\n        }\n      }\n    },\n  });\n}\n","export const STREAMABLE_VALUE_TYPE = Symbol.for('ui.streamable.value');\nexport const DEV_DEFAULT_STREAMABLE_WARNING_TIME = 15 * 1000;\n","import {\n  InvalidToolArgumentsError,\n  LanguageModelV1,\n  NoSuchToolError,\n} from '@ai-sdk/provider';\nimport { ReactNode } from 'react';\nimport { z } from 'zod';\n\nimport { safeParseJSON } from '@ai-sdk/provider-utils';\nimport { CallSettings } from '../../core/prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../../core/prompt/convert-to-language-model-prompt';\nimport { getValidatedPrompt } from '../../core/prompt/get-validated-prompt';\nimport { prepareCallSettings } from '../../core/prompt/prepare-call-settings';\nimport { prepareToolsAndToolChoice } from '../../core/prompt/prepare-tools-and-tool-choice';\nimport { Prompt } from '../../core/prompt/prompt';\nimport { CallWarning, CoreToolChoice, FinishReason } from '../../core/types';\nimport {\n  CompletionTokenUsage,\n  calculateCompletionTokenUsage,\n} from '../../core/types/token-usage';\nimport { retryWithExponentialBackoff } from '../../core/util/retry-with-exponential-backoff';\nimport { createStreamableUI } from '../streamable';\nimport { createResolvablePromise } from '../utils';\n\ntype Streamable = ReactNode | Promise<ReactNode>;\n\ntype Renderer<T extends Array<any>> = (\n  ...args: T\n) =>\n  | Streamable\n  | Generator<Streamable, Streamable, void>\n  | AsyncGenerator<Streamable, Streamable, void>;\n\ntype RenderTool<PARAMETERS extends z.ZodTypeAny = any> = {\n  description?: string;\n  parameters: PARAMETERS;\n  generate?: Renderer<\n    [\n      z.infer<PARAMETERS>,\n      {\n        toolName: string;\n        toolCallId: string;\n      },\n    ]\n  >;\n};\n\ntype RenderText = Renderer<\n  [\n    {\n      /**\n       * The full text content from the model so far.\n       */\n      content: string;\n      /**\n       * The new appended text content from the model since the last `text` call.\n       */\n      delta: string;\n      /**\n       * Whether the model is done generating text.\n       * If `true`, the `content` will be the final output and this call will be the last.\n       */\n      done: boolean;\n    },\n  ]\n>;\n\ntype RenderResult = {\n  value: ReactNode;\n} & Awaited<ReturnType<LanguageModelV1['doStream']>>;\n\nconst defaultTextRenderer: RenderText = ({ content }: { content: string }) =>\n  content;\n\n/**\n * `streamUI` is a helper function to create a streamable UI from LLMs.\n */\nexport async function streamUI<\n  TOOLS extends { [name: string]: z.ZodTypeAny } = {},\n>({\n  model,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  initial,\n  text,\n  onFinish,\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\n     * The language model to use.\n     */\n    model: LanguageModelV1;\n\n    /**\n     * The tools that the model can call. The model needs to support calling tools.\n     */\n    tools?: {\n      [name in keyof TOOLS]: RenderTool<TOOLS[name]>;\n    };\n\n    /**\n     * The tool choice strategy. Default: 'auto'.\n     */\n    toolChoice?: CoreToolChoice<TOOLS>;\n\n    text?: RenderText;\n    initial?: ReactNode;\n    /**\n     * Callback that is called when the LLM response and the final object validation are finished.\n     */\n    onFinish?: (event: {\n      /**\n       * The reason why the generation finished.\n       */\n      finishReason: FinishReason;\n      /**\n       * The token usage of the generated response.\n       */\n      usage: CompletionTokenUsage;\n      /**\n       * The final ui node that was generated.\n       */\n      value: ReactNode;\n      /**\n       * Warnings from the model provider (e.g. unsupported settings)\n       */\n      warnings?: CallWarning[];\n      /**\n       * Optional raw response data.\n       */\n      rawResponse?: {\n        /**\n         * Response headers.\n         */\n        headers?: Record<string, string>;\n      };\n    }) => Promise<void> | void;\n  }): Promise<RenderResult> {\n  // TODO: Remove these errors after the experimental phase.\n  if (typeof model === 'string') {\n    throw new Error(\n      '`model` cannot be a string in `streamUI`. Use the actual model instance instead.',\n    );\n  }\n  if ('functions' in settings) {\n    throw new Error(\n      '`functions` is not supported in `streamUI`, use `tools` instead.',\n    );\n  }\n  if ('provider' in settings) {\n    throw new Error(\n      '`provider` is no longer needed in `streamUI`. Use `model` instead.',\n    );\n  }\n  if (tools) {\n    for (const [name, tool] of Object.entries(tools)) {\n      if ('render' in tool) {\n        throw new Error(\n          'Tool definition in `streamUI` should not have `render` property. Use `generate` instead. Found in tool: ' +\n            name,\n        );\n      }\n    }\n  }\n\n  const ui = createStreamableUI(initial);\n\n  // The default text renderer just returns the content as string.\n  const textRender = text || defaultTextRenderer;\n\n  let finished: Promise<void> | undefined;\n\n  async function handleRender(\n    args: [payload: any] | [payload: any, options: any],\n    renderer: undefined | Renderer<any>,\n    res: ReturnType<typeof createStreamableUI>,\n    lastCall = false,\n  ) {\n    if (!renderer) return;\n\n    const resolvable = createResolvablePromise<void>();\n\n    if (finished) {\n      finished = finished.then(() => resolvable.promise);\n    } else {\n      finished = resolvable.promise;\n    }\n\n    const value = renderer(...args);\n    if (\n      value instanceof Promise ||\n      (value &&\n        typeof value === 'object' &&\n        'then' in value &&\n        typeof value.then === 'function')\n    ) {\n      const node = await (value as Promise<React.ReactNode>);\n\n      if (lastCall) {\n        res.done(node);\n      } else {\n        res.update(node);\n      }\n\n      resolvable.resolve(void 0);\n    } else if (\n      value &&\n      typeof value === 'object' &&\n      Symbol.asyncIterator in value\n    ) {\n      const it = value as AsyncGenerator<\n        React.ReactNode,\n        React.ReactNode,\n        void\n      >;\n      while (true) {\n        const { done, value } = await it.next();\n        if (lastCall && done) {\n          res.done(value);\n        } else {\n          res.update(value);\n        }\n        if (done) break;\n      }\n      resolvable.resolve(void 0);\n    } else if (value && typeof value === 'object' && Symbol.iterator in value) {\n      const it = value as Generator<React.ReactNode, React.ReactNode, void>;\n      while (true) {\n        const { done, value } = it.next();\n        if (lastCall && done) {\n          res.done(value);\n        } else {\n          res.update(value);\n        }\n        if (done) break;\n      }\n      resolvable.resolve(void 0);\n    } else {\n      if (lastCall) {\n        res.done(value);\n      } else {\n        res.update(value);\n      }\n      resolvable.resolve(void 0);\n    }\n  }\n\n  const retry = retryWithExponentialBackoff({ maxRetries });\n  const validatedPrompt = getValidatedPrompt({ system, prompt, messages });\n  const result = await retry(() =>\n    model.doStream({\n      mode: {\n        type: 'regular',\n        ...prepareToolsAndToolChoice({ tools, toolChoice }),\n      },\n      ...prepareCallSettings(settings),\n      inputFormat: validatedPrompt.type,\n      prompt: convertToLanguageModelPrompt(validatedPrompt),\n      abortSignal,\n      headers,\n    }),\n  );\n\n  const [stream, forkedStream] = result.stream.tee();\n\n  (async () => {\n    try {\n      // Consume the forked stream asynchonously.\n\n      let content = '';\n      let hasToolCall = false;\n\n      const reader = forkedStream.getReader();\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        switch (value.type) {\n          case 'text-delta': {\n            content += value.textDelta;\n            handleRender(\n              [{ content, done: false, delta: value.textDelta }],\n              textRender,\n              ui,\n            );\n            break;\n          }\n\n          case 'tool-call-delta': {\n            hasToolCall = true;\n            break;\n          }\n\n          case 'tool-call': {\n            const toolName = value.toolName as keyof TOOLS & string;\n\n            if (!tools) {\n              throw new NoSuchToolError({ toolName: toolName });\n            }\n\n            const tool = tools[toolName];\n            if (!tool) {\n              throw new NoSuchToolError({\n                toolName,\n                availableTools: Object.keys(tools),\n              });\n            }\n\n            hasToolCall = true;\n            const parseResult = safeParseJSON({\n              text: value.args,\n              schema: tool.parameters,\n            });\n\n            if (parseResult.success === false) {\n              throw new InvalidToolArgumentsError({\n                toolName,\n                toolArgs: value.args,\n                cause: parseResult.error,\n              });\n            }\n\n            handleRender(\n              [\n                parseResult.value,\n                {\n                  toolName,\n                  toolCallId: value.toolCallId,\n                },\n              ],\n              tool.generate,\n              ui,\n              true,\n            );\n\n            break;\n          }\n\n          case 'error': {\n            throw value.error;\n          }\n\n          case 'finish': {\n            onFinish?.({\n              finishReason: value.finishReason,\n              usage: calculateCompletionTokenUsage(value.usage),\n              value: ui.value,\n              warnings: result.warnings,\n              rawResponse: result.rawResponse,\n            });\n          }\n        }\n      }\n\n      if (hasToolCall) {\n        await finished;\n      } else {\n        handleRender([{ content, done: true }], textRender, ui, true);\n        await finished;\n      }\n    } catch (error) {\n      // During the stream rendering, we don't want to throw the error to the\n      // parent scope but only let the React's error boundary to catch it.\n      ui.error(error);\n    }\n  })();\n\n  return {\n    ...result,\n    stream,\n    value: ui.value,\n  };\n}\n","// This file provides the AI context to all AI Actions via AsyncLocalStorage.\n\nimport * as React from 'react';\nimport { InternalAIProvider } from './rsc-shared.mjs';\nimport {\n  withAIState,\n  getAIStateDeltaPromise,\n  sealMutableAIState,\n} from './ai-state';\nimport type {\n  ServerWrappedActions,\n  AIAction,\n  AIActions,\n  AIProvider,\n  InternalAIStateStorageOptions,\n  OnSetAIState,\n  OnGetUIState,\n} from './types';\n\nasync function innerAction<T>(\n  {\n    action,\n    options,\n  }: { action: AIAction; options: InternalAIStateStorageOptions },\n  state: T,\n  ...args: unknown[]\n) {\n  'use server';\n  return await withAIState(\n    {\n      state,\n      options,\n    },\n    async () => {\n      const result = await action(...args);\n      sealMutableAIState();\n      return [getAIStateDeltaPromise() as Promise<T>, result];\n    },\n  );\n}\n\nfunction wrapAction<T = unknown>(\n  action: AIAction,\n  options: InternalAIStateStorageOptions,\n) {\n  return innerAction.bind(null, { action, options }) as AIAction<T>;\n}\n\nexport function createAI<\n  AIState = any,\n  UIState = any,\n  Actions extends AIActions = {},\n>({\n  actions,\n  initialAIState,\n  initialUIState,\n\n  onSetAIState,\n  onGetUIState,\n}: {\n  actions: Actions;\n  initialAIState?: AIState;\n  initialUIState?: UIState;\n\n  /**\n   * This function is called whenever the AI state is updated by an Action.\n   * You can use this to persist the AI state to a database, or to send it to a\n   * logging service.\n   */\n  onSetAIState?: OnSetAIState<AIState>;\n\n  /**\n   * This function is used to retrieve the UI state based on the AI state.\n   * For example, to render the initial UI state based on a given AI state, or\n   * to sync the UI state when the application is already loaded.\n   *\n   * If returning `undefined`, the client side UI state will not be updated.\n   *\n   * This function must be annotated with the `\"use server\"` directive.\n   *\n   * @example\n   * ```tsx\n   * onGetUIState: async () => {\n   *   'use server';\n   *\n   *   const currentAIState = getAIState();\n   *   const externalAIState = await loadAIStateFromDatabase();\n   *\n   *   if (currentAIState === externalAIState) return undefined;\n   *\n   *   // Update current AI state and return the new UI state\n   *   const state = getMutableAIState()\n   *   state.done(externalAIState)\n   *\n   *   return <div>...</div>;\n   * }\n   * ```\n   */\n  onGetUIState?: OnGetUIState<UIState>;\n}) {\n  // Wrap all actions with our HoC.\n  const wrappedActions: ServerWrappedActions = {};\n  for (const name in actions) {\n    wrappedActions[name] = wrapAction(actions[name], {\n      onSetAIState,\n    });\n  }\n\n  const wrappedSyncUIState = onGetUIState\n    ? wrapAction(onGetUIState, {})\n    : undefined;\n\n  const AI: AIProvider<AIState, UIState, Actions> = async props => {\n    if ('useState' in React) {\n      // This file must be running on the React Server layer.\n      // Ideally we should be using `import \"server-only\"` here but we can have a\n      // more customized error message with this implementation.\n      throw new Error(\n        'This component can only be used inside Server Components.',\n      );\n    }\n\n    let uiState = props.initialUIState ?? initialUIState;\n    let aiState = props.initialAIState ?? initialAIState;\n    let aiStateDelta = undefined;\n\n    if (wrappedSyncUIState) {\n      const [newAIStateDelta, newUIState] = await wrappedSyncUIState(aiState);\n      if (newUIState !== undefined) {\n        aiStateDelta = newAIStateDelta;\n        uiState = newUIState;\n      }\n    }\n\n    return (\n      <InternalAIProvider\n        wrappedActions={wrappedActions}\n        wrappedSyncUIState={wrappedSyncUIState}\n        initialUIState={uiState}\n        initialAIState={aiState}\n        initialAIStatePatch={aiStateDelta}\n      >\n        {props.children}\n      </InternalAIProvider>\n    );\n  };\n\n  return AI;\n}\n"],"names":["getErrorMessage","formatStreamPart","streamable","zodToJsonSchema","value","React","jsx"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACEO,SAAS;IACd,IAAI,SAA6B;IACjC,MAAM,UAAU,IAAI,QAAW,CAAC,KAAK;QACnC,UAAU;QACV,SAAS;IACX;IACA,OAAO;QACL;QACA;QACA;IACF;AACF;AAGA,IAAM,IAAI;IACP,OAAO,EACN,CAAA,EAAA,UAAA;IACA,CAAA,EACF;QAIE,MAAM,QAAQ,MAAM;QACpB,IAAI,MAAM,IAAA,EAAM;YACd,OAAO,MAAM,KAAA;QACf;QAEA,IAAI,MAAM,MAAA,EAAQ;YAChB,OACE,aAAA,GAAA,CAAA,GAAA,iOAAA,CAAA,OAAA,EAAA,iOAAA,CAAA,WAAA,EAAA;gBACG,UAAA;oBAAA;oBACD,aAAA,GAAA,CAAA,GAAA,iOAAA,CAAA,MAAA,EAAC,+MAAA,CAAA,WAAA,EAAA;wBAAS,UAAU,MAAM,KAAA;wBACxB,UAAA,aAAA,GAAA,CAAA,GAAA,iOAAA,CAAA,MAAA,EAAC,GAAA;4BAAE,GAAG,MAAM,KAAA;4BAAO,GAAG,MAAM,IAAA;wBAAA;oBAAM;iBACpC;YAAA;QAGN;QAEA,OACE,aAAA,GAAA,CAAA,GAAA,iOAAA,CAAA,MAAA,EAAC,+MAAA,CAAA,WAAA,EAAA;YAAS,UAAU,MAAM,KAAA;YACxB,UAAA,aAAA,GAAA,CAAA,GAAA,iOAAA,CAAA,MAAA,EAAC,GAAA;gBAAE,GAAG,MAAM,KAAA;gBAAO,GAAG,MAAM,IAAA;YAAA;QAAM;IAGxC;CAIF,CAAE,EAAC;AAEI,SAAS,qBAAqB,YAAA;IACnC,MAAM,EAAE,OAAA,EAAS,OAAA,EAAS,MAAA,EAAO,GAAI;IAErC,OAAO;QACL,KACE,aAAA,GAAA,CAAA,GAAA,iOAAA,CAAA,MAAA,EAAC,+MAAA,CAAA,WAAA,EAAA;YAAS,UAAU;YAClB,UAAA,aAAA,GAAA,CAAA,GAAA,iOAAA,CAAA,MAAA,EAAC,GAAA;gBAAE,GAAG;gBAAc,GAAG;YAAA;QAAS;QAGpC;QACA;IACF;AACF;AAEO,IAAM,aAAa,CAAC,IACzB,OAAO,MAAM;AAER,IAAM,gBAAgB,OAAO;IAClC,MAAM,SAAS,OAAO,SAAA;IACtB,MAAO,KAAM;QACX,MAAM,EAAE,IAAA,EAAK,GAAI,MAAM,OAAO,IAAA;QAC9B,IAAI,MAAM;IACZ;AACF;;AD7DA,IAAM,sBAAsB,IAAI,8CAAA,CAAA,oBAAA;AAShC,SAAS,uBAAuB,OAAA;IAC9B,MAAM,QAAQ,oBAAoB,QAAA;IAClC,IAAI,CAAC,OAAO;QACV,MAAM,IAAI,MAAM;IAClB;IACA,OAAO;AACT;AAEO,SAAS,YACd,EAAE,OAAA,MAAA,EAAO,OAAA,EAAQ,EACjB,EAAA;IAEA,OAAO,oBAAoB,GAAA,CACzB;QACE,cAAc;QACd,eAAe;QACf,QAAQ;QACR;IACF,GACA;AAEJ;AAEO,SAAS;IACd,MAAM,QAAQ,uBAAuB;IACrC,OAAO,MAAM,oBAAA;AACf;AAKO,SAAS;IACd,MAAM,QAAQ,uBAAuB;IACrC,MAAM,MAAA,GAAS;AACjB;AAgBA,SAAS,WAAA,GACJ,IAAA;IAEH,MAAM,QAAQ,uBACZ;IAGF,IAAI,KAAK,MAAA,GAAS,GAAG;QACnB,MAAM,MAAM,IAAA,CAAK,EAAC;QAClB,IAAI,OAAO,MAAM,YAAA,KAAiB,UAAU;YAC1C,MAAM,IAAI,MACR,CAAA,mBAAA,EAAsB,OACpB,KACD,qDAAA,CAAA;QAEL;QACA,OAAO,MAAM,YAAA,CAAa,IAAsC;IAClE;IAEA,OAAO,MAAM,YAAA;AACf;AA0BA,SAAS,kBAAA,GACJ,IAAA;IAQH,MAAM,QAAQ,uBACZ;IAGF,IAAI,MAAM,MAAA,EAAQ;QAChB,MAAM,IAAI,MACR;IAEJ;IAEA,IAAI,CAAC,MAAM,oBAAA,EAAsB;QAC/B,MAAM,EAAE,OAAA,EAAS,OAAA,EAAQ,GAAI;QAC7B,MAAM,oBAAA,GAAuB;QAC7B,MAAM,oBAAA,GAAuB;IAC/B;IAEA,SAAS,SAAS,QAAA,EAA6B,IAAA;QA/IjD,IAAA,IAAA;QAgJI,IAAI,KAAK,MAAA,GAAS,GAAG;YACnB,IAAI,OAAO,MAAM,YAAA,KAAiB,UAAU;gBAC1C,MAAM,MAAM,IAAA,CAAK,EAAC;gBAClB,MAAM,IAAI,MACR,CAAA,sBAAA,EAAyB,OACvB,KACD,mDAAA,CAAA;YAEL;QACF;QAEA,IAAI,WAAW,WAAW;YACxB,IAAI,KAAK,MAAA,GAAS,GAAG;gBACnB,MAAM,YAAA,CAAa,IAAA,CAAK,EAAE,CAAA,GAAI,SAAS,MAAM,YAAA,CAAa,IAAA,CAAK,EAAE,CAAC;YACpE,OAAO;gBACL,MAAM,YAAA,GAAe,SAAS,MAAM,YAAY;YAClD;QACF,OAAO;YACL,IAAI,KAAK,MAAA,GAAS,GAAG;gBACnB,MAAM,YAAA,CAAa,IAAA,CAAK,EAAE,CAAA,GAAI;YAChC,OAAO;gBACL,MAAM,YAAA,GAAe;YACvB;QACF;QAEA,CAAA,KAAA,CAAA,KAAA,MAAM,OAAA,EAAQ,YAAA,KAAd,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,IAA6B;YAC3B,KAAK,KAAK,MAAA,GAAS,IAAI,IAAA,CAAK,EAAC,GAAI,KAAA;YACjC,OAAO,MAAM,YAAA;YACb;QACF;IACF;IAEA,MAAM,eAAe;QACnB,KAAK;YACH,IAAI,KAAK,MAAA,GAAS,GAAG;gBACnB,MAAM,MAAM,IAAA,CAAK,EAAC;gBAClB,IAAI,OAAO,MAAM,YAAA,KAAiB,UAAU;oBAC1C,MAAM,IAAI,MACR,CAAA,mBAAA,EAAsB,OACpB,KACD,qDAAA,CAAA;gBAEL;gBACA,OAAO,MAAM,YAAA,CAAa,IAAG;YAC/B;YAEA,OAAO,MAAM,YAAA;QACf;QACA,QAAQ,SAAS,OAAO,UAAA;YACtB,SAAS,YAAY;QACvB;QACA,MAAM,SAAS,KAAA,GAAQ,QAAA;YACrB,IAAI,SAAS,MAAA,GAAS,GAAG;gBACvB,SAAS,QAAA,CAAS,EAAC,EAAwB;YAC7C;YAEA,MAAM,QAAsB,8JAAA,IAAA,CAAK,MAAM,aAAA,EAAe,MAAM,YAAY;YACxE,MAAM,oBAAA,CAAsB;QAC9B;IACF;IAEA,OAAO;AACT;;;;;AI9MA,eAAsB,MAAM,SAAA;IAC1B,OAAO,IAAI,QAAQ,CAAA,UAAW,WAAW,SAAS;AACpD;;ADUO,IAAM,8BACX,CAAC,EACC,aAAa,CAAA,EACb,mBAAmB,GAAA,EACnB,gBAAgB,CAAA,EAClB,GAAI,CAAC,CAAA,GACL,OAAe,IACb,6BAA6B,GAAG;YAC9B;YACA,WAAW;YACX;QACF;AAEJ,eAAe,6BACb,CAAA,EACA,EACE,UAAA,EACA,SAAA,EACA,aAAA,EACF,EACA,SAAoB,EAAC;IAErB,IAAI;QACF,OAAO,MAAM;IACf,EAAA,OAAS,OAAO;QACd,IAAI,CAAA,GAAA,wLAAA,CAAA,eAAA,EAAa,QAAQ;YACvB,MAAM;QACR;QAEA,IAAI,eAAe,GAAG;YACpB,MAAM;QACR;QAEA,MAAM,eAAe,CAAA,GAAA,wLAAA,CAAA,kBAAA,EAAgB;QACrC,MAAM,YAAY;eAAI;YAAQ;SAAK;QACnC,MAAM,YAAY,UAAU,MAAA;QAE5B,IAAI,YAAY,YAAY;YAC1B,MAAM,IAAI,+KAAA,CAAA,aAAA,CAAW;gBACnB,SAAS,CAAA,aAAA,EAAgB,UAAS,uBAAA,EAA0B,aAAY,CAAA;gBACxE,QAAQ;gBACR,QAAQ;YACV;QACF;QAEA,IACE,iBAAiB,SACjB,+KAAA,CAAA,eAAA,CAAa,cAAA,CAAe,UAC5B,MAAM,WAAA,KAAgB,QACtB,aAAa,YACb;YACA,MAAM,MAAM;YACZ,OAAO,6BACL,GACA;gBAAE;gBAAY,WAAW,gBAAgB;gBAAW;YAAc,GAClE;QAEJ;QAEA,IAAI,cAAc,GAAG;YACnB,MAAM;QACR;QAEA,MAAM,IAAI,+KAAA,CAAA,aAAA,CAAW;YACnB,SAAS,CAAA,aAAA,EAAgB,UAAS,qCAAA,EAAwC,aAAY,CAAA,CAAA;YACtF,QAAQ;YACR,QAAQ;QACV;IACF;AACF;;AEjFA,IAAM,qBAAqB;IACzB;QAAE,UAAU;QAAsB,OAAO;YAAC;YAAM;YAAM;SAAI;IAAE;IAC5D;QAAE,UAAU;QAAsB,OAAO;YAAC;YAAM;YAAM;YAAM;SAAI;IAAE;IAClE;QAAE,UAAU;QAAuB,OAAO;YAAC;YAAM;SAAI;IAAE;IACvD;QAAE,UAAU;QAAuB,OAAO;YAAC;YAAM;YAAM;YAAM;SAAI;IAAE;CACrE;AAEO,SAAS,oBACd,KAAA;IAEA,KAAA,MAAW,EAAE,KAAA,EAAO,QAAA,EAAS,IAAK,mBAAoB;QACpD,IACE,MAAM,MAAA,IAAU,MAAM,MAAA,IACtB,MAAM,KAAA,CAAM,CAAC,MAAM,QAAU,KAAA,CAAM,MAAK,KAAM,OAC9C;YACA,OAAO;QACT;IACF;IAEA,OAAO,KAAA;AACT;;;ACeO,SAAS,+BACd,OAAA;IAEA,IAAI,mBAAmB,YAAY;QACjC,OAAO;IACT;IAEA,IAAI,OAAO,YAAY,UAAU;QAC/B,IAAI;YACF,OAAO,CAAA,GAAA,wLAAA,CAAA,4BAAA,EAA0B;QACnC,EAAA,OAAS,OAAO;YACd,MAAM,IAAI,+KAAA,CAAA,0BAAA,CAAwB;gBAChC,SACE;gBACF;gBACA,OAAO;YACT;QACF;IACF;IAEA,IAAI,mBAAmB,aAAa;QAClC,OAAO,IAAI,WAAW;IACxB;IAEA,MAAM,IAAI,+KAAA,CAAA,0BAAA,CAAwB;QAAE;IAAQ;AAC9C;;AC5DO,IAAM,0BAAN,cAAsC;IAG3C,YAAY,EACV,IAAA,EACA,UAAU,CAAA,uBAAA,EAA0B,KAAI,yDAAA,CAAA,EAC1C,CAGG;QACD,KAAA,CAAM;QAEN,IAAA,CAAK,IAAA,GAAO;QAEZ,IAAA,CAAK,IAAA,GAAO;IACd;IAEA,OAAO,0BACL,KAAA,EACkC;QAClC,OACE,iBAAiB,SACjB,MAAM,IAAA,KAAS,gCACf,OAAQ,MAAkC,IAAA,KAAS;IAEvD;IAEA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,MAAM,IAAA,CAAK,IAAA;QACb;IACF;AACF;;ACvBO,SAAS,6BACd,MAAA;IAEA,MAAM,wBAA+C,EAAC;IAEtD,IAAI,OAAO,MAAA,IAAU,MAAM;QACzB,sBAAsB,IAAA,CAAK;YAAE,MAAM;YAAU,SAAS,OAAO,MAAA;QAAO;IACtE;IAEA,MAAM,aAAa,OAAO,IAAA;IAC1B,OAAQ;QACN,KAAK;YAAU;gBACb,sBAAsB,IAAA,CAAK;oBACzB,MAAM;oBACN,SAAS;wBAAC;4BAAE,MAAM;4BAAQ,MAAM,OAAO,MAAA;wBAAO;qBAAC;gBACjD;gBACA;YACF;QAEA,KAAK;YAAY;gBACf,sBAAsB,IAAA,IACjB,OAAO,QAAA,CAAS,GAAA,CAAI;gBAEzB;YACF;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAA,yBAAA,EAA4B,iBAAgB,CAAE;YAChE;IACF;IAEA,OAAO;AACT;AAEO,SAAS,8BACd,OAAA;IAEA,MAAM,OAAO,QAAQ,IAAA;IACrB,OAAQ;QACN,KAAK;YAAU;gBACb,OAAO;oBAAE,MAAM;oBAAU,SAAS,QAAQ,OAAA;gBAAQ;YACpD;QAEA,KAAK;YAAQ;gBACX,IAAI,OAAO,QAAQ,OAAA,KAAY,UAAU;oBACvC,OAAO;wBACL,MAAM;wBACN,SAAS;4BAAC;gCAAE,MAAM;gCAAQ,MAAM,QAAQ,OAAA;4BAAQ;yBAAC;oBACnD;gBACF;gBAEA,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA,CAAQ,GAAA,CACvB,CAAC;wBApEX,IAAA;wBAqEY,OAAQ,KAAK,IAAA;4BACX,KAAK;gCAAQ;oCACX,OAAO;gCACT;4BAEA,KAAK;gCAAS;oCACZ,IAAI,KAAK,KAAA,YAAiB,KAAK;wCAC7B,OAAO;4CACL,MAAM;4CACN,OAAO,KAAK,KAAA;4CACZ,UAAU,KAAK,QAAA;wCACjB;oCACF;oCAGA,IAAI,OAAO,KAAK,KAAA,KAAU,UAAU;wCAClC,IAAI;4CACF,MAAM,MAAM,IAAI,IAAI,KAAK,KAAK;4CAE9B,OAAQ,IAAI,QAAA;gDACV,KAAK;gDACL,KAAK;oDAAU;wDACb,OAAO;4DACL,MAAM;4DACN,OAAO;4DACP,UAAU,KAAK,QAAA;wDACjB;oDACF;gDACA,KAAK;oDAAS;wDACZ,IAAI;4DACF,MAAM,CAAC,QAAQ,cAAa,GAAI,KAAK,KAAA,CAAM,KAAA,CAAM;4DACjD,MAAM,WAAW,OAAO,KAAA,CAAM,IAAG,CAAE,EAAC,CAAE,KAAA,CAAM,IAAG,CAAE,EAAC;4DAElD,IAAI,YAAY,QAAQ,iBAAiB,MAAM;gEAC7C,MAAM,IAAI,MAAM;4DAClB;4DAEA,OAAO;gEACL,MAAM;gEACN,OACE,+BAA+B;gEACjC;4DACF;wDACF,EAAA,OAAS,OAAO;4DACd,MAAM,IAAI,MACR,CAAA,2BAAA,EAA8BA,CAAAA,GAAAA,wLAAAA,CAAAA,kBAAAA,EAC5B,SACD,CAAA;wDAEL;oDACF;gDACA;oDAAS;wDACP,MAAM,IAAI,MACR,CAAA,0BAAA,EAA6B,IAAI,QAAQ,CAAA,CAAA;oDAE7C;4CACF;wCACF,EAAA,OAAS,UAAU,CAEnB;oCACF;oCAEA,MAAM,aAAa,+BAA+B,KAAK,KAAK;oCAE5D,OAAO;wCACL,MAAM;wCACN,OAAO;wCACP,UAAA,CAAU,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,oBAAoB;oCACjD;gCACF;wBACF;oBACF;gBAEJ;YACF;QAEA,KAAK;YAAa;gBAChB,IAAI,OAAO,QAAQ,OAAA,KAAY,UAAU;oBACvC,OAAO;wBACL,MAAM;wBACN,SAAS;4BAAC;gCAAE,MAAM;gCAAQ,MAAM,QAAQ,OAAA;4BAAQ;yBAAC;oBACnD;gBACF;gBAEA,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA,CAAQ,MAAA,CAAA,2BAAA;oBAEvB,CAAA,OAAQ,KAAK,IAAA,KAAS,UAAU,KAAK,IAAA,KAAS;gBAElD;YACF;QAEA,KAAK;YAAQ;gBACX,OAAO;YACT;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,wBAAwB;oBAAE,MAAM;gBAAiB;YAC7D;IACF;AACF;;ACzJO,SAAS,mBAAmB,MAAA;IACjC,IAAI,OAAO,MAAA,IAAU,QAAQ,OAAO,QAAA,IAAY,MAAM;QACpD,MAAM,IAAI,+KAAA,CAAA,qBAAA,CAAmB;YAC3B;YACA,SAAS;QACX;IACF;IAEA,IAAI,OAAO,MAAA,IAAU,QAAQ,OAAO,QAAA,IAAY,MAAM;QACpD,MAAM,IAAI,+KAAA,CAAA,qBAAA,CAAmB;YAC3B;YACA,SAAS;QACX;IACF;IAEA,IAAI,OAAO,QAAA,IAAY,MAAM;QAC3B,KAAA,MAAW,WAAW,OAAO,QAAA,CAAU;YACrC,IAAI,QAAQ,IAAA,KAAS,YAAY,OAAO,QAAQ,OAAA,KAAY,UAAU;gBACpE,MAAM,IAAI,+KAAA,CAAA,qBAAA,CAAmB;oBAC3B;oBACA,SAAS;gBACX;YACF;QACF;IACF;IAEA,OAAO,OAAO,MAAA,IAAU,OACpB;QACE,MAAM;QACN,QAAQ,OAAO,MAAA;QACf,UAAU,KAAA;QACV,QAAQ,OAAO,MAAA;IACjB,IACA;QACE,MAAM;QACN,QAAQ,KAAA;QACR,UAAU,OAAO,QAAA;QAAA,wCAAA;QACjB,QAAQ,OAAO,MAAA;IACjB;AACN;;ACnDO,SAAS,oBAAoB,EAClC,SAAA,EACA,WAAA,EACA,IAAA,EACA,eAAA,EACA,gBAAA,EACA,IAAA,EACA,UAAA,EACF;IACE,IAAI,aAAa,MAAM;QACrB,IAAI,CAAC,OAAO,SAAA,CAAU,YAAY;YAChC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,YAAY,GAAG;YACjB,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,eAAe,MAAM;QACvB,IAAI,OAAO,gBAAgB,UAAU;YACnC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,QAAQ,MAAM;QAChB,IAAI,OAAO,SAAS,UAAU;YAC5B,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,mBAAmB,MAAM;QAC3B,IAAI,OAAO,oBAAoB,UAAU;YACvC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,oBAAoB,MAAM;QAC5B,IAAI,OAAO,qBAAqB,UAAU;YACxC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,QAAQ,MAAM;QAChB,IAAI,CAAC,OAAO,SAAA,CAAU,OAAO;YAC3B,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,cAAc,MAAM;QACtB,IAAI,CAAC,OAAO,SAAA,CAAU,aAAa;YACjC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,aAAa,GAAG;YAClB,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,OAAO;QACL;QACA,aAAa,eAAA,OAAA,cAAe;QAC5B;QACA;QACA;QACA;QACA,YAAY,cAAA,OAAA,aAAc;IAC5B;AACF;;AChFO,SAAS,8BAA8B,KAAA;IAI5C,OAAO;QACL,cAAc,MAAM,YAAA;QACpB,kBAAkB,MAAM,gBAAA;QACxB,aAAa,MAAM,YAAA,GAAe,MAAM,gBAAA;IAC1C;AACF;;ACnCO,SAAS,uBACd,SAAA;IAGA,OAAO,CAAA,GAAA,mLAAA,CAAA,UAAA,EAAgB;AACzB;;ACTO,SAAS,iBACd,MAAA;IAEA,OAAO,UAAU,QAAQ,OAAO,IAAA,CAAK,QAAQ,MAAA,GAAS;AACxD;;ACKO,SAAS,0BAEd,EACA,KAAA,EACA,UAAA,EACF;IAOE,IAAI,CAAC,iBAAiB,QAAQ;QAC5B,OAAO;YACL,OAAO,KAAA;YACP,YAAY,KAAA;QACd;IACF;IAEA,OAAO;QACL,OAAO,OAAO,OAAA,CAAQ,OAAO,GAAA,CAAI,CAAC,CAAC,MAAM,KAAI,GAAA,CAAO;gBAClD,MAAM;gBACN;gBACA,aAAa,KAAK,WAAA;gBAClB,YAAY,uBAAuB,KAAK,UAAU;YACpD,CAAA;QACA,YACE,cAAc,OACV;YAAE,MAAM;QAAO,IACf,OAAO,eAAe,WACtB;YAAE,MAAM;QAAW,IACnB;YAAE,MAAM;YAAiB,UAAU,WAAW,QAAA;QAAmB;IACzE;AACF;;AC+BO,SAAS,6BACd,YAAA;IAEA,MAAM,cAAc,IAAI;IACxB,IAAI;IAEJ,OAAO,IAAI,gBAAgB;QACzB,MAAM,OAAM,UAAA;YACV,oBAAoB,CAAA,GAAA,sJAAA,CAAA,eAAA,EAClB,CAAC;gBACC,IACG,UAAU,SACT,MAAM,IAAA,KAAS,WACf,MAAM,IAAA,KAAS,YAAA,6DAAA;gBAAA,4CAAA;gBAGhB,MAAc,KAAA,KAAU,QACzB;oBACA,WAAW,SAAA;oBACX;gBACF;gBAEA,IAAI,UAAU,OAAO;oBACnB,MAAM,gBAAgB,eAClB,aAAa,MAAM,IAAA,EAAM;wBACvB,OAAO,MAAM,KAAA;oBACf,KACA,MAAM,IAAA;oBACV,IAAI,eAAe,WAAW,OAAA,CAAQ;gBACxC;YACF;QAEJ;QAEA,WAAU,KAAA;YACR,kBAAkB,IAAA,CAAK,YAAY,MAAA,CAAO;QAC5C;IACF;AACF;AAwBO,SAAS,2BACd,EAAA;IAEA,MAAM,cAAc,IAAI;IACxB,IAAI,qBAAqB;IACzB,MAAM,YAAY,MAAM,CAAC;IAEzB,OAAO,IAAI,gBAAgB;QACzB,MAAM;YACJ,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA;QACzC;QAEA,MAAM,WAAU,OAAA,EAAS,UAAA;YACvB,MAAM,UAAU,OAAO,YAAY,WAAW,UAAU,QAAQ,OAAA;YAEhE,WAAW,OAAA,CAAQ,YAAY,MAAA,CAAO;YAEtC,sBAAsB;YAEtB,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA,CAAQ;YAC/C,IAAI,UAAU,MAAA,IAAU,OAAO,YAAY,UAAU;gBACnD,MAAM,UAAU,MAAA,CAAO;YACzB;QACF;QAEA,MAAM;YACJ,MAAM,oBAAoB,8BAA8B;YAGxD,IAAI,UAAU,YAAA,EAAc;gBAC1B,MAAM,UAAU,YAAA,CAAa;YAC/B;YAEA,IAAI,UAAU,OAAA,IAAW,CAAC,mBAAmB;gBAC3C,MAAM,UAAU,OAAA,CAAQ;YAC1B;QACF;IACF;AACF;AAEA,SAAS,8BACP,SAAA;IAEA,OAAO,iCAAiC;AAC1C;AAgBO,SAAS;IACd,IAAI,gBAAgB;IAEpB,OAAO,CAAC;QACN,IAAI,eAAe;YACjB,OAAO,KAAK,SAAA;YACZ,IAAI,MAAM,gBAAgB;QAC5B;QACA,OAAO;IACT;AACF;AAoBO,SAAS,SACd,QAAA,EACA,YAAA,EACA,SAAA;IAEA,IAAI,CAAC,SAAS,EAAA,EAAI;QAChB,IAAI,SAAS,IAAA,EAAM;YACjB,MAAM,SAAS,SAAS,IAAA,CAAK,SAAA;YAC7B,OAAO,IAAI,eAAe;gBACxB,MAAM,OAAM,UAAA;oBACV,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;oBACrC,IAAI,CAAC,MAAM;wBACT,MAAM,YAAY,IAAI,cAAc,MAAA,CAAO;wBAC3C,WAAW,KAAA,CAAM,IAAI,MAAM,CAAA,gBAAA,EAAmB,UAAS,CAAE;oBAC3D;gBACF;YACF;QACF,OAAO;YACL,OAAO,IAAI,eAAe;gBACxB,OAAM,UAAA;oBACJ,WAAW,KAAA,CAAM,IAAI,MAAM;gBAC7B;YACF;QACF;IACF;IAEA,MAAM,qBAAqB,SAAS,IAAA,IAAQ;IAE5C,OAAO,mBACJ,WAAA,CAAY,6BAA6B,eACzC,WAAA,CAAY,2BAA2B;AAC5C;AAeA,SAAS;IACP,OAAO,IAAI,eAAe;QACxB,OAAM,UAAA;YACJ,WAAW,KAAA;QACb;IACF;AACF;AAMO,SAAS,0BAA6B,QAAA;IAC3C,IAAI,KAAK,QAAA,CAAS,OAAO,aAAa,CAAA;IACtC,OAAO,IAAI,eAAkB;QAC3B,MAAM,MAAK,UAAA;YACT,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,GAAG,IAAA;YACjC,IAAI,MAAM,WAAW,KAAA;iBAChB,WAAW,OAAA,CAAQ;QAC1B;QAEA,MAAM,QAAO,MAAA;YApSjB,IAAA;YAqSM,MAAA,CAAA,CAAM,KAAA,GAAG,MAAA,KAAH,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,IAAY,OAAA;QACpB;IACF;AACF;;AC9MO,SAAS;IACd,MAAM,UAAU,IAAI;IACpB,MAAM,UAAU,IAAI;IACpB,OAAO,IAAI,gBAAgB;QACzB,WAAW,OAAO,OAAO;YACvB,MAAM,UAAU,QAAQ,MAAA,CAAO;YAC/B,WAAW,OAAA,CAAQ,QAAQ,MAAA,CAAO,CAAA,GAAA,4KAAA,CAAA,mBAAA,EAAiB,QAAQ;QAC7D;IACF;AACF;;AC0KA,SAAS;IAGP,MAAM,UAAU;IAChB,OAAO,CAAA,OAAQ,QAAQ,KAAK,KAAA,CAAM;AACpC;AAOA,gBAAgB,WAAW,MAAA;IACzB,MAAM,UAAU;IAEhB,WAAA,IAAe,SAAS,OAAQ;QAG9B,IAAI,yBAAyB,OAAO;YAClC,QAAQ;gBACN,IAAI,MAAM,EAAA;gBACV,SAAS,MAAM,OAAA,CAAQ,OAAA;gBACvB,QAAS,MAAc,MAAA;gBAAA,2BAAA;gBACvB,OAAQ,MAAc,KAAA;gBAAA,2BAAA;gBACtB,SAAS,MAAM,OAAA,CAAQ,GAAA,CAAI,CAAA;oBArSnC,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;oBAqS8C,OAAA;wBACpC,OAAO;4BACL,SAAA,CAAS,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,OAAA;4BACvB,eAAA,CAAe,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,YAAA;4BAC7B,MAAA,CAAM,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,IAAA;4BACpB,YAAA,CAAA,CAAY,KAAA,CAAA,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,SAAA,KAAd,OAAA,KAAA,IAAA,GAAyB,MAAA,IAAA,CACjC,KAAA,CAAA,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,SAAA,KAAd,OAAA,KAAA,IAAA,GAAyB,GAAA,CAAI,CAAC,UAAU,QAAA,CAAW;oCACjD;oCACA,IAAI,SAAS,EAAA;oCACb,UAAU,SAAS,QAAA;oCACnB,MAAM,SAAS,IAAA;gCACjB,CAAA,KACA,KAAA;wBACN;wBACA,eAAe,OAAO,YAAA;wBACtB,OAAO,OAAO,KAAA;oBAChB;gBAAA;YACF;QACF;QAEA,MAAM,OAAO,QAAQ;QAErB,IAAI,MAAM,MAAM;IAClB;AACF;AAEA,SAAS;IAGP,MAAM,oBAAoB;IAC1B,IAAI;IACJ,OAAO,CAAA;QApUT,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;QAqUI,IAAI,sBAAsB,OAAO;YAC/B,MAAM,QAAA,CAAQ,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,KAAA;YAC/B,IAAA,CAAI,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,IAAA,EAAM;gBAC7B,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS,CAAA,4BAAA,EAA+B,MAAM,aAAA,CAAc,IAAI,CAAA,iBAAA,CAAA;gBAClE;YACF,OAAA,IAAA,CAAW,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,IAAA,EAAM;gBAChD,wBAAwB;gBACxB,MAAM,WAAW,MAAM,UAAA,CAAW,EAAC;gBACnC,IAAI,SAAS,KAAA,KAAU,GAAG;oBACxB,OAAO;wBACL,QAAQ;wBACR,SAAS,CAAA,wBAAA,EAA2B,SAAS,EAAE,CAAA,6CAAA,EAAA,CAAgD,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAI,CAAA,iBAAA,CAAA;oBACxH;gBACF,OAAO;oBACL,OAAO;wBACL,QAAQ;wBACR,SAAS,CAAA,aAAA,EAAgB,SAAS,EAAE,CAAA,6CAAA,EAAA,CAAgD,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAI,CAAA,iBAAA,CAAA;oBAC7G;gBACF;YACF,OAAA,IAAA,CAAW,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,SAAA,EAAW;gBACzC,OAAO;oBACL,QAAQ;oBACR,SAAS,iBAAA,CAAiB,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,SAAS;gBAC1D;YACF,OAAA,IAAA,CAAW,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,SAAA,EAAW;gBACrD,OAAO;oBACL,QAAQ;oBACR,SAAS,iBAAA,CAAiB,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,SAAS;gBACtE;YACF,OAAA,IACE,yBAAA,CAAA,CAAA,CACC,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,mBAAA,CAAA,CAClC,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,MAAA,GACrC;gBACA,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS;gBACX;YACF,OAAA,IACE,yBAAA,CAAA,CACA,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,cACnC;gBACA,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS;gBACX;YACF;QACF;QAEA,MAAM,OAAO,kBACX,sBAAsB,SAAS,KAAK,OAAA,CAAQ,EAAC,CAAE,KAAA,CAAM,OAAA,GACjD,KAAK,OAAA,CAAQ,EAAC,CAAE,KAAA,CAAM,OAAA,GACtB,aAAa,QACb,KAAK,OAAA,CAAQ,EAAC,CAAE,IAAA,GAChB;QAGN,OAAO;IACT;IAEA,SAAS,iBAAiB,aAAA;QACxB,IAAI,qBAAqB,cACtB,OAAA,CAAQ,OAAO,QACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,MAAM,OACd,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO;QAElB,OAAO,CAAA,EAAG,mBAAkB,CAAA;IAC9B;AACF;AAEA,IAAM,qCAAqC,OACzC;AAaF,SAAS,sBACP,IAAA;IAEA,OACE,aAAa,QACb,KAAK,OAAA,IACL,KAAK,OAAA,CAAQ,EAAC,IACd,WAAW,KAAK,OAAA,CAAQ,EAAC;AAE7B;AAEA,SAAS,aAAa,IAAA;IACpB,OACE,aAAa,QACb,KAAK,OAAA,IACL,KAAK,OAAA,CAAQ,EAAC,IACd,UAAU,KAAK,OAAA,CAAQ,EAAC;AAE5B;AAKO,SAAS,aACd,GAAA,EACA,SAAA;IAGA,MAAM,KAIG;IAET,IAAI;IACJ,IAAI,OAAO,aAAA,IAAiB,KAAK;QAC/B,SAAS,0BAA0B,WAAW,MAAM,WAAA,CAClD,2BAAA,CACE,MAAA,OAAA,KAAA,IAAA,GAAI,2BAAA,KAAA,CAA+B,MAAA,OAAA,KAAA,IAAA,GAAI,uBAAA,IACnC;YACE,GAAG,EAAA;YACH,SAAS,KAAA;QACX,IACA;YACE,GAAG,EAAA;QACL;IAGV,OAAO;QACL,SAAS,SACP,KACA,qBAAkB,CAClB,MAAA,OAAA,KAAA,IAAA,GAAI,2BAAA,KAAA,CAA+B,MAAA,OAAA,KAAA,IAAA,GAAI,uBAAA,IACnC;YACE,GAAG,EAAA;YACH,SAAS,KAAA;QACX,IACA;YACE,GAAG,EAAA;QACL;IAER;IAEA,IAAI,MAAA,CAAO,GAAG,2BAAA,IAA+B,GAAG,uBAAA,GAA0B;QACxE,MAAM,0BAA0B,8BAA8B;QAC9D,OAAO,OAAO,WAAA,CAAY;IAC5B,OAAO;QACL,OAAO,OAAO,WAAA,CAAY;IAC5B;AACF;AAEA,SAAS,8BACP,SAAA;IAIA,MAAM,cAAc,IAAI;IACxB,IAAI,eAAe;IACnB,IAAI,qBAAqB;IACzB,IAAI,oCAAoC;IACxC,IAAI,wBAAwB;IAE5B,IAAI,uBACF,SAAA,CAAU,mCAAkC,IAAK,EAAC;IAEpD,MAAM,SAAS,CAAA,GAAA,4KAAA,CAAA,qBAAA;IAEf,OAAO,IAAI,gBAAgB;QACzB,MAAM,WAAU,KAAA,EAAO,UAAA;YACrB,MAAM,UAAU,OAAO;YACvB,qCAAqC;YAErC,MAAM,yBACJ,gBAAA,CACC,QAAQ,UAAA,CAAW,wBAClB,QAAQ,UAAA,CAAW,iBAAgB;YAEvC,IAAI,wBAAwB;gBAC1B,wBAAwB;gBACxB,sBAAsB;gBACtB,eAAe;gBACf;YACF;YAGA,IAAI,CAAC,uBAAuB;gBAC1B,WAAW,OAAA,CACT,YAAY,MAAA,CAAOC,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;gBAE9C;YACF,OAAO;gBACL,sBAAsB;YACxB;QACF;QACA,MAAM,OAAM,UAAA;YACV,IAAI;gBACF,IACE,CAAC,gBACD,yBAAA,CACC,UAAU,2BAAA,IACT,UAAU,uBAAA,GACZ;oBACA,wBAAwB;oBACxB,MAAM,UAAU,KAAK,KAAA,CAAM;oBAE3B,IAAI,0BAA2C;2BAC1C;qBACL;oBAEA,IAAI,mBAMY,KAAA;oBAEhB,IAAI,UAAU,2BAAA,EAA6B;wBAIzC,IAAI,QAAQ,aAAA,KAAkB,KAAA,GAAW;4BACvC,QAAQ,IAAA,CACN;wBAEJ;wBAEA,MAAM,mBAAmB,KAAK,KAAA,CAC5B,QAAQ,aAAA,CAAc,SAAA;wBAGxB,mBAAmB,MAAM,UAAU,2BAAA,CACjC;4BACE,MAAM,QAAQ,aAAA,CAAc,IAAA;4BAC5B,WAAW;wBACb,GACA,CAAA;4BAEE,0BAA0B;mCACrB;gCACH;oCACE,MAAM;oCACN,SAAS;oCACT,eAAe,QAAQ,aAAA;gCACzB;gCACA;oCACE,MAAM;oCACN,MAAM,QAAQ,aAAA,CAAc,IAAA;oCAC5B,SAAS,KAAK,SAAA,CAAU;gCAC1B;6BACF;4BAEA,OAAO;wBACT;oBAEJ;oBACA,IAAI,UAAU,uBAAA,EAAyB;wBACrC,MAAM,YAA6B;4BACjC,OAAO,EAAC;wBACV;wBACA,KAAA,MAAW,QAAQ,QAAQ,UAAA,CAAY;4BACrC,UAAU,KAAA,CAAM,IAAA,CAAK;gCACnB,IAAI,KAAK,EAAA;gCACT,MAAM;gCACN,MAAM;oCACJ,MAAM,KAAK,QAAA,CAAS,IAAA;oCACpB,WAAW,KAAK,KAAA,CAAM,KAAK,QAAA,CAAS,SAAS;gCAC/C;4BACF;wBACF;wBACA,IAAI,gBAAgB;wBACpB,IAAI;4BACF,mBAAmB,MAAM,UAAU,uBAAA,CACjC,WACA,CAAA;gCACE,IAAI,QAAQ;oCACV,MAAM,EAAE,YAAA,EAAc,aAAA,EAAe,gBAAA,EAAiB,GACpD;oCAEF,0BAA0B;2CACrB;wCAAA,+DAAA;2CAEC,kBAAkB,IAClB;4CACE;gDACE,MAAM;gDACN,SAAS;gDACT,YAAY,QAAQ,UAAA,CAAW,GAAA,CAC7B,CAAC,KAAA,CAAkB;wDACjB,IAAI,GAAG,EAAA;wDACP,MAAM;wDACN,UAAU;4DACR,MAAM,GAAG,QAAA,CAAS,IAAA;4DAAA,wGAAA;4DAElB,WAAW,KAAK,SAAA,CACd,GAAG,QAAA,CAAS,SAAA;wDAEhB;oDACF,CAAA;4CAEJ;yCACF,GACA,EAAC;wCAAA,0CAAA;wCAEL;4CACE,MAAM;4CACN;4CACA,MAAM;4CACN,SAAS,KAAK,SAAA,CAAU;wCAC1B;qCACF;oCACA;gCACF;gCAEA,OAAO;4BACT;wBAEJ,EAAA,OAAS,GAAG;4BACV,QAAQ,KAAA,CAAM,0CAA0C;wBAC1D;oBACF;oBAEA,IAAI,CAAC,kBAAkB;wBAIrB,WAAW,OAAA,CACT,YAAY,MAAA,CACVA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EACE,QAAQ,aAAA,GAAgB,kBAAkB,cAAA,oCAAA;wBAE1C,KAAK,KAAA,CAAM;wBAIjB;oBACF,OAAA,IAAW,OAAO,qBAAqB,UAAU;wBAE/C,WAAW,OAAA,CACT,YAAY,MAAA,CAAOA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;wBAE9C,oCAAoC;wBACpC;oBACF;oBAOA,MAAM,oBAA2C;wBAC/C,GAAG,SAAA;wBACH,SAAS,KAAA;oBACX;oBAEA,UAAU,OAAA,GAAU,KAAA;oBAEpB,MAAM,eAAe,aAAa,kBAAkB;wBAClD,GAAG,iBAAA;wBACH,CAAC,mCAAkC,EAAG;oBACxC;oBAEA,MAAM,SAAS,aAAa,SAAA;oBAE5B,MAAO,KAAM;wBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;wBACrC,IAAI,MAAM;4BACR;wBACF;wBACA,WAAW,OAAA,CAAQ;oBACrB;gBACF;YACF,SAAE;gBACA,IAAI,UAAU,OAAA,IAAW,mCAAmC;oBAC1D,MAAM,UAAU,OAAA,CAAQ;gBAC1B;YACF;QACF;IACF;AACF;;AC7sBO,IAAM,wBAAwB,OAAO,GAAA,CAAI;AACzC,IAAM,sCAAsC,KAAK;;AhBoExD,SAAS,mBAAmB,YAAA;IAC1B,IAAI,eAAe;IACnB,IAAI,SAAS;IACb,IAAI,EAAE,GAAA,EAAK,OAAA,EAAS,MAAA,EAAO,GAAI,qBAAqB;IAEpD,SAAS,aAAa,MAAA;QACpB,IAAI,QAAQ;YACV,MAAM,IAAI,MAAM,SAAS;QAC3B;IACF;IAEA,IAAI;IACJ,SAAS;QACP,wCAA4C;YAC1C,IAAI,gBAAgB;gBAClB,aAAa;YACf;YACA,iBAAiB,WAAW;gBAC1B,QAAQ,IAAA,CACN;YAEJ,GAAG;QACL;IACF;IACA;IAEA,MAAMC,cAAkC;QACtC,OAAO;QACP,QAAO,KAAA;YACL,aAAa;YAGb,IAAI,UAAU,cAAc;gBAC1B;gBACA,OAAOA;YACT;YAEA,MAAM,aAAa;YACnB,eAAe;YAEf,QAAQ;gBAAE,OAAO;gBAAc,MAAM;gBAAO,MAAM,WAAW,OAAA;YAAQ;YACrE,UAAU,WAAW,OAAA;YACrB,SAAS,WAAW,MAAA;YAEpB;YAEA,OAAOA;QACT;QACA,QAAO,KAAA;YACL,aAAa;YAEb,MAAM,aAAa;YACnB,eAAe;YAEf,QAAQ;gBAAE;gBAAO,MAAM;gBAAO,QAAQ;gBAAM,MAAM,WAAW,OAAA;YAAQ;YACrE,UAAU,WAAW,OAAA;YACrB,SAAS,WAAW,MAAA;YAEpB;YAEA,OAAOA;QACT;QACA,OAAM,KAAA;YACJ,aAAa;YAEb,IAAI,gBAAgB;gBAClB,aAAa;YACf;YACA,SAAS;YACT,OAAO;YAEP,OAAOA;QACT;QACA,MAAA,GAAQ,IAAA;YACN,aAAa;YAEb,IAAI,gBAAgB;gBAClB,aAAa;YACf;YACA,SAAS;YACT,IAAI,KAAK,MAAA,EAAQ;gBACf,QAAQ;oBAAE,OAAO,IAAA,CAAK,EAAC;oBAAG,MAAM;gBAAK;gBACrC,OAAOA;YACT;YACA,QAAQ;gBAAE,OAAO;gBAAc,MAAM;YAAK;YAE1C,OAAOA;QACT;IACF;IAEA,OAAOA;AACT;AAEA,IAAM,iCAAiC,OAAO;AAM9C,SAAS,sBACP,YAAA;IAEA,MAAM,mBACJ,wBAAwB,kBACvB,OAAO,iBAAiB,YACvB,iBAAiB,QACjB,eAAe,gBACf,OAAO,aAAa,SAAA,KAAc,cAClC,YAAY,gBACZ,OAAO,aAAa,MAAA,KAAW;IAEnC,IAAI,CAAC,kBAAkB;QACrB,OAAO,0BAAgC;IACzC;IAEA,MAAM,kBAAkB;IAMxB,eAAA,CAAgB,+BAA8B,GAAI;IAElD,CAAC;QACC,IAAI;YAEF,MAAM,SAAS,aAAa,SAAA;YAE5B,MAAO,KAAM;gBACX,MAAM,EAAE,KAAA,EAAO,IAAA,EAAK,GAAI,MAAM,OAAO,IAAA;gBACrC,IAAI,MAAM;oBACR;gBACF;gBAGA,eAAA,CAAgB,+BAA8B,GAAI;gBAClD,IAAI,OAAO,UAAU,UAAU;oBAC7B,gBAAgB,MAAA,CAAO;gBACzB,OAAO;oBACL,gBAAgB,MAAA,CAAO;gBACzB;gBAEA,eAAA,CAAgB,+BAA8B,GAAI;YACpD;YAEA,eAAA,CAAgB,+BAA8B,GAAI;YAClD,gBAAgB,IAAA;QAClB,EAAA,OAAS,GAAG;YACV,eAAA,CAAgB,+BAA8B,GAAI;YAClD,gBAAgB,KAAA,CAAM;QACxB;IACF,CAAA;IAEA,OAAO;AACT;AAuDA,SAAS,0BAA4C,YAAA;IACnD,IAAI,SAAS;IACb,IAAI,SAAS;IACb,IAAI,aAAa;IAEjB,IAAI,eAAe;IACnB,IAAI;IACJ,IAAI,iBACF,WAAW,OAAA;IACb,IAAI;IAEJ,SAAS,aAAa,MAAA;QACpB,IAAI,QAAQ;YACV,MAAM,IAAI,MAAM,SAAS;QAC3B;QACA,IAAI,QAAQ;YACV,MAAM,IAAI,MACR,SAAS;QAEb;IACF;IAEA,IAAI;IACJ,SAAS;QACP,wCAA4C;YAC1C,IAAI,gBAAgB;gBAClB,aAAa;YACf;YACA,iBAAiB,WAAW;gBAC1B,QAAQ,IAAA,CACN;YAEJ,GAAG;QACL;IACF;IACA;IAEA,SAAS,cAAc,YAAA;QAErB,IAAI;QAEJ,IAAI,iBAAiB,KAAA,GAAW;YAC9B,OAAO;gBAAE,OAAO;YAAa;QAC/B,OAAO;YACL,IAAI,qBAAqB,CAAC,cAAc;gBACtC,OAAO;oBAAE,MAAM;gBAAkB;YACnC,OAAO;gBACL,OAAO;oBAAE,MAAM;gBAAa;YAC9B;QACF;QAEA,IAAI,gBAAgB;YAClB,KAAK,IAAA,GAAO;QACd;QAEA,IAAI,cAAc;YAChB,KAAK,IAAA,GAAO;QACd;QAEA,OAAO;IACT;IAGA,SAAS,kBAAkB,KAAA;QAEzB,oBAAoB,KAAA;QACpB,IAAI,OAAO,UAAU,UAAU;YAC7B,IAAI,OAAO,iBAAiB,UAAU;gBACpC,IAAI,MAAM,UAAA,CAAW,eAAe;oBAClC,oBAAoB;wBAAC;wBAAG,MAAM,KAAA,CAAM,aAAa,MAAM;qBAAC;gBAC1D;YACF;QACF;QAEA,eAAe;IACjB;IAEA,MAAMA,cAA2C;QAC/C,IAAA,CAAK,+BAA8B,EAAE,MAAgB;YACnD,SAAS;QACX;QACA,IAAI,SAAQ;YACV,OAAO,cAAc;QACvB;QACA,QAAO,KAAA;YACL,aAAa;YAEb,MAAM,kBAAkB,WAAW,OAAA;YACnC,aAAa;YAEb,kBAAkB;YAClB,iBAAiB,WAAW,OAAA;YAC5B,gBAAgB;YAEhB;YAEA,OAAOA;QACT;QACA,QAAO,KAAA;YACL,aAAa;YAEb,IACE,OAAO,iBAAiB,YACxB,OAAO,iBAAiB,aACxB;gBACA,MAAM,IAAI,MACR,CAAA,wDAAA,EAA2D,OAAO,aAAY,CAAA;YAElF;YACA,IAAI,OAAO,UAAU,UAAU;gBAC7B,MAAM,IAAI,MACR,CAAA,gDAAA,EAAmD,OAAO,MAAK,CAAA;YAEnE;YAEA,MAAM,kBAAkB,WAAW,OAAA;YACnC,aAAa;YAEb,IAAI,OAAO,iBAAiB,UAAU;gBACpC,oBAAoB;oBAAC;oBAAG;iBAAK;gBAC5B,eAA0B,eAAe;YAC5C,OAAO;gBACL,oBAAoB,KAAA;gBACpB,eAAe;YACjB;YAEA,iBAAiB,WAAW,OAAA;YAC5B,gBAAgB;YAEhB;YAEA,OAAOA;QACT;QACA,OAAM,KAAA;YACJ,aAAa;YAEb,IAAI,gBAAgB;gBAClB,aAAa;YACf;YACA,SAAS;YACT,eAAe;YACf,iBAAiB,KAAA;YAEjB,WAAW,OAAA,CAAQ;gBAAE;YAAM;YAE3B,OAAOA;QACT;QACA,MAAA,GAAQ,IAAA;YACN,aAAa;YAEb,IAAI,gBAAgB;gBAClB,aAAa;YACf;YACA,SAAS;YACT,iBAAiB,KAAA;YAEjB,IAAI,KAAK,MAAA,EAAQ;gBACf,kBAAkB,IAAA,CAAK,EAAE;gBACzB,WAAW,OAAA,CAAQ;gBACnB,OAAOA;YACT;YAEA,WAAW,OAAA,CAAQ,CAAC;YAEpB,OAAOA;QACT;IACF;IAEA,OAAOA;AACT;AAoBO,SAAS,OAOd,OAAA;IAgDA,MAAM,KAAK,mBAAmB,QAAQ,OAAO;IAG7C,MAAM,OAAO,QAAQ,IAAA,GACjB,QAAQ,IAAA,GACR,CAAC,EAAE,OAAA,EAAQ,GAA2B;IAE1C,MAAM,YAAY,QAAQ,SAAA,GACtB,OAAO,OAAA,CAAQ,QAAQ,SAAS,EAAE,GAAA,CAChC,CAAC,CAAC,MAAM,EAAE,WAAA,EAAa,UAAA,EAAY,CAAA;QACjC,OAAO;YACL;YACA;YACA,YAAYC,CAAAA,GAAAA,mLAAAA,CAAAA,UAAAA,EAAgB;QAC9B;IACF,KAEF,KAAA;IAEJ,MAAM,QAAQ,QAAQ,KAAA,GAClB,OAAO,OAAA,CAAQ,QAAQ,KAAK,EAAE,GAAA,CAC5B,CAAC,CAAC,MAAM,EAAE,WAAA,EAAa,UAAA,EAAY,CAAA;QACjC,OAAO;YACL,MAAM;YACN,UAAU;gBACR;gBACA;gBACA,YAAYA,CAAAA,GAAAA,mLAAAA,CAAAA,UAAAA,EAAgB;YAI9B;QACF;IACF,KAEF,KAAA;IAEJ,IAAI,aAAa,OAAO;QACtB,MAAM,IAAI,MACR;IAEJ;IAEA,IAAI;IAEJ,eAAe,aACb,IAAA,EACA,QAAA,EACA,GAAA;QAEA,IAAI,CAAC,UAAU;QAEf,MAAM,aAAa;QAEnB,IAAI,UAAU;YACZ,WAAW,SAAS,IAAA,CAAK,IAAM,WAAW,OAAO;QACnD,OAAO;YACL,WAAW,WAAW,OAAA;QACxB;QAEA,MAAM,QAAQ,SAAS;QACvB,IACE,iBAAiB,WAChB,SACC,OAAO,UAAU,YACjB,UAAU,SACV,OAAO,MAAM,IAAA,KAAS,YACxB;YACA,MAAM,OAAO,MAAO;YACpB,IAAI,MAAA,CAAO;YACX,WAAW,OAAA,CAAQ,KAAA;QACrB,OAAA,IACE,SACA,OAAO,UAAU,YACjB,OAAO,aAAA,IAAiB,OACxB;YACA,MAAM,KAAK;YAKX,MAAO,KAAM;gBACX,MAAM,EAAE,IAAA,EAAM,OAAAC,MAAAA,EAAM,GAAI,MAAM,GAAG,IAAA;gBACjC,IAAI,MAAA,CAAOA;gBACX,IAAI,MAAM;YACZ;YACA,WAAW,OAAA,CAAQ,KAAA;QACrB,OAAA,IAAW,SAAS,OAAO,UAAU,YAAY,OAAO,QAAA,IAAY,OAAO;YACzE,MAAM,KAAK;YACX,MAAO,KAAM;gBACX,MAAM,EAAE,IAAA,EAAM,OAAAA,MAAAA,EAAM,GAAI,GAAG,IAAA;gBAC3B,IAAI,MAAA,CAAOA;gBACX,IAAI,MAAM;YACZ;YACA,WAAW,OAAA,CAAQ,KAAA;QACrB,OAAO;YACL,IAAI,MAAA,CAAO;YACX,WAAW,OAAA,CAAQ,KAAA;QACrB;IACF;IAEA,CAAC;QACC,IAAI,cAAc;QAClB,IAAI,UAAU;QAEd,cACE,aACG,MAAM,QAAQ,QAAA,CAAS,IAAA,CAAK,WAAA,CAAY,MAAA,CAAO;YAC9C,OAAO,QAAQ,KAAA;YACf,UAAU,QAAQ,QAAA;YAClB,aAAa,QAAQ,WAAA;YACrB,QAAQ;YACR,GAAI,YACA;gBACE;YACF,IACA,CAAC,CAAA;YACL,GAAI,QACA;gBACE;YACF,IACA,CAAC,CAAA;QACP,IACA;YACE,GAAI,YACA;gBACE,MAAM,6BAA4B,mBAAA;oBAxoBlD,IAAA,IAAA;oBAyoBkB,cAAc;oBACd,aACE,oBAAoB,SAAA,EAAA,CACpB,KAAA,CAAA,KAAA,QAAQ,SAAA,KAAR,OAAA,KAAA,IAAA,EAAA,CAAoB,oBAAoB,IAAA,CAAA,KAAxC,OAAA,KAAA,IAAA,GACI,MAAA,EACJ;gBAEJ;YACF,IACA,CAAC,CAAA;YACL,GAAI,QACA;gBACE,MAAM,yBAAwB,eAAA;oBArpB9C,IAAA,IAAA;oBAspBkB,cAAc;oBAGd,KAAA,MAAW,QAAQ,gBAAgB,KAAA,CAAO;wBACxC,aACE,KAAK,IAAA,CAAK,SAAA,EAAA,CACV,KAAA,CAAA,KAAA,QAAQ,KAAA,KAAR,OAAA,KAAA,IAAA,EAAA,CAAgB,KAAK,IAAA,CAAK,IAAA,CAAA,KAA1B,OAAA,KAAA,IAAA,GAAwC,MAAA,EACxC;oBAEJ;gBACF;YACF,IACA,CAAC,CAAA;YACL,QAAO,KAAA;gBACL,WAAW;gBACX,aAAa;oBAAE;oBAAS,MAAM;oBAAO,OAAO;gBAAM,GAAG,MAAM;YAC7D;YACA,MAAM;gBACJ,IAAI,aAAa;oBACf,MAAM;oBACN,GAAG,IAAA;oBACH;gBACF;gBAEA,aAAa;oBAAE;oBAAS,MAAM;gBAAK,GAAG,MAAM;gBAC5C,MAAM;gBACN,GAAG,IAAA;YACL;QACF;IAGN,CAAA;IAEA,OAAO,GAAG,KAAA;AACZ;;;AiBjnBA,IAAM,sBAAkC,CAAC,EAAE,OAAA,EAAQ,GACjD;AAKF,eAAsB,SAEpB,EACA,KAAA,EACA,KAAA,EACA,UAAA,EACA,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,OAAA,EACA,IAAA,EACA,QAAA,EACA,GAAG,UACL;IAqDE,IAAI,OAAO,UAAU,UAAU;QAC7B,MAAM,IAAI,MACR;IAEJ;IACA,IAAI,eAAe,UAAU;QAC3B,MAAM,IAAI,MACR;IAEJ;IACA,IAAI,cAAc,UAAU;QAC1B,MAAM,IAAI,MACR;IAEJ;IACA,IAAI,OAAO;QACT,KAAA,MAAW,CAAC,MAAM,KAAI,IAAK,OAAO,OAAA,CAAQ,OAAQ;YAChD,IAAI,YAAY,MAAM;gBACpB,MAAM,IAAI,MACR,6GACE;YAEN;QACF;IACF;IAEA,MAAM,KAAK,mBAAmB;IAG9B,MAAM,aAAa,QAAQ;IAE3B,IAAI;IAEJ,eAAe,aACb,IAAA,EACA,QAAA,EACA,GAAA,EACA,WAAW,KAAA;QAEX,IAAI,CAAC,UAAU;QAEf,MAAM,aAAa;QAEnB,IAAI,UAAU;YACZ,WAAW,SAAS,IAAA,CAAK,IAAM,WAAW,OAAO;QACnD,OAAO;YACL,WAAW,WAAW,OAAA;QACxB;QAEA,MAAM,QAAQ,YAAY;QAC1B,IACE,iBAAiB,WAChB,SACC,OAAO,UAAU,YACjB,UAAU,SACV,OAAO,MAAM,IAAA,KAAS,YACxB;YACA,MAAM,OAAO,MAAO;YAEpB,IAAI,UAAU;gBACZ,IAAI,IAAA,CAAK;YACX,OAAO;gBACL,IAAI,MAAA,CAAO;YACb;YAEA,WAAW,OAAA,CAAQ,KAAA;QACrB,OAAA,IACE,SACA,OAAO,UAAU,YACjB,OAAO,aAAA,IAAiB,OACxB;YACA,MAAM,KAAK;YAKX,MAAO,KAAM;gBACX,MAAM,EAAE,IAAA,EAAM,OAAAA,MAAAA,EAAM,GAAI,MAAM,GAAG,IAAA;gBACjC,IAAI,YAAY,MAAM;oBACpB,IAAI,IAAA,CAAKA;gBACX,OAAO;oBACL,IAAI,MAAA,CAAOA;gBACb;gBACA,IAAI,MAAM;YACZ;YACA,WAAW,OAAA,CAAQ,KAAA;QACrB,OAAA,IAAW,SAAS,OAAO,UAAU,YAAY,OAAO,QAAA,IAAY,OAAO;YACzE,MAAM,KAAK;YACX,MAAO,KAAM;gBACX,MAAM,EAAE,IAAA,EAAM,OAAAA,MAAAA,EAAM,GAAI,GAAG,IAAA;gBAC3B,IAAI,YAAY,MAAM;oBACpB,IAAI,IAAA,CAAKA;gBACX,OAAO;oBACL,IAAI,MAAA,CAAOA;gBACb;gBACA,IAAI,MAAM;YACZ;YACA,WAAW,OAAA,CAAQ,KAAA;QACrB,OAAO;YACL,IAAI,UAAU;gBACZ,IAAI,IAAA,CAAK;YACX,OAAO;gBACL,IAAI,MAAA,CAAO;YACb;YACA,WAAW,OAAA,CAAQ,KAAA;QACrB;IACF;IAEA,MAAM,QAAQ,4BAA4B;QAAE;IAAW;IACvD,MAAM,kBAAkB,mBAAmB;QAAE;QAAQ;QAAQ;IAAS;IACtE,MAAM,SAAS,MAAM,MAAM,IACzB,MAAM,QAAA,CAAS;YACb,MAAM;gBACJ,MAAM;gBACN,GAAG,0BAA0B;oBAAE;oBAAO;gBAAW,EAAC;YACpD;YACA,GAAG,oBAAoB,SAAQ;YAC/B,aAAa,gBAAgB,IAAA;YAC7B,QAAQ,6BAA6B;YACrC;YACA;QACF;IAGF,MAAM,CAAC,QAAQ,aAAY,GAAI,OAAO,MAAA,CAAO,GAAA;IAE7C,CAAC;QACC,IAAI;YAGF,IAAI,UAAU;YACd,IAAI,cAAc;YAElB,MAAM,SAAS,aAAa,SAAA;YAC5B,MAAO,KAAM;gBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;gBACrC,IAAI,MAAM;gBAEV,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBAAc;4BACjB,WAAW,MAAM,SAAA;4BACjB,aACE;gCAAC;oCAAE;oCAAS,MAAM;oCAAO,OAAO,MAAM,SAAA;gCAAU;6BAAC,EACjD,YACA;4BAEF;wBACF;oBAEA,KAAK;wBAAmB;4BACtB,cAAc;4BACd;wBACF;oBAEA,KAAK;wBAAa;4BAChB,MAAM,WAAW,MAAM,QAAA;4BAEvB,IAAI,CAAC,OAAO;gCACV,MAAM,IAAI,+KAAA,CAAA,kBAAA,CAAgB;oCAAE;gCAAmB;4BACjD;4BAEA,MAAM,OAAO,KAAA,CAAM,SAAQ;4BAC3B,IAAI,CAAC,MAAM;gCACT,MAAM,IAAI,+KAAA,CAAA,kBAAA,CAAgB;oCACxB;oCACA,gBAAgB,OAAO,IAAA,CAAK;gCAC9B;4BACF;4BAEA,cAAc;4BACd,MAAM,cAAc,CAAA,GAAA,wLAAA,CAAA,gBAAA,EAAc;gCAChC,MAAM,MAAM,IAAA;gCACZ,QAAQ,KAAK,UAAA;4BACf;4BAEA,IAAI,YAAY,OAAA,KAAY,OAAO;gCACjC,MAAM,IAAI,+KAAA,CAAA,4BAAA,CAA0B;oCAClC;oCACA,UAAU,MAAM,IAAA;oCAChB,OAAO,YAAY,KAAA;gCACrB;4BACF;4BAEA,aACE;gCACE,YAAY,KAAA;gCACZ;oCACE;oCACA,YAAY,MAAM,UAAA;gCACpB;6BACF,EACA,KAAK,QAAA,EACL,IACA;4BAGF;wBACF;oBAEA,KAAK;wBAAS;4BACZ,MAAM,MAAM,KAAA;wBACd;oBAEA,KAAK;wBAAU;4BACb,YAAA,OAAA,KAAA,IAAA,SAAW;gCACT,cAAc,MAAM,YAAA;gCACpB,OAAO,8BAA8B,MAAM,KAAK;gCAChD,OAAO,GAAG,KAAA;gCACV,UAAU,OAAO,QAAA;gCACjB,aAAa,OAAO,WAAA;4BACtB;wBACF;gBACF;YACF;YAEA,IAAI,aAAa;gBACf,MAAM;YACR,OAAO;gBACL,aAAa;oBAAC;wBAAE;wBAAS,MAAM;oBAAK;iBAAC,EAAG,YAAY,IAAI;gBACxD,MAAM;YACR;QACF,EAAA,OAAS,OAAO;YAGd,GAAG,KAAA,CAAM;QACX;IACF,CAAA;IAEA,OAAO;QACL,GAAG,MAAA;QACH;QACA,OAAO,GAAG,KAAA;IACZ;AACF;;;;ICxWe;AAAf,0BACE,EACE,MAAA,EACA,OAAA,EACF,EACA,MAAA,EAAA,GACG,IAAA;IAGH,OAAO,MAAM,YACX;QACE,OAAA;QACA;IACF,GACA;QACE,MAAM,SAAS,MAAM,UAAU;QAC/B;QACA,OAAO;YAAC;YAAwC;SAAM;IACxD;AAEJ;AAEA,SAAS,WACP,MAAA,EACA,OAAA;IAEA,OAAO,YAAY,IAAA,CAAK,MAAM;QAAE;QAAQ;IAAQ;AAClD;AAEO,SAAS,SAId,EACA,OAAA,EACA,cAAA,EACA,cAAA,EAEA,YAAA,EACA,YAAA,EACF;IA0CE,MAAM,iBAAuC,CAAC;IAC9C,IAAA,MAAW,QAAQ,QAAS;QAC1B,cAAA,CAAe,KAAI,GAAI,WAAW,OAAA,CAAQ,KAAI,EAAG;YAC/C;QACF;IACF;IAEA,MAAM,qBAAqB,eACvB,WAAW,cAAc,CAAC,KAC1B,KAAA;IAEJ,MAAM,KAA4C,OAAM;QAhH1D,IAAA,IAAA;QAiHI,IAAI,cAAcC,iNAAO;YAIvB,MAAM,IAAI,MACR;QAEJ;QAEA,IAAI,UAAA,CAAU,KAAA,MAAM,cAAA,KAAN,OAAA,KAAwB;QACtC,IAAI,UAAA,CAAU,KAAA,MAAM,cAAA,KAAN,OAAA,KAAwB;QACtC,IAAI,eAAe,KAAA;QAEnB,IAAI,oBAAoB;YACtB,MAAM,CAAC,iBAAiB,WAAU,GAAI,MAAM,mBAAmB;YAC/D,IAAI,eAAe,KAAA,GAAW;gBAC5B,eAAe;gBACf,UAAU;YACZ;QACF;QAEA,OACE,aAAA,GAAAC,CAAAA,GAAAA,iOAAAA,CAAAA,MAAAA,EAAC,mJAAA,CAAA,qBAAA,EAAA;YACC;YACA;YACA,gBAAgB;YAChB,gBAAgB;YAChB,qBAAqB;YAEpB,UAAA,MAAM,QAAA;QAAA;IAGb;IAEA,OAAO;AACT"}},
    {"offset": {"line": 1745, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 1750, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/node_modules/ai/streams/index.ts","/turbopack/[project]/node_modules/ai/core/util/retry-with-exponential-backoff.ts","/turbopack/[project]/node_modules/ai/core/util/delay.ts","/turbopack/[project]/node_modules/ai/core/embed/embed.ts","/turbopack/[project]/node_modules/ai/core/util/split-array.ts","/turbopack/[project]/node_modules/ai/core/embed/embed-many.ts","/turbopack/[project]/node_modules/ai/core/generate-object/generate-object.ts","/turbopack/[project]/node_modules/ai/core/util/detect-image-mimetype.ts","/turbopack/[project]/node_modules/ai/core/prompt/data-content.ts","/turbopack/[project]/node_modules/ai/core/prompt/invalid-message-role-error.ts","/turbopack/[project]/node_modules/ai/core/prompt/convert-to-language-model-prompt.ts","/turbopack/[project]/node_modules/ai/core/prompt/get-validated-prompt.ts","/turbopack/[project]/node_modules/ai/core/prompt/prepare-call-settings.ts","/turbopack/[project]/node_modules/ai/core/types/token-usage.ts","/turbopack/[project]/node_modules/ai/core/util/convert-zod-to-json-schema.ts","/turbopack/[project]/node_modules/ai/core/util/prepare-response-headers.ts","/turbopack/[project]/node_modules/ai/core/generate-object/inject-json-schema-into-system.ts","/turbopack/[project]/node_modules/ai/core/generate-object/stream-object.ts","/turbopack/[project]/node_modules/ai/core/util/async-iterable-stream.ts","/turbopack/[project]/node_modules/ai/core/util/is-non-empty-object.ts","/turbopack/[project]/node_modules/ai/core/prompt/prepare-tools-and-tool-choice.ts","/turbopack/[project]/node_modules/ai/core/telemetry/get-base-telemetry-attributes.ts","/turbopack/[project]/node_modules/ai/core/telemetry/get-tracer.ts","/turbopack/[project]/node_modules/ai/core/telemetry/noop-tracer.ts","/turbopack/[project]/node_modules/ai/core/telemetry/record-span.ts","/turbopack/[project]/node_modules/ai/core/generate-text/tool-call.ts","/turbopack/[project]/node_modules/ai/core/generate-text/generate-text.ts","/turbopack/[project]/node_modules/ai/core/generate-text/run-tools-transformation.ts","/turbopack/[project]/node_modules/ai/core/generate-text/stream-text.ts","/turbopack/[project]/node_modules/ai/core/prompt/convert-to-core-messages.ts","/turbopack/[project]/node_modules/ai/core/registry/invalid-model-id-error.ts","/turbopack/[project]/node_modules/ai/core/registry/no-such-model-error.ts","/turbopack/[project]/node_modules/ai/core/registry/no-such-provider-error.ts","/turbopack/[project]/node_modules/ai/core/registry/provider-registry.ts","/turbopack/[project]/node_modules/ai/core/tool/tool.ts","/turbopack/[project]/node_modules/ai/core/types/errors.ts","/turbopack/[project]/node_modules/ai/core/util/cosine-similarity.ts","/turbopack/[project]/node_modules/ai/streams/ai-stream.ts","/turbopack/[project]/node_modules/ai/streams/stream-data.ts","/turbopack/[project]/node_modules/ai/streams/anthropic-stream.ts","/turbopack/[project]/node_modules/ai/streams/assistant-response.ts","/turbopack/[project]/node_modules/ai/streams/aws-bedrock-stream.ts","/turbopack/[project]/node_modules/ai/streams/cohere-stream.ts","/turbopack/[project]/node_modules/ai/streams/google-generative-ai-stream.ts","/turbopack/[project]/node_modules/ai/streams/huggingface-stream.ts","/turbopack/[project]/node_modules/ai/streams/inkeep-stream.ts","/turbopack/[project]/node_modules/ai/streams/langchain-adapter.ts","/turbopack/[project]/node_modules/ai/streams/langchain-stream.ts","/turbopack/[project]/node_modules/ai/streams/mistral-stream.ts","/turbopack/[project]/node_modules/ai/streams/openai-stream.ts","/turbopack/[project]/node_modules/ai/streams/replicate-stream.ts","/turbopack/[project]/node_modules/ai/core/util/merge-streams.ts","/turbopack/[project]/node_modules/ai/streams/stream-to-response.ts","/turbopack/[project]/node_modules/ai/streams/streaming-text-response.ts"],"sourcesContent":["// forwarding exports from ui-utils:\nexport {\n  formatStreamPart,\n  parseStreamPart,\n  readDataStream,\n  parseComplexResponse,\n} from '@ai-sdk/ui-utils';\nexport type {\n  AssistantStatus,\n  UseAssistantOptions,\n  Message,\n  CreateMessage,\n  DataMessage,\n  AssistantMessage,\n  JSONValue,\n  ChatRequest,\n  ChatRequestOptions,\n  Function,\n  FunctionCall,\n  FunctionCallHandler,\n  ToolInvocation,\n  Tool,\n  ToolCall,\n  ToolCallHandler,\n  ToolChoice,\n  StreamPart,\n  IdGenerator,\n  RequestOptions,\n} from '@ai-sdk/ui-utils';\n\nimport { generateId as generateIdImpl } from '@ai-sdk/provider-utils';\nexport const generateId = generateIdImpl;\n\n/**\n@deprecated Use `generateId` instead.\n */\n// TODO remove nanoid export (breaking change)\nexport const nanoid = generateIdImpl;\n\nexport * from '../core/index';\nexport * from './ai-stream';\nexport * from './anthropic-stream';\nexport * from './assistant-response';\nexport * from './aws-bedrock-stream';\nexport * from './cohere-stream';\nexport * from './google-generative-ai-stream';\nexport * from './huggingface-stream';\nexport * from './inkeep-stream';\nexport * as LangChainAdapter from './langchain-adapter';\nexport * from './langchain-stream';\nexport * from './mistral-stream';\nexport * from './openai-stream';\nexport * from './replicate-stream';\nexport * from './stream-data';\nexport * from './stream-to-response';\nexport * from './streaming-text-response';\n","import { APICallError, RetryError } from '@ai-sdk/provider';\nimport { getErrorMessage, isAbortError } from '@ai-sdk/provider-utils';\nimport { delay } from './delay';\n\nexport type RetryFunction = <OUTPUT>(\n  fn: () => PromiseLike<OUTPUT>,\n) => PromiseLike<OUTPUT>;\n\n/**\nThe `retryWithExponentialBackoff` strategy retries a failed API call with an exponential backoff.\nYou can configure the maximum number of retries, the initial delay, and the backoff factor.\n */\nexport const retryWithExponentialBackoff =\n  ({\n    maxRetries = 2,\n    initialDelayInMs = 2000,\n    backoffFactor = 2,\n  } = {}): RetryFunction =>\n  async <OUTPUT>(f: () => PromiseLike<OUTPUT>) =>\n    _retryWithExponentialBackoff(f, {\n      maxRetries,\n      delayInMs: initialDelayInMs,\n      backoffFactor,\n    });\n\nasync function _retryWithExponentialBackoff<OUTPUT>(\n  f: () => PromiseLike<OUTPUT>,\n  {\n    maxRetries,\n    delayInMs,\n    backoffFactor,\n  }: { maxRetries: number; delayInMs: number; backoffFactor: number },\n  errors: unknown[] = [],\n): Promise<OUTPUT> {\n  try {\n    return await f();\n  } catch (error) {\n    if (isAbortError(error)) {\n      throw error; // don't retry when the request was aborted\n    }\n\n    if (maxRetries === 0) {\n      throw error; // don't wrap the error when retries are disabled\n    }\n\n    const errorMessage = getErrorMessage(error);\n    const newErrors = [...errors, error];\n    const tryNumber = newErrors.length;\n\n    if (tryNumber > maxRetries) {\n      throw new RetryError({\n        message: `Failed after ${tryNumber} attempts. Last error: ${errorMessage}`,\n        reason: 'maxRetriesExceeded',\n        errors: newErrors,\n      });\n    }\n\n    if (\n      error instanceof Error &&\n      APICallError.isAPICallError(error) &&\n      error.isRetryable === true &&\n      tryNumber <= maxRetries\n    ) {\n      await delay(delayInMs);\n      return _retryWithExponentialBackoff(\n        f,\n        { maxRetries, delayInMs: backoffFactor * delayInMs, backoffFactor },\n        newErrors,\n      );\n    }\n\n    if (tryNumber === 1) {\n      throw error; // don't wrap the error when a non-retryable error occurs on the first try\n    }\n\n    throw new RetryError({\n      message: `Failed after ${tryNumber} attempts with non-retryable error: '${errorMessage}'`,\n      reason: 'errorNotRetryable',\n      errors: newErrors,\n    });\n  }\n}\n","export async function delay(delayInMs: number): Promise<void> {\n  return new Promise(resolve => setTimeout(resolve, delayInMs));\n}\n","import { Embedding, EmbeddingModel } from '../types';\nimport { EmbeddingTokenUsage } from '../types/token-usage';\nimport { retryWithExponentialBackoff } from '../util/retry-with-exponential-backoff';\n\n/**\nEmbed a value using an embedding model. The type of the value is defined by the embedding model.\n\n@param model - The embedding model to use.\n@param value - The value that should be embedded.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@returns A result object that contains the embedding, the value, and additional information.\n */\nexport async function embed<VALUE>({\n  model,\n  value,\n  maxRetries,\n  abortSignal,\n  headers,\n}: {\n  /**\nThe embedding model to use.\n     */\n  model: EmbeddingModel<VALUE>;\n\n  /**\nThe value that should be embedded.\n   */\n  value: VALUE;\n\n  /**\nMaximum number of retries per embedding model call. Set to 0 to disable retries.\n\n@default 2\n   */\n  maxRetries?: number;\n\n  /**\nAbort signal.\n */\n  abortSignal?: AbortSignal;\n\n  /**\nAdditional headers to include in the request.\nOnly applicable for HTTP-based providers.\n */\n  headers?: Record<string, string>;\n}): Promise<EmbedResult<VALUE>> {\n  const retry = retryWithExponentialBackoff({ maxRetries });\n\n  const modelResponse = await retry(() =>\n    model.doEmbed({ values: [value], abortSignal, headers }),\n  );\n\n  return new EmbedResult({\n    value,\n    embedding: modelResponse.embeddings[0],\n    usage: modelResponse.usage ?? { tokens: NaN },\n    rawResponse: modelResponse.rawResponse,\n  });\n}\n\n/**\nThe result of a `embed` call.\nIt contains the embedding, the value, and additional information.\n */\nexport class EmbedResult<VALUE> {\n  /**\nThe value that was embedded.\n   */\n  readonly value: VALUE;\n\n  /**\nThe embedding of the value.\n  */\n  readonly embedding: Embedding;\n\n  /**\nThe embedding token usage.\n  */\n  readonly usage: EmbeddingTokenUsage;\n\n  /**\nOptional raw response data.\n   */\n  readonly rawResponse?: {\n    /**\nResponse headers.\n     */\n    headers?: Record<string, string>;\n  };\n\n  constructor(options: {\n    value: VALUE;\n    embedding: Embedding;\n    usage: EmbeddingTokenUsage;\n    rawResponse?: { headers?: Record<string, string> };\n  }) {\n    this.value = options.value;\n    this.embedding = options.embedding;\n    this.usage = options.usage;\n    this.rawResponse = options.rawResponse;\n  }\n}\n","/**\n * Splits an array into chunks of a specified size.\n *\n * @template T - The type of elements in the array.\n * @param {T[]} array - The array to split.\n * @param {number} chunkSize - The size of each chunk.\n * @returns {T[][]} - A new array containing the chunks.\n */\nexport function splitArray<T>(array: T[], chunkSize: number): T[][] {\n  if (chunkSize <= 0) {\n    throw new Error('chunkSize must be greater than 0');\n  }\n\n  const result = [];\n  for (let i = 0; i < array.length; i += chunkSize) {\n    result.push(array.slice(i, i + chunkSize));\n  }\n\n  return result;\n}\n","import { Embedding, EmbeddingModel } from '../types';\nimport { EmbeddingTokenUsage } from '../types/token-usage';\nimport { retryWithExponentialBackoff } from '../util/retry-with-exponential-backoff';\nimport { splitArray } from '../util/split-array';\n\n/**\nEmbed several values using an embedding model. The type of the value is defined \nby the embedding model.\n\n`embedMany` automatically splits large requests into smaller chunks if the model\nhas a limit on how many embeddings can be generated in a single call.\n\n@param model - The embedding model to use.\n@param values - The values that should be embedded.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@returns A result object that contains the embeddings, the value, and additional information.\n */\nexport async function embedMany<VALUE>({\n  model,\n  values,\n  maxRetries,\n  abortSignal,\n  headers,\n}: {\n  /**\nThe embedding model to use.\n     */\n  model: EmbeddingModel<VALUE>;\n\n  /**\nThe values that should be embedded.\n   */\n  values: Array<VALUE>;\n\n  /**\nMaximum number of retries per embedding model call. Set to 0 to disable retries.\n\n@default 2\n   */\n  maxRetries?: number;\n\n  /**\nAbort signal.\n */\n  abortSignal?: AbortSignal;\n\n  /**\nAdditional headers to include in the request.\nOnly applicable for HTTP-based providers.\n */\n  headers?: Record<string, string>;\n}): Promise<EmbedManyResult<VALUE>> {\n  const retry = retryWithExponentialBackoff({ maxRetries });\n  const maxEmbeddingsPerCall = model.maxEmbeddingsPerCall;\n\n  // the model has not specified limits on\n  // how many embeddings can be generated in a single call\n  if (maxEmbeddingsPerCall == null) {\n    const modelResponse = await retry(() =>\n      model.doEmbed({ values, abortSignal, headers }),\n    );\n\n    return new EmbedManyResult({\n      values,\n      embeddings: modelResponse.embeddings,\n      usage: modelResponse.usage ?? { tokens: NaN },\n    });\n  }\n\n  // split the values into chunks that are small enough for the model:\n  const valueChunks = splitArray(values, maxEmbeddingsPerCall);\n\n  // serially embed the chunks:\n  const embeddings = [];\n  let tokens = 0;\n\n  for (const chunk of valueChunks) {\n    const modelResponse = await retry(() =>\n      model.doEmbed({ values: chunk, abortSignal, headers }),\n    );\n    embeddings.push(...modelResponse.embeddings);\n    tokens += modelResponse.usage?.tokens ?? NaN;\n  }\n\n  return new EmbedManyResult({ values, embeddings, usage: { tokens } });\n}\n\n/**\nThe result of a `embedMany` call.\nIt contains the embeddings, the values, and additional information.\n */\nexport class EmbedManyResult<VALUE> {\n  /**\nThe values that were embedded.\n   */\n  readonly values: Array<VALUE>;\n\n  /**\nThe embeddings. They are in the same order as the values.\n  */\n  readonly embeddings: Array<Embedding>;\n\n  /**\nThe embedding token usage.\n  */\n  readonly usage: EmbeddingTokenUsage;\n\n  constructor(options: {\n    values: Array<VALUE>;\n    embeddings: Array<Embedding>;\n    usage: EmbeddingTokenUsage;\n  }) {\n    this.values = options.values;\n    this.embeddings = options.embeddings;\n    this.usage = options.usage;\n  }\n}\n","import { NoObjectGeneratedError } from '@ai-sdk/provider';\nimport { safeParseJSON } from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { CallSettings } from '../prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../prompt/convert-to-language-model-prompt';\nimport { getValidatedPrompt } from '../prompt/get-validated-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { Prompt } from '../prompt/prompt';\nimport { CallWarning, FinishReason, LanguageModel, LogProbs } from '../types';\nimport {\n  CompletionTokenUsage,\n  calculateCompletionTokenUsage,\n} from '../types/token-usage';\nimport { convertZodToJSONSchema } from '../util/convert-zod-to-json-schema';\nimport { prepareResponseHeaders } from '../util/prepare-response-headers';\nimport { retryWithExponentialBackoff } from '../util/retry-with-exponential-backoff';\nimport { injectJsonSchemaIntoSystem } from './inject-json-schema-into-system';\n\n/**\nGenerate a structured, typed object for a given prompt and schema using a language model.\n\nThis function does not stream the output. If you want to stream the output, use `streamObject` instead.\n\n@param model - The language model to use.\n\n@param schema - The schema of the object that the model should generate.\n@param mode - The mode to use for object generation. Not all models support all modes. Defaults to 'auto'.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting. \nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param presencePenalty - Presence penalty setting. \nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@returns \nA result object that contains the generated object, the finish reason, the token usage, and additional information.\n */\nexport async function generateObject<T>({\n  model,\n  schema,\n  mode,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe schema of the object that the model should generate.\n     */\n    schema: z.Schema<T>;\n\n    /**\nThe mode to use for object generation.\n\nThe Zod schema is converted in a JSON schema and used in one of the following ways\n\n- 'auto': The provider will choose the best mode for the model.\n- 'tool': A tool with the JSON schema as parameters is is provided and the provider is instructed to use it.\n- 'json': The JSON schema and a instruction is injected into the prompt. If the provider supports JSON mode, it is enabled.\n- 'grammar': The provider is instructed to converted the JSON schema into a provider specific grammar and use it to select the output tokens.\n\nPlease note that most providers do not support all modes.\n\nDefault and recommended: 'auto' (best mode for the model).\n     */\n    mode?: 'auto' | 'json' | 'tool' | 'grammar';\n  }): Promise<GenerateObjectResult<T>> {\n  const retry = retryWithExponentialBackoff({ maxRetries });\n  const jsonSchema = convertZodToJSONSchema(schema);\n\n  // use the default provider mode when the mode is set to 'auto' or unspecified\n  if (mode === 'auto' || mode == null) {\n    mode = model.defaultObjectGenerationMode;\n  }\n\n  let result: string;\n  let finishReason: FinishReason;\n  let usage: Parameters<typeof calculateCompletionTokenUsage>[0];\n  let warnings: CallWarning[] | undefined;\n  let rawResponse: { headers?: Record<string, string> } | undefined;\n  let logprobs: LogProbs | undefined;\n\n  switch (mode) {\n    case 'json': {\n      const validatedPrompt = getValidatedPrompt({\n        system: injectJsonSchemaIntoSystem({ system, schema: jsonSchema }),\n        prompt,\n        messages,\n      });\n\n      const generateResult = await retry(() => {\n        return model.doGenerate({\n          mode: { type: 'object-json' },\n          ...prepareCallSettings(settings),\n          inputFormat: validatedPrompt.type,\n          prompt: convertToLanguageModelPrompt(validatedPrompt),\n          abortSignal,\n          headers,\n        });\n      });\n\n      if (generateResult.text === undefined) {\n        throw new NoObjectGeneratedError();\n      }\n\n      result = generateResult.text;\n      finishReason = generateResult.finishReason;\n      usage = generateResult.usage;\n      warnings = generateResult.warnings;\n      rawResponse = generateResult.rawResponse;\n      logprobs = generateResult.logprobs;\n\n      break;\n    }\n\n    case 'grammar': {\n      const validatedPrompt = getValidatedPrompt({\n        system: injectJsonSchemaIntoSystem({ system, schema: jsonSchema }),\n        prompt,\n        messages,\n      });\n\n      const generateResult = await retry(() =>\n        model.doGenerate({\n          mode: { type: 'object-grammar', schema: jsonSchema },\n          ...prepareCallSettings(settings),\n          inputFormat: validatedPrompt.type,\n          prompt: convertToLanguageModelPrompt(validatedPrompt),\n          abortSignal,\n        }),\n      );\n\n      if (generateResult.text === undefined) {\n        throw new NoObjectGeneratedError();\n      }\n\n      result = generateResult.text;\n      finishReason = generateResult.finishReason;\n      usage = generateResult.usage;\n      warnings = generateResult.warnings;\n      rawResponse = generateResult.rawResponse;\n      logprobs = generateResult.logprobs;\n\n      break;\n    }\n\n    case 'tool': {\n      const validatedPrompt = getValidatedPrompt({\n        system,\n        prompt,\n        messages,\n      });\n\n      const generateResult = await retry(() =>\n        model.doGenerate({\n          mode: {\n            type: 'object-tool',\n            tool: {\n              type: 'function',\n              name: 'json',\n              description: 'Respond with a JSON object.',\n              parameters: jsonSchema,\n            },\n          },\n          ...prepareCallSettings(settings),\n          inputFormat: validatedPrompt.type,\n          prompt: convertToLanguageModelPrompt(validatedPrompt),\n          abortSignal,\n        }),\n      );\n\n      const functionArgs = generateResult.toolCalls?.[0]?.args;\n\n      if (functionArgs === undefined) {\n        throw new NoObjectGeneratedError();\n      }\n\n      result = functionArgs;\n      finishReason = generateResult.finishReason;\n      usage = generateResult.usage;\n      warnings = generateResult.warnings;\n      rawResponse = generateResult.rawResponse;\n      logprobs = generateResult.logprobs;\n\n      break;\n    }\n\n    case undefined: {\n      throw new Error('Model does not have a default object generation mode.');\n    }\n\n    default: {\n      const _exhaustiveCheck: never = mode;\n      throw new Error(`Unsupported mode: ${_exhaustiveCheck}`);\n    }\n  }\n\n  const parseResult = safeParseJSON({ text: result, schema });\n\n  if (!parseResult.success) {\n    throw parseResult.error;\n  }\n\n  return new GenerateObjectResult({\n    object: parseResult.value,\n    finishReason,\n    usage: calculateCompletionTokenUsage(usage),\n    warnings,\n    rawResponse,\n    logprobs,\n  });\n}\n\n/**\nThe result of a `generateObject` call.\n */\nexport class GenerateObjectResult<T> {\n  /**\nThe generated object (typed according to the schema).\n   */\n  readonly object: T;\n\n  /**\nThe reason why the generation finished.\n   */\n  readonly finishReason: FinishReason;\n\n  /**\nThe token usage of the generated text.\n   */\n  readonly usage: CompletionTokenUsage;\n\n  /**\nWarnings from the model provider (e.g. unsupported settings)\n   */\n  readonly warnings: CallWarning[] | undefined;\n\n  /**\nOptional raw response data.\n   */\n  rawResponse?: {\n    /**\nResponse headers.\n */\n    headers?: Record<string, string>;\n  };\n\n  /**\nLogprobs for the completion. \n`undefined` if the mode does not support logprobs or if was not enabled\n   */\n  readonly logprobs: LogProbs | undefined;\n\n  constructor(options: {\n    object: T;\n    finishReason: FinishReason;\n    usage: CompletionTokenUsage;\n    warnings: CallWarning[] | undefined;\n    rawResponse?: {\n      headers?: Record<string, string>;\n    };\n    logprobs: LogProbs | undefined;\n  }) {\n    this.object = options.object;\n    this.finishReason = options.finishReason;\n    this.usage = options.usage;\n    this.warnings = options.warnings;\n    this.rawResponse = options.rawResponse;\n    this.logprobs = options.logprobs;\n  }\n\n  /**\nConverts the object to a JSON response.\nThe response will have a status code of 200 and a content type of `application/json; charset=utf-8`.\n   */\n  toJsonResponse(init?: ResponseInit): Response {\n    return new Response(JSON.stringify(this.object), {\n      status: init?.status ?? 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'application/json; charset=utf-8',\n      }),\n    });\n  }\n}\n\n/**\n * @deprecated Use `generateObject` instead.\n */\nexport const experimental_generateObject = generateObject;\n","const mimeTypeSignatures = [\n  { mimeType: 'image/gif' as const, bytes: [0x47, 0x49, 0x46] },\n  { mimeType: 'image/png' as const, bytes: [0x89, 0x50, 0x4e, 0x47] },\n  { mimeType: 'image/jpeg' as const, bytes: [0xff, 0xd8] },\n  { mimeType: 'image/webp' as const, bytes: [0x52, 0x49, 0x46, 0x46] },\n];\n\nexport function detectImageMimeType(\n  image: Uint8Array,\n): 'image/jpeg' | 'image/png' | 'image/gif' | 'image/webp' | undefined {\n  for (const { bytes, mimeType } of mimeTypeSignatures) {\n    if (\n      image.length >= bytes.length &&\n      bytes.every((byte, index) => image[index] === byte)\n    ) {\n      return mimeType;\n    }\n  }\n\n  return undefined;\n}\n","import { InvalidDataContentError } from '@ai-sdk/provider';\nimport {\n  convertBase64ToUint8Array,\n  convertUint8ArrayToBase64,\n} from '@ai-sdk/provider-utils';\n\n/**\nData content. Can either be a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer.\n */\nexport type DataContent = string | Uint8Array | ArrayBuffer | Buffer;\n\n/**\nConverts data content to a base64-encoded string.\n\n@param content - Data content to convert.\n@returns Base64-encoded string.\n*/\nexport function convertDataContentToBase64String(content: DataContent): string {\n  if (typeof content === 'string') {\n    return content;\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return convertUint8ArrayToBase64(new Uint8Array(content));\n  }\n\n  return convertUint8ArrayToBase64(content);\n}\n\n/**\nConverts data content to a Uint8Array.\n\n@param content - Data content to convert.\n@returns Uint8Array.\n */\nexport function convertDataContentToUint8Array(\n  content: DataContent,\n): Uint8Array {\n  if (content instanceof Uint8Array) {\n    return content;\n  }\n\n  if (typeof content === 'string') {\n    try {\n      return convertBase64ToUint8Array(content);\n    } catch (error) {\n      throw new InvalidDataContentError({\n        message:\n          'Invalid data content. Content string is not a base64-encoded image.',\n        content,\n        cause: error,\n      });\n    }\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return new Uint8Array(content);\n  }\n\n  throw new InvalidDataContentError({ content });\n}\n","export class InvalidMessageRoleError extends Error {\n  readonly role: string;\n\n  constructor({\n    role,\n    message = `Invalid message role: '${role}'. Must be one of: \"system\", \"user\", \"assistant\", \"tool\".`,\n  }: {\n    role: string;\n    message?: string;\n  }) {\n    super(message);\n\n    this.name = 'AI_InvalidMessageRoleError';\n\n    this.role = role;\n  }\n\n  static isInvalidMessageRoleError(\n    error: unknown,\n  ): error is InvalidMessageRoleError {\n    return (\n      error instanceof Error &&\n      error.name === 'AI_InvalidMessageRoleError' &&\n      typeof (error as InvalidMessageRoleError).role === 'string'\n    );\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      role: this.role,\n    };\n  }\n}\n","import {\n  LanguageModelV1ImagePart,\n  LanguageModelV1Message,\n  LanguageModelV1Prompt,\n  LanguageModelV1TextPart,\n} from '@ai-sdk/provider';\nimport { CoreMessage } from '../prompt/message';\nimport { detectImageMimeType } from '../util/detect-image-mimetype';\nimport { convertDataContentToUint8Array } from './data-content';\nimport { ValidatedPrompt } from './get-validated-prompt';\nimport { InvalidMessageRoleError } from './invalid-message-role-error';\nimport { getErrorMessage } from '@ai-sdk/provider-utils';\n\nexport function convertToLanguageModelPrompt(\n  prompt: ValidatedPrompt,\n): LanguageModelV1Prompt {\n  const languageModelMessages: LanguageModelV1Prompt = [];\n\n  if (prompt.system != null) {\n    languageModelMessages.push({ role: 'system', content: prompt.system });\n  }\n\n  const promptType = prompt.type;\n  switch (promptType) {\n    case 'prompt': {\n      languageModelMessages.push({\n        role: 'user',\n        content: [{ type: 'text', text: prompt.prompt }],\n      });\n      break;\n    }\n\n    case 'messages': {\n      languageModelMessages.push(\n        ...prompt.messages.map(convertToLanguageModelMessage),\n      );\n      break;\n    }\n\n    default: {\n      const _exhaustiveCheck: never = promptType;\n      throw new Error(`Unsupported prompt type: ${_exhaustiveCheck}`);\n    }\n  }\n\n  return languageModelMessages;\n}\n\nexport function convertToLanguageModelMessage(\n  message: CoreMessage,\n): LanguageModelV1Message {\n  const role = message.role;\n  switch (role) {\n    case 'system': {\n      return { role: 'system', content: message.content };\n    }\n\n    case 'user': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'user',\n          content: [{ type: 'text', text: message.content }],\n        };\n      }\n\n      return {\n        role: 'user',\n        content: message.content.map(\n          (part): LanguageModelV1TextPart | LanguageModelV1ImagePart => {\n            switch (part.type) {\n              case 'text': {\n                return part;\n              }\n\n              case 'image': {\n                if (part.image instanceof URL) {\n                  return {\n                    type: 'image',\n                    image: part.image,\n                    mimeType: part.mimeType,\n                  };\n                }\n\n                // try to convert string image parts to urls\n                if (typeof part.image === 'string') {\n                  try {\n                    const url = new URL(part.image);\n\n                    switch (url.protocol) {\n                      case 'http:':\n                      case 'https:': {\n                        return {\n                          type: 'image',\n                          image: url,\n                          mimeType: part.mimeType,\n                        };\n                      }\n                      case 'data:': {\n                        try {\n                          const [header, base64Content] = part.image.split(',');\n                          const mimeType = header.split(';')[0].split(':')[1];\n\n                          if (mimeType == null || base64Content == null) {\n                            throw new Error('Invalid data URL format');\n                          }\n\n                          return {\n                            type: 'image',\n                            image:\n                              convertDataContentToUint8Array(base64Content),\n                            mimeType,\n                          };\n                        } catch (error) {\n                          throw new Error(\n                            `Error processing data URL: ${getErrorMessage(\n                              message,\n                            )}`,\n                          );\n                        }\n                      }\n                      default: {\n                        throw new Error(\n                          `Unsupported URL protocol: ${url.protocol}`,\n                        );\n                      }\n                    }\n                  } catch (_ignored) {\n                    // not a URL\n                  }\n                }\n\n                const imageUint8 = convertDataContentToUint8Array(part.image);\n\n                return {\n                  type: 'image',\n                  image: imageUint8,\n                  mimeType: part.mimeType ?? detectImageMimeType(imageUint8),\n                };\n              }\n            }\n          },\n        ),\n      };\n    }\n\n    case 'assistant': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'assistant',\n          content: [{ type: 'text', text: message.content }],\n        };\n      }\n\n      return {\n        role: 'assistant',\n        content: message.content.filter(\n          // remove empty text parts:\n          part => part.type !== 'text' || part.text !== '',\n        ),\n      };\n    }\n\n    case 'tool': {\n      return message;\n    }\n\n    default: {\n      const _exhaustiveCheck: never = role;\n      throw new InvalidMessageRoleError({ role: _exhaustiveCheck });\n    }\n  }\n}\n","import { InvalidPromptError } from '@ai-sdk/provider';\nimport { CoreMessage } from './message';\nimport { Prompt } from './prompt';\n\nexport type ValidatedPrompt =\n  | {\n      type: 'prompt';\n      prompt: string;\n      messages: undefined;\n      system?: string;\n    }\n  | {\n      type: 'messages';\n      prompt: undefined;\n      messages: CoreMessage[];\n      system?: string;\n    };\n\nexport function getValidatedPrompt(prompt: Prompt): ValidatedPrompt {\n  if (prompt.prompt == null && prompt.messages == null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt or messages must be defined',\n    });\n  }\n\n  if (prompt.prompt != null && prompt.messages != null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt and messages cannot be defined at the same time',\n    });\n  }\n\n  if (prompt.messages != null) {\n    for (const message of prompt.messages) {\n      if (message.role === 'system' && typeof message.content !== 'string') {\n        throw new InvalidPromptError({\n          prompt,\n          message: 'system message content must be a string',\n        });\n      }\n    }\n  }\n\n  return prompt.prompt != null\n    ? {\n        type: 'prompt',\n        prompt: prompt.prompt,\n        messages: undefined,\n        system: prompt.system,\n      }\n    : {\n        type: 'messages',\n        prompt: undefined,\n        messages: prompt.messages!, // only possible case bc of checks above\n        system: prompt.system,\n      };\n}\n","import { InvalidArgumentError } from '@ai-sdk/provider';\nimport { CallSettings } from './call-settings';\n\n/**\n * Validates call settings and sets default values.\n */\nexport function prepareCallSettings({\n  maxTokens,\n  temperature,\n  topP,\n  presencePenalty,\n  frequencyPenalty,\n  seed,\n  maxRetries,\n}: CallSettings): CallSettings {\n  if (maxTokens != null) {\n    if (!Number.isInteger(maxTokens)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxTokens',\n        value: maxTokens,\n        message: 'maxTokens must be an integer',\n      });\n    }\n\n    if (maxTokens < 1) {\n      throw new InvalidArgumentError({\n        parameter: 'maxTokens',\n        value: maxTokens,\n        message: 'maxTokens must be >= 1',\n      });\n    }\n  }\n\n  if (temperature != null) {\n    if (typeof temperature !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'temperature',\n        value: temperature,\n        message: 'temperature must be a number',\n      });\n    }\n  }\n\n  if (topP != null) {\n    if (typeof topP !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'topP',\n        value: topP,\n        message: 'topP must be a number',\n      });\n    }\n  }\n\n  if (presencePenalty != null) {\n    if (typeof presencePenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'presencePenalty',\n        value: presencePenalty,\n        message: 'presencePenalty must be a number',\n      });\n    }\n  }\n\n  if (frequencyPenalty != null) {\n    if (typeof frequencyPenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'frequencyPenalty',\n        value: frequencyPenalty,\n        message: 'frequencyPenalty must be a number',\n      });\n    }\n  }\n\n  if (seed != null) {\n    if (!Number.isInteger(seed)) {\n      throw new InvalidArgumentError({\n        parameter: 'seed',\n        value: seed,\n        message: 'seed must be an integer',\n      });\n    }\n  }\n\n  if (maxRetries != null) {\n    if (!Number.isInteger(maxRetries)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be an integer',\n      });\n    }\n\n    if (maxRetries < 0) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be >= 0',\n      });\n    }\n  }\n\n  return {\n    maxTokens,\n    temperature: temperature ?? 0,\n    topP,\n    presencePenalty,\n    frequencyPenalty,\n    seed,\n    maxRetries: maxRetries ?? 2,\n  };\n}\n","/**\nRepresents the number of tokens used in a prompt and completion.\n */\nexport type CompletionTokenUsage = {\n  /**\nThe number of tokens used in the prompt\n   */\n  promptTokens: number;\n\n  /**\nThe number of tokens used in the completion.\n */\n  completionTokens: number;\n\n  /**\nThe total number of tokens used (promptTokens + completionTokens).\n   */\n  totalTokens: number;\n};\n\n/**\nRepresents the number of tokens used in an embedding.\n */\nexport type EmbeddingTokenUsage = {\n  /**\nThe number of tokens used in the embedding.\n   */\n  tokens: number;\n};\n\nexport function calculateCompletionTokenUsage(usage: {\n  promptTokens: number;\n  completionTokens: number;\n}): CompletionTokenUsage {\n  return {\n    promptTokens: usage.promptTokens,\n    completionTokens: usage.completionTokens,\n    totalTokens: usage.promptTokens + usage.completionTokens,\n  };\n}\n","import { JSONSchema7 } from 'json-schema';\nimport * as z from 'zod';\nimport zodToJsonSchema from 'zod-to-json-schema';\n\nexport function convertZodToJSONSchema(\n  zodSchema: z.Schema<unknown>,\n): JSONSchema7 {\n  // we assume that zodToJsonSchema will return a valid JSONSchema7\n  return zodToJsonSchema(zodSchema) as JSONSchema7;\n}\n","export function prepareResponseHeaders(\n  init: ResponseInit | undefined,\n  { contentType }: { contentType: string },\n) {\n  const headers = new Headers(init?.headers ?? {});\n\n  if (!headers.has('Content-Type')) {\n    headers.set('Content-Type', contentType);\n  }\n\n  return headers;\n}\n","import { JSONSchema7 } from 'json-schema';\n\nconst DEFAULT_SCHEMA_PREFIX = 'JSON schema:';\nconst DEFAULT_SCHEMA_SUFFIX =\n  'You MUST answer with a JSON object that matches the JSON schema above.';\n\nexport function injectJsonSchemaIntoSystem({\n  system,\n  schema,\n  schemaPrefix = DEFAULT_SCHEMA_PREFIX,\n  schemaSuffix = DEFAULT_SCHEMA_SUFFIX,\n}: {\n  system?: string;\n  schema: JSONSchema7;\n  schemaPrefix?: string;\n  schemaSuffix?: string;\n}): string {\n  return [\n    system,\n    system != null ? '' : null, // add a newline if system is not null\n    schemaPrefix,\n    JSON.stringify(schema),\n    schemaSuffix,\n  ]\n    .filter(line => line != null)\n    .join('\\n');\n}\n","import {\n  LanguageModelV1CallOptions,\n  LanguageModelV1StreamPart,\n} from '@ai-sdk/provider';\nimport { safeValidateTypes } from '@ai-sdk/provider-utils';\nimport {\n  DeepPartial,\n  isDeepEqualData,\n  parsePartialJson,\n} from '@ai-sdk/ui-utils';\nimport { ServerResponse } from 'http';\nimport { z } from 'zod';\nimport { CallSettings } from '../prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../prompt/convert-to-language-model-prompt';\nimport { getValidatedPrompt } from '../prompt/get-validated-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { Prompt } from '../prompt/prompt';\nimport { CallWarning, FinishReason, LanguageModel, LogProbs } from '../types';\nimport {\n  CompletionTokenUsage,\n  calculateCompletionTokenUsage,\n} from '../types/token-usage';\nimport {\n  AsyncIterableStream,\n  createAsyncIterableStream,\n} from '../util/async-iterable-stream';\nimport { convertZodToJSONSchema } from '../util/convert-zod-to-json-schema';\nimport { prepareResponseHeaders } from '../util/prepare-response-headers';\nimport { retryWithExponentialBackoff } from '../util/retry-with-exponential-backoff';\nimport { injectJsonSchemaIntoSystem } from './inject-json-schema-into-system';\n\n/**\nGenerate a structured, typed object for a given prompt and schema using a language model.\n\nThis function streams the output. If you do not want to stream the output, use `generateObject` instead.\n\n@param model - The language model to use.\n\n@param schema - The schema of the object that the model should generate.\n@param mode - The mode to use for object generation. Not all models support all modes. Defaults to 'auto'.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting. \nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param presencePenalty - Presence penalty setting. \nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@return\nA result object for accessing the partial object stream and additional information.\n */\nexport async function streamObject<T>({\n  model,\n  schema,\n  mode,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  onFinish,\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe schema of the object that the model should generate.\n */\n    schema: z.Schema<T>;\n\n    /**\nThe mode to use for object generation.\n\nThe Zod schema is converted in a JSON schema and used in one of the following ways\n\n- 'auto': The provider will choose the best mode for the model.\n- 'tool': A tool with the JSON schema as parameters is is provided and the provider is instructed to use it.\n- 'json': The JSON schema and a instruction is injected into the prompt. If the provider supports JSON mode, it is enabled.\n- 'grammar': The provider is instructed to converted the JSON schema into a provider specific grammar and use it to select the output tokens.\n\nPlease note that most providers do not support all modes.\n\nDefault and recommended: 'auto' (best mode for the model).\n     */\n    mode?: 'auto' | 'json' | 'tool' | 'grammar';\n\n    /**\nCallback that is called when the LLM response and the final object validation are finished.\n     */\n    onFinish?: (event: {\n      /**\nThe token usage of the generated response.\n*/\n      usage: CompletionTokenUsage;\n\n      /**\nThe generated object (typed according to the schema). Can be undefined if the final object does not match the schema.\n   */\n      object: T | undefined;\n\n      /**\nOptional error object. This is e.g. a TypeValidationError when the final object does not match the schema.\n   */\n      error: unknown | undefined;\n\n      /**\nOptional raw response data.\n   */\n      rawResponse?: {\n        /**\nResponse headers.\n     */\n        headers?: Record<string, string>;\n      };\n\n      /**\nWarnings from the model provider (e.g. unsupported settings).\n       */\n      warnings?: CallWarning[];\n    }) => Promise<void> | void;\n  }): Promise<StreamObjectResult<T>> {\n  const retry = retryWithExponentialBackoff({ maxRetries });\n  const jsonSchema = convertZodToJSONSchema(schema);\n\n  // use the default provider mode when the mode is set to 'auto' or unspecified\n  if (mode === 'auto' || mode == null) {\n    mode = model.defaultObjectGenerationMode;\n  }\n\n  let callOptions: LanguageModelV1CallOptions;\n  let transformer: Transformer<\n    LanguageModelV1StreamPart,\n    string | Omit<LanguageModelV1StreamPart, 'text-delta'>\n  >;\n\n  switch (mode) {\n    case 'json': {\n      const validatedPrompt = getValidatedPrompt({\n        system: injectJsonSchemaIntoSystem({ system, schema: jsonSchema }),\n        prompt,\n        messages,\n      });\n\n      callOptions = {\n        mode: { type: 'object-json' },\n        ...prepareCallSettings(settings),\n        inputFormat: validatedPrompt.type,\n        prompt: convertToLanguageModelPrompt(validatedPrompt),\n        abortSignal,\n        headers,\n      };\n\n      transformer = {\n        transform: (chunk, controller) => {\n          switch (chunk.type) {\n            case 'text-delta':\n              controller.enqueue(chunk.textDelta);\n              break;\n            case 'finish':\n            case 'error':\n              controller.enqueue(chunk);\n              break;\n          }\n        },\n      };\n\n      break;\n    }\n\n    case 'grammar': {\n      const validatedPrompt = getValidatedPrompt({\n        system: injectJsonSchemaIntoSystem({ system, schema: jsonSchema }),\n        prompt,\n        messages,\n      });\n\n      callOptions = {\n        mode: { type: 'object-grammar', schema: jsonSchema },\n        ...prepareCallSettings(settings),\n        inputFormat: validatedPrompt.type,\n        prompt: convertToLanguageModelPrompt(validatedPrompt),\n        abortSignal,\n        headers,\n      };\n\n      transformer = {\n        transform: (chunk, controller) => {\n          switch (chunk.type) {\n            case 'text-delta':\n              controller.enqueue(chunk.textDelta);\n              break;\n            case 'finish':\n            case 'error':\n              controller.enqueue(chunk);\n              break;\n          }\n        },\n      };\n\n      break;\n    }\n\n    case 'tool': {\n      const validatedPrompt = getValidatedPrompt({\n        system,\n        prompt,\n        messages,\n      });\n\n      callOptions = {\n        mode: {\n          type: 'object-tool',\n          tool: {\n            type: 'function',\n            name: 'json',\n            description: 'Respond with a JSON object.',\n            parameters: jsonSchema,\n          },\n        },\n        ...prepareCallSettings(settings),\n        inputFormat: validatedPrompt.type,\n        prompt: convertToLanguageModelPrompt(validatedPrompt),\n        abortSignal,\n        headers,\n      };\n\n      transformer = {\n        transform(chunk, controller) {\n          switch (chunk.type) {\n            case 'tool-call-delta':\n              controller.enqueue(chunk.argsTextDelta);\n              break;\n            case 'finish':\n            case 'error':\n              controller.enqueue(chunk);\n              break;\n          }\n        },\n      };\n\n      break;\n    }\n\n    case undefined: {\n      throw new Error('Model does not have a default object generation mode.');\n    }\n\n    default: {\n      const _exhaustiveCheck: never = mode;\n      throw new Error(`Unsupported mode: ${_exhaustiveCheck}`);\n    }\n  }\n\n  const result = await retry(() => model.doStream(callOptions));\n\n  return new StreamObjectResult({\n    stream: result.stream.pipeThrough(new TransformStream(transformer)),\n    warnings: result.warnings,\n    rawResponse: result.rawResponse,\n    schema,\n    onFinish,\n  });\n}\n\nexport type ObjectStreamInputPart =\n  | {\n      type: 'error';\n      error: unknown;\n    }\n  | {\n      type: 'finish';\n      finishReason: FinishReason;\n      logprobs?: LogProbs;\n      usage: {\n        promptTokens: number;\n        completionTokens: number;\n        totalTokens: number;\n      };\n    };\n\nexport type ObjectStreamPart<T> =\n  | ObjectStreamInputPart\n  | {\n      type: 'object';\n      object: DeepPartial<T>;\n    }\n  | {\n      type: 'text-delta';\n      textDelta: string;\n    };\n\n/**\nThe result of a `streamObject` call that contains the partial object stream and additional information.\n */\nexport class StreamObjectResult<T> {\n  private readonly originalStream: ReadableStream<ObjectStreamPart<T>>;\n\n  /**\nWarnings from the model provider (e.g. unsupported settings)\n   */\n  readonly warnings: CallWarning[] | undefined;\n\n  /**\nThe generated object (typed according to the schema). Resolved when the response is finished.\n   */\n  readonly object: Promise<T>;\n\n  /**\nThe token usage of the generated response. Resolved when the response is finished.\n   */\n  readonly usage: Promise<CompletionTokenUsage>;\n\n  /**\nOptional raw response data.\n   */\n  rawResponse?: {\n    /**\nResponse headers.\n */\n    headers?: Record<string, string>;\n  };\n\n  constructor({\n    stream,\n    warnings,\n    rawResponse,\n    schema,\n    onFinish,\n  }: {\n    stream: ReadableStream<\n      string | Omit<LanguageModelV1StreamPart, 'text-delta'>\n    >;\n    warnings: CallWarning[] | undefined;\n    rawResponse?: {\n      headers?: Record<string, string>;\n    };\n    schema: z.Schema<T>;\n    onFinish: Parameters<typeof streamObject<T>>[0]['onFinish'];\n  }) {\n    this.warnings = warnings;\n    this.rawResponse = rawResponse;\n\n    // initialize object promise\n    let resolveObject: (value: T | PromiseLike<T>) => void;\n    let rejectObject: (reason?: any) => void;\n    this.object = new Promise<T>((resolve, reject) => {\n      resolveObject = resolve;\n      rejectObject = reject;\n    });\n\n    // initialize usage promise\n    let resolveUsage: (\n      value: CompletionTokenUsage | PromiseLike<CompletionTokenUsage>,\n    ) => void;\n    this.usage = new Promise<CompletionTokenUsage>(resolve => {\n      resolveUsage = resolve;\n    });\n\n    // store information for onFinish callback:\n    let usage: CompletionTokenUsage | undefined;\n    let object: T | undefined;\n    let error: unknown | undefined;\n\n    // pipe chunks through a transformation stream that extracts metadata:\n    let accumulatedText = '';\n    let delta = '';\n    let latestObject: DeepPartial<T> | undefined = undefined;\n\n    this.originalStream = stream.pipeThrough(\n      new TransformStream<string | ObjectStreamInputPart, ObjectStreamPart<T>>({\n        async transform(chunk, controller): Promise<void> {\n          // process partial text chunks\n          if (typeof chunk === 'string') {\n            accumulatedText += chunk;\n            delta += chunk;\n\n            const currentObject = parsePartialJson(\n              accumulatedText,\n            ) as DeepPartial<T>;\n\n            if (!isDeepEqualData(latestObject, currentObject)) {\n              latestObject = currentObject;\n\n              controller.enqueue({\n                type: 'object',\n                object: currentObject,\n              });\n\n              controller.enqueue({\n                type: 'text-delta',\n                textDelta: delta,\n              });\n\n              delta = '';\n            }\n\n            return;\n          }\n\n          switch (chunk.type) {\n            case 'finish': {\n              // send final text delta:\n              if (delta !== '') {\n                controller.enqueue({\n                  type: 'text-delta',\n                  textDelta: delta,\n                });\n              }\n\n              // store usage for promises and onFinish callback:\n              usage = calculateCompletionTokenUsage(chunk.usage);\n\n              controller.enqueue({ ...chunk, usage });\n\n              // resolve promises that can be resolved now:\n              resolveUsage(usage);\n\n              // resolve the object promise with the latest object:\n              const validationResult = safeValidateTypes({\n                value: latestObject,\n                schema,\n              });\n\n              if (validationResult.success) {\n                object = validationResult.value;\n                resolveObject(object);\n              } else {\n                error = validationResult.error;\n                rejectObject(error);\n              }\n\n              break;\n            }\n\n            default: {\n              controller.enqueue(chunk);\n              break;\n            }\n          }\n        },\n\n        // invoke onFinish callback and resolve toolResults promise when the stream is about to close:\n        async flush(controller) {\n          try {\n            // call onFinish callback:\n            await onFinish?.({\n              usage: usage ?? {\n                promptTokens: NaN,\n                completionTokens: NaN,\n                totalTokens: NaN,\n              },\n              object,\n              error,\n              rawResponse,\n              warnings,\n            });\n          } catch (error) {\n            controller.error(error);\n          }\n        },\n      }),\n    );\n  }\n\n  /**\nStream of partial objects. It gets more complete as the stream progresses.\n  \nNote that the partial object is not validated. \nIf you want to be certain that the actual content matches your schema, you need to implement your own validation for partial results.\n   */\n  get partialObjectStream(): AsyncIterableStream<DeepPartial<T>> {\n    return createAsyncIterableStream(this.originalStream, {\n      transform(chunk, controller) {\n        switch (chunk.type) {\n          case 'object':\n            controller.enqueue(chunk.object);\n            break;\n\n          case 'text-delta':\n          case 'finish':\n            break;\n\n          case 'error':\n            controller.error(chunk.error);\n            break;\n\n          default: {\n            const _exhaustiveCheck: never = chunk;\n            throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n          }\n        }\n      },\n    });\n  }\n\n  /**\nText stream of the JSON representation of the generated object. It contains text chunks. \nWhen the stream is finished, the object is valid JSON that can be parsed.\n   */\n  get textStream(): AsyncIterableStream<string> {\n    return createAsyncIterableStream(this.originalStream, {\n      transform(chunk, controller) {\n        switch (chunk.type) {\n          case 'text-delta':\n            controller.enqueue(chunk.textDelta);\n            break;\n\n          case 'object':\n          case 'finish':\n            break;\n\n          case 'error':\n            controller.error(chunk.error);\n            break;\n\n          default: {\n            const _exhaustiveCheck: never = chunk;\n            throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n          }\n        }\n      },\n    });\n  }\n\n  /**\nStream of different types of events, including partial objects, errors, and finish events.\n   */\n  get fullStream(): AsyncIterableStream<ObjectStreamPart<T>> {\n    return createAsyncIterableStream(this.originalStream, {\n      transform(chunk, controller) {\n        controller.enqueue(chunk);\n      },\n    });\n  }\n\n  /**\nWrites text delta output to a Node.js response-like object.\nIt sets a `Content-Type` header to `text/plain; charset=utf-8` and \nwrites each text delta as a separate chunk.\n\n@param response A Node.js response-like object (ServerResponse).\n@param init Optional headers and status code.\n   */\n  pipeTextStreamToResponse(\n    response: ServerResponse,\n    init?: { headers?: Record<string, string>; status?: number },\n  ) {\n    response.writeHead(init?.status ?? 200, {\n      'Content-Type': 'text/plain; charset=utf-8',\n      ...init?.headers,\n    });\n\n    const reader = this.textStream\n      .pipeThrough(new TextEncoderStream())\n      .getReader();\n\n    const read = async () => {\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n          if (done) break;\n          response.write(value);\n        }\n      } catch (error) {\n        throw error;\n      } finally {\n        response.end();\n      }\n    };\n\n    read();\n  }\n\n  /**\nCreates a simple text stream response.\nThe response has a `Content-Type` header set to `text/plain; charset=utf-8`.\nEach text delta is encoded as UTF-8 and sent as a separate chunk.\nNon-text-delta events are ignored.\n\n@param init Optional headers and status code.\n   */\n  toTextStreamResponse(init?: ResponseInit): Response {\n    return new Response(this.textStream.pipeThrough(new TextEncoderStream()), {\n      status: init?.status ?? 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n    });\n  }\n}\n\n/**\n * @deprecated Use `streamObject` instead.\n */\nexport const experimental_streamObject = streamObject;\n","export type AsyncIterableStream<T> = AsyncIterable<T> & ReadableStream<T>;\n\nexport function createAsyncIterableStream<S, T>(\n  source: ReadableStream<S>,\n  transformer: Transformer<S, T>,\n): AsyncIterableStream<T> {\n  const transformedStream: any = source.pipeThrough(\n    new TransformStream(transformer),\n  );\n\n  transformedStream[Symbol.asyncIterator] = () => {\n    const reader = transformedStream.getReader();\n    return {\n      async next(): Promise<IteratorResult<string>> {\n        const { done, value } = await reader.read();\n        return done ? { done: true, value: undefined } : { done: false, value };\n      },\n    };\n  };\n\n  return transformedStream;\n}\n","export function isNonEmptyObject(\n  object: Record<string, unknown> | undefined | null,\n): object is Record<string, unknown> {\n  return object != null && Object.keys(object).length > 0;\n}\n","import {\n  LanguageModelV1FunctionTool,\n  LanguageModelV1ToolChoice,\n} from '@ai-sdk/provider';\nimport { CoreTool } from '../tool/tool';\nimport { CoreToolChoice } from '../types/language-model';\nimport { convertZodToJSONSchema } from '../util/convert-zod-to-json-schema';\nimport { isNonEmptyObject } from '../util/is-non-empty-object';\n\nexport function prepareToolsAndToolChoice<\n  TOOLS extends Record<string, CoreTool>,\n>({\n  tools,\n  toolChoice,\n}: {\n  tools: TOOLS | undefined;\n  toolChoice: CoreToolChoice<TOOLS> | undefined;\n}): {\n  tools: LanguageModelV1FunctionTool[] | undefined;\n  toolChoice: LanguageModelV1ToolChoice | undefined;\n} {\n  if (!isNonEmptyObject(tools)) {\n    return {\n      tools: undefined,\n      toolChoice: undefined,\n    };\n  }\n\n  return {\n    tools: Object.entries(tools).map(([name, tool]) => ({\n      type: 'function' as const,\n      name,\n      description: tool.description,\n      parameters: convertZodToJSONSchema(tool.parameters),\n    })),\n    toolChoice:\n      toolChoice == null\n        ? { type: 'auto' }\n        : typeof toolChoice === 'string'\n        ? { type: toolChoice }\n        : { type: 'tool' as const, toolName: toolChoice.toolName as string },\n  };\n}\n","import { Attributes } from '@opentelemetry/api';\nimport { CallSettings } from '../prompt/call-settings';\nimport { LanguageModel } from '../types/language-model';\nimport { TelemetrySettings } from './telemetry-settings';\n\nexport function getBaseTelemetryAttributes({\n  operationName,\n  model,\n  settings,\n  telemetry,\n  headers,\n}: {\n  operationName: string;\n  model: LanguageModel;\n  settings: Omit<CallSettings, 'abortSignal' | 'headers'>;\n  telemetry: TelemetrySettings | undefined;\n  headers: Record<string, string | undefined> | undefined;\n}): Attributes {\n  return {\n    'ai.model.provider': model.provider,\n    'ai.model.id': model.modelId,\n\n    // settings:\n    ...Object.entries(settings).reduce((attributes, [key, value]) => {\n      attributes[`ai.settings.${key}`] = value;\n      return attributes;\n    }, {} as Attributes),\n\n    // special telemetry information\n    'operation.name': operationName,\n    'resource.name': telemetry?.functionId,\n    'ai.telemetry.functionId': telemetry?.functionId,\n\n    // add metadata as attributes:\n    ...Object.entries(telemetry?.metadata ?? {}).reduce(\n      (attributes, [key, value]) => {\n        attributes[`ai.telemetry.metadata.${key}`] = value;\n        return attributes;\n      },\n      {} as Attributes,\n    ),\n\n    // request headers\n    ...Object.entries(headers ?? {}).reduce((attributes, [key, value]) => {\n      if (value !== undefined) {\n        attributes[`ai.request.headers.${key}`] = value;\n      }\n      return attributes;\n    }, {} as Attributes),\n  };\n}\n","import { Tracer, trace } from '@opentelemetry/api';\nimport { noopTracer } from './noop-tracer';\n\n/**\n * Tracer variable for testing. Tests can set this to a mock tracer.\n */\nlet testTracer: Tracer | undefined = undefined;\n\nexport function setTestTracer(tracer: Tracer | undefined) {\n  testTracer = tracer;\n}\n\nexport function getTracer({ isEnabled }: { isEnabled: boolean }): Tracer {\n  if (!isEnabled) {\n    return noopTracer;\n  }\n\n  if (testTracer) {\n    return testTracer;\n  }\n\n  return trace.getTracer('ai');\n}\n","import { Span, SpanContext, Tracer } from '@opentelemetry/api';\n\n/**\n * Tracer implementation that does nothing (null object).\n */\nexport const noopTracer: Tracer = {\n  startSpan(): Span {\n    return noopSpan;\n  },\n\n  startActiveSpan<F extends (span: Span) => unknown>(\n    name: unknown,\n    arg1: unknown,\n    arg2?: unknown,\n    arg3?: F,\n  ): ReturnType<any> {\n    if (typeof arg1 === 'function') {\n      return arg1(noopSpan);\n    }\n    if (typeof arg2 === 'function') {\n      return arg2(noopSpan);\n    }\n    if (typeof arg3 === 'function') {\n      return arg3(noopSpan);\n    }\n  },\n};\n\nconst noopSpan: Span = {\n  spanContext() {\n    return noopSpanContext;\n  },\n  setAttribute() {\n    return this;\n  },\n  setAttributes() {\n    return this;\n  },\n  addEvent() {\n    return this;\n  },\n  addLink() {\n    return this;\n  },\n  addLinks() {\n    return this;\n  },\n  setStatus() {\n    return this;\n  },\n  updateName() {\n    return this;\n  },\n  end() {\n    return this;\n  },\n  isRecording() {\n    return false;\n  },\n  recordException() {\n    return this;\n  },\n};\n\nconst noopSpanContext: SpanContext = {\n  traceId: '',\n  spanId: '',\n  traceFlags: 0,\n};\n","import { Attributes, Span, Tracer, SpanStatusCode } from '@opentelemetry/api';\n\nexport function recordSpan<T>({\n  name,\n  tracer,\n  attributes,\n  fn,\n  endWhenDone = true,\n}: {\n  name: string;\n  tracer: Tracer;\n  attributes: Attributes;\n  fn: (span: Span) => Promise<T>;\n  endWhenDone?: boolean;\n}) {\n  return tracer.startActiveSpan(name, { attributes }, async span => {\n    try {\n      const result = await fn(span);\n\n      if (endWhenDone) {\n        span.end();\n      }\n\n      return result;\n    } catch (error) {\n      try {\n        if (error instanceof Error) {\n          span.recordException({\n            name: error.name,\n            message: error.message,\n            stack: error.stack,\n          });\n          span.setStatus({\n            code: SpanStatusCode.ERROR,\n            message: error.message,\n          });\n        } else {\n          span.setStatus({ code: SpanStatusCode.ERROR });\n        }\n      } finally {\n        // always stop the span when there is an error:\n        span.end();\n      }\n\n      throw error;\n    }\n  });\n}\n","import {\n  InvalidToolArgumentsError,\n  LanguageModelV1FunctionToolCall,\n  NoSuchToolError,\n} from '@ai-sdk/provider';\nimport { safeParseJSON } from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { CoreTool } from '../tool';\nimport { ValueOf } from '../util/value-of';\n\n/**\nTyped tool call that is returned by generateText and streamText. \nIt contains the tool call ID, the tool name, and the tool arguments. \n */\nexport interface ToolCall<NAME extends string, ARGS> {\n  /**\nID of the tool call. This ID is used to match the tool call with the tool result.\n */\n  toolCallId: string;\n\n  /**\nName of the tool that is being called.\n */\n  toolName: NAME;\n\n  /**\nArguments of the tool call. This is a JSON-serializable object that matches the tool's input schema.\n   */\n  args: ARGS;\n}\n\n// transforms the tools into a tool call union\nexport type ToToolCall<TOOLS extends Record<string, CoreTool>> = ValueOf<{\n  [NAME in keyof TOOLS]: {\n    type: 'tool-call';\n    toolCallId: string;\n    toolName: NAME & string;\n    args: z.infer<TOOLS[NAME]['parameters']>;\n  };\n}>;\n\nexport type ToToolCallArray<TOOLS extends Record<string, CoreTool>> = Array<\n  ToToolCall<TOOLS>\n>;\n\nexport function parseToolCall<TOOLS extends Record<string, CoreTool>>({\n  toolCall,\n  tools,\n}: {\n  toolCall: LanguageModelV1FunctionToolCall;\n  tools?: TOOLS;\n}): ToToolCall<TOOLS> {\n  const toolName = toolCall.toolName as keyof TOOLS & string;\n\n  if (tools == null) {\n    throw new NoSuchToolError({ toolName: toolCall.toolName });\n  }\n\n  const tool = tools[toolName];\n\n  if (tool == null) {\n    throw new NoSuchToolError({\n      toolName: toolCall.toolName,\n      availableTools: Object.keys(tools),\n    });\n  }\n\n  const parseResult = safeParseJSON({\n    text: toolCall.args,\n    schema: tool.parameters,\n  });\n\n  if (parseResult.success === false) {\n    throw new InvalidToolArgumentsError({\n      toolName,\n      toolArgs: toolCall.args,\n      cause: parseResult.error,\n    });\n  }\n\n  return {\n    type: 'tool-call',\n    toolCallId: toolCall.toolCallId,\n    toolName,\n    args: parseResult.value,\n  };\n}\n","import { Tracer } from '@opentelemetry/api';\nimport { CoreAssistantMessage, CoreToolMessage } from '../prompt';\nimport { CallSettings } from '../prompt/call-settings';\nimport {\n  convertToLanguageModelMessage,\n  convertToLanguageModelPrompt,\n} from '../prompt/convert-to-language-model-prompt';\nimport { getValidatedPrompt } from '../prompt/get-validated-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { prepareToolsAndToolChoice } from '../prompt/prepare-tools-and-tool-choice';\nimport { Prompt } from '../prompt/prompt';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { CoreTool } from '../tool/tool';\nimport {\n  CallWarning,\n  CoreToolChoice,\n  FinishReason,\n  LanguageModel,\n  LogProbs,\n} from '../types';\nimport {\n  CompletionTokenUsage,\n  calculateCompletionTokenUsage,\n} from '../types/token-usage';\nimport { retryWithExponentialBackoff } from '../util/retry-with-exponential-backoff';\nimport { ToToolCallArray, parseToolCall } from './tool-call';\nimport { ToToolResultArray } from './tool-result';\n\n/**\nGenerate a text and call tools for a given prompt using a language model.\n\nThis function does not stream the output. If you want to stream the output, use `streamText` instead.\n\n@param model - The language model to use.\n\n@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.\n@param toolChoice - The tool choice strategy. Default: 'auto'.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param presencePenalty - Presence penalty setting. \nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@param maxToolRoundtrips - Maximal number of automatic roundtrips for tool calls.\n\n@returns\nA result object that contains the generated text, the results of the tool calls, and additional information.\n */\nexport async function generateText<TOOLS extends Record<string, CoreTool>>({\n  model,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  maxAutomaticRoundtrips = 0,\n  maxToolRoundtrips = maxAutomaticRoundtrips,\n  experimental_telemetry: telemetry,\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe tools that the model can call. The model needs to support calling tools.\n*/\n    tools?: TOOLS;\n\n    /**\nThe tool choice strategy. Default: 'auto'.\n     */\n    toolChoice?: CoreToolChoice<TOOLS>;\n\n    /**\n@deprecated Use `maxToolRoundtrips` instead.\n     */\n    maxAutomaticRoundtrips?: number;\n\n    /**\nMaximal number of automatic roundtrips for tool calls.\n\nAn automatic tool call roundtrip is another LLM call with the \ntool call results when all tool calls of the last assistant \nmessage have results.\n\nA maximum number is required to prevent infinite loops in the\ncase of misconfigured tools.\n\nBy default, it's set to 0, which will disable the feature.\n     */\n    maxToolRoundtrips?: number;\n\n    /**\n     * Optional telemetry configuration (experimental).\n     */\n    experimental_telemetry?: TelemetrySettings;\n  }): Promise<GenerateTextResult<TOOLS>> {\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    operationName: 'ai.generateText',\n    model,\n    telemetry,\n    headers,\n    settings: { ...settings, maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n  return recordSpan({\n    name: 'ai.generateText',\n    attributes: {\n      ...baseTelemetryAttributes,\n      // specific settings that only make sense on the outer level:\n      'ai.prompt': JSON.stringify({ system, prompt, messages }),\n      'ai.settings.maxToolRoundtrips': maxToolRoundtrips,\n    },\n    tracer,\n    fn: async span => {\n      const retry = retryWithExponentialBackoff({ maxRetries });\n      const validatedPrompt = getValidatedPrompt({\n        system,\n        prompt,\n        messages,\n      });\n\n      const mode = {\n        type: 'regular' as const,\n        ...prepareToolsAndToolChoice({ tools, toolChoice }),\n      };\n      const callSettings = prepareCallSettings(settings);\n      const promptMessages = convertToLanguageModelPrompt(validatedPrompt);\n\n      let currentModelResponse: Awaited<\n        ReturnType<LanguageModel['doGenerate']>\n      >;\n      let currentToolCalls: ToToolCallArray<TOOLS> = [];\n      let currentToolResults: ToToolResultArray<TOOLS> = [];\n      let roundtrips = 0;\n      const responseMessages: Array<CoreAssistantMessage | CoreToolMessage> =\n        [];\n\n      do {\n        // once we have a roundtrip, we need to switch to messages format:\n        const currentInputFormat =\n          roundtrips === 0 ? validatedPrompt.type : 'messages';\n\n        currentModelResponse = await retry(() =>\n          recordSpan({\n            name: 'ai.generateText.doGenerate',\n            attributes: {\n              ...baseTelemetryAttributes,\n              'ai.prompt.format': currentInputFormat,\n              'ai.prompt.messages': JSON.stringify(promptMessages),\n            },\n            tracer,\n            fn: async span => {\n              const result = await model.doGenerate({\n                mode,\n                ...callSettings,\n                inputFormat: currentInputFormat,\n                prompt: promptMessages,\n                abortSignal,\n                headers,\n              });\n\n              // Add response information to the span:\n              span.setAttributes({\n                'ai.finishReason': result.finishReason,\n                'ai.usage.promptTokens': result.usage.promptTokens,\n                'ai.usage.completionTokens': result.usage.completionTokens,\n                'ai.result.text': result.text,\n                'ai.result.toolCalls': JSON.stringify(result.toolCalls),\n              });\n\n              return result;\n            },\n          }),\n        );\n\n        // parse tool calls:\n        currentToolCalls = (currentModelResponse.toolCalls ?? []).map(\n          modelToolCall => parseToolCall({ toolCall: modelToolCall, tools }),\n        );\n\n        // execute tools:\n        currentToolResults =\n          tools == null\n            ? []\n            : await executeTools({\n                toolCalls: currentToolCalls,\n                tools,\n                tracer,\n              });\n\n        // append to messages for potential next roundtrip:\n        const newResponseMessages = toResponseMessages({\n          text: currentModelResponse.text ?? '',\n          toolCalls: currentToolCalls,\n          toolResults: currentToolResults,\n        });\n        responseMessages.push(...newResponseMessages);\n        promptMessages.push(\n          ...newResponseMessages.map(convertToLanguageModelMessage),\n        );\n      } while (\n        // there are tool calls:\n        currentToolCalls.length > 0 &&\n        // all current tool calls have results:\n        currentToolResults.length === currentToolCalls.length &&\n        // the number of roundtrips is less than the maximum:\n        roundtrips++ < maxToolRoundtrips\n      );\n\n      // Add response information to the span:\n      span.setAttributes({\n        'ai.finishReason': currentModelResponse.finishReason,\n        'ai.usage.promptTokens': currentModelResponse.usage.promptTokens,\n        'ai.usage.completionTokens':\n          currentModelResponse.usage.completionTokens,\n        'ai.result.text': currentModelResponse.text,\n        'ai.result.toolCalls': JSON.stringify(currentModelResponse.toolCalls),\n      });\n\n      return new GenerateTextResult({\n        // Always return a string so that the caller doesn't have to check for undefined.\n        // If they need to check if the model did not return any text,\n        // they can check the length of the string:\n        text: currentModelResponse.text ?? '',\n        toolCalls: currentToolCalls,\n        toolResults: currentToolResults,\n        finishReason: currentModelResponse.finishReason,\n        usage: calculateCompletionTokenUsage(currentModelResponse.usage),\n        warnings: currentModelResponse.warnings,\n        rawResponse: currentModelResponse.rawResponse,\n        logprobs: currentModelResponse.logprobs,\n        responseMessages,\n      });\n    },\n  });\n}\n\nasync function executeTools<TOOLS extends Record<string, CoreTool>>({\n  toolCalls,\n  tools,\n  tracer,\n}: {\n  toolCalls: ToToolCallArray<TOOLS>;\n  tools: TOOLS;\n  tracer: Tracer;\n}): Promise<ToToolResultArray<TOOLS>> {\n  const toolResults = await Promise.all(\n    toolCalls.map(async toolCall => {\n      const tool = tools[toolCall.toolName];\n\n      if (tool?.execute == null) {\n        return undefined;\n      }\n\n      const result = await recordSpan({\n        name: 'ai.toolCall',\n        attributes: {\n          'ai.toolCall.name': toolCall.toolName,\n          'ai.toolCall.id': toolCall.toolCallId,\n          'ai.toolCall.args': JSON.stringify(toolCall.args),\n        },\n        tracer,\n        fn: async span => {\n          const result = await tool.execute!(toolCall.args);\n\n          try {\n            span.setAttributes({\n              'ai.toolCall.result': JSON.stringify(result),\n            });\n          } catch (ignored) {\n            // JSON stringify might fail if the result is not serializable,\n            // in which case we just ignore it. In the future we might want to\n            // add an optional serialize method to the tool interface and warn\n            // if the result is not serializable.\n          }\n\n          return result;\n        },\n      });\n\n      return {\n        toolCallId: toolCall.toolCallId,\n        toolName: toolCall.toolName,\n        args: toolCall.args,\n        result,\n      } as ToToolResultArray<TOOLS>[number];\n    }),\n  );\n\n  return toolResults.filter(\n    (result): result is NonNullable<typeof result> => result != null,\n  );\n}\n\n/**\nThe result of a `generateText` call.\nIt contains the generated text, the tool calls that were made during the generation, and the results of the tool calls.\n */\nexport class GenerateTextResult<TOOLS extends Record<string, CoreTool>> {\n  /**\nThe generated text.\n   */\n  readonly text: string;\n\n  /**\nThe tool calls that were made during the generation.\n   */\n  readonly toolCalls: ToToolCallArray<TOOLS>;\n\n  /**\nThe results of the tool calls.\n   */\n  readonly toolResults: ToToolResultArray<TOOLS>;\n\n  /**\nThe reason why the generation finished.\n   */\n  readonly finishReason: FinishReason;\n\n  /**\nThe token usage of the generated text.\n   */\n  readonly usage: CompletionTokenUsage;\n\n  /**\nWarnings from the model provider (e.g. unsupported settings)\n   */\n  readonly warnings: CallWarning[] | undefined;\n\n  /**\nThe response messages that were generated during the call. It consists of an assistant message,\npotentially containing tool calls. \nWhen there are tool results, there is an additional tool message with the tool results that are available.\nIf there are tools that do not have execute functions, they are not included in the tool results and\nneed to be added separately.\n   */\n  readonly responseMessages: Array<CoreAssistantMessage | CoreToolMessage>;\n\n  /**\nOptional raw response data.\n   */\n  rawResponse?: {\n    /**\nResponse headers.\n   */\n    headers?: Record<string, string>;\n  };\n\n  /**\nLogprobs for the completion. \n`undefined` if the mode does not support logprobs or if was not enabled\n   */\n  readonly logprobs: LogProbs | undefined;\n\n  constructor(options: {\n    text: string;\n    toolCalls: ToToolCallArray<TOOLS>;\n    toolResults: ToToolResultArray<TOOLS>;\n    finishReason: FinishReason;\n    usage: CompletionTokenUsage;\n    warnings: CallWarning[] | undefined;\n    rawResponse?: {\n      headers?: Record<string, string>;\n    };\n    logprobs: LogProbs | undefined;\n    responseMessages: Array<CoreAssistantMessage | CoreToolMessage>;\n  }) {\n    this.text = options.text;\n    this.toolCalls = options.toolCalls;\n    this.toolResults = options.toolResults;\n    this.finishReason = options.finishReason;\n    this.usage = options.usage;\n    this.warnings = options.warnings;\n    this.rawResponse = options.rawResponse;\n    this.logprobs = options.logprobs;\n    this.responseMessages = options.responseMessages;\n  }\n}\n\n/**\nConverts the result of a `generateText` call to a list of response messages.\n */\nfunction toResponseMessages<TOOLS extends Record<string, CoreTool>>({\n  text,\n  toolCalls,\n  toolResults,\n}: {\n  text: string;\n  toolCalls: ToToolCallArray<TOOLS>;\n  toolResults: ToToolResultArray<TOOLS>;\n}): Array<CoreAssistantMessage | CoreToolMessage> {\n  const responseMessages: Array<CoreAssistantMessage | CoreToolMessage> = [];\n\n  responseMessages.push({\n    role: 'assistant',\n    content: [{ type: 'text', text }, ...toolCalls],\n  });\n\n  if (toolResults.length > 0) {\n    responseMessages.push({\n      role: 'tool',\n      content: toolResults.map(result => ({\n        type: 'tool-result',\n        toolCallId: result.toolCallId,\n        toolName: result.toolName,\n        result: result.result,\n      })),\n    });\n  }\n\n  return responseMessages;\n}\n\n/**\n * @deprecated Use `generateText` instead.\n */\nexport const experimental_generateText = generateText;\n","import { LanguageModelV1StreamPart, NoSuchToolError } from '@ai-sdk/provider';\nimport { generateId } from '@ai-sdk/ui-utils';\nimport { Tracer } from '@opentelemetry/api';\nimport { recordSpan } from '../telemetry/record-span';\nimport { CoreTool } from '../tool';\nimport { calculateCompletionTokenUsage } from '../types/token-usage';\nimport { TextStreamPart } from './stream-text';\nimport { parseToolCall } from './tool-call';\n\nexport function runToolsTransformation<TOOLS extends Record<string, CoreTool>>({\n  tools,\n  generatorStream,\n  tracer,\n}: {\n  tools?: TOOLS;\n  generatorStream: ReadableStream<LanguageModelV1StreamPart>;\n  tracer: Tracer;\n}): ReadableStream<TextStreamPart<TOOLS>> {\n  let canClose = false;\n  const outstandingToolCalls = new Set<string>();\n\n  // tool results stream\n  let toolResultsStreamController: ReadableStreamDefaultController<\n    TextStreamPart<TOOLS>\n  > | null = null;\n  const toolResultsStream = new ReadableStream<TextStreamPart<TOOLS>>({\n    start(controller) {\n      toolResultsStreamController = controller;\n    },\n  });\n\n  // forward stream\n  const forwardStream = new TransformStream<\n    LanguageModelV1StreamPart,\n    TextStreamPart<TOOLS>\n  >({\n    transform(\n      chunk: LanguageModelV1StreamPart,\n      controller: TransformStreamDefaultController<TextStreamPart<TOOLS>>,\n    ) {\n      const chunkType = chunk.type;\n\n      switch (chunkType) {\n        // forward:\n        case 'text-delta':\n        case 'error': {\n          controller.enqueue(chunk);\n          break;\n        }\n\n        // process tool call:\n        case 'tool-call': {\n          const toolName = chunk.toolName as keyof TOOLS & string;\n\n          if (tools == null) {\n            toolResultsStreamController!.enqueue({\n              type: 'error',\n              error: new NoSuchToolError({ toolName: chunk.toolName }),\n            });\n            break;\n          }\n\n          const tool = tools[toolName];\n\n          if (tool == null) {\n            toolResultsStreamController!.enqueue({\n              type: 'error',\n              error: new NoSuchToolError({\n                toolName: chunk.toolName,\n                availableTools: Object.keys(tools),\n              }),\n            });\n\n            break;\n          }\n\n          try {\n            const toolCall = parseToolCall({\n              toolCall: chunk,\n              tools,\n            });\n\n            controller.enqueue(toolCall);\n\n            if (tool.execute != null) {\n              const toolExecutionId = generateId(); // use our own id to guarantee uniqueness\n              outstandingToolCalls.add(toolExecutionId);\n\n              // Note: we don't await the tool execution here (by leaving out 'await' on recordSpan),\n              // because we want to process the next chunk as soon as possible.\n              // This is important for the case where the tool execution takes a long time.\n              recordSpan({\n                name: 'ai.toolCall',\n                attributes: {\n                  'ai.toolCall.name': toolCall.toolName,\n                  'ai.toolCall.id': toolCall.toolCallId,\n                  'ai.toolCall.args': JSON.stringify(toolCall.args),\n                },\n                tracer,\n                fn: async span =>\n                  tool.execute!(toolCall.args).then(\n                    (result: any) => {\n                      toolResultsStreamController!.enqueue({\n                        ...toolCall,\n                        type: 'tool-result',\n                        result,\n                      } as any);\n\n                      outstandingToolCalls.delete(toolExecutionId);\n\n                      // close the tool results controller if no more outstanding tool calls\n                      if (canClose && outstandingToolCalls.size === 0) {\n                        toolResultsStreamController!.close();\n                      }\n\n                      // record telemetry\n                      try {\n                        span.setAttributes({\n                          'ai.toolCall.result': JSON.stringify(result),\n                        });\n                      } catch (ignored) {\n                        // JSON stringify might fail if the result is not serializable,\n                        // in which case we just ignore it. In the future we might want to\n                        // add an optional serialize method to the tool interface and warn\n                        // if the result is not serializable.\n                      }\n                    },\n                    (error: any) => {\n                      toolResultsStreamController!.enqueue({\n                        type: 'error',\n                        error,\n                      });\n\n                      outstandingToolCalls.delete(toolExecutionId);\n\n                      // close the tool results controller if no more outstanding tool calls\n                      if (canClose && outstandingToolCalls.size === 0) {\n                        toolResultsStreamController!.close();\n                      }\n                    },\n                  ),\n              });\n            }\n          } catch (error) {\n            toolResultsStreamController!.enqueue({\n              type: 'error',\n              error,\n            });\n          }\n\n          break;\n        }\n\n        // process finish:\n        case 'finish': {\n          controller.enqueue({\n            type: 'finish',\n            finishReason: chunk.finishReason,\n            logprobs: chunk.logprobs,\n            usage: calculateCompletionTokenUsage(chunk.usage),\n          });\n          break;\n        }\n\n        // ignore\n        case 'tool-call-delta': {\n          break;\n        }\n\n        default: {\n          const _exhaustiveCheck: never = chunkType;\n          throw new Error(`Unhandled chunk type: ${_exhaustiveCheck}`);\n        }\n      }\n    },\n\n    flush() {\n      canClose = true;\n\n      if (outstandingToolCalls.size === 0) {\n        toolResultsStreamController!.close();\n      }\n    },\n  });\n\n  // combine the generator stream and the tool results stream\n  return new ReadableStream<TextStreamPart<TOOLS>>({\n    async start(controller) {\n      // need to wait for both pipes so there are no dangling promises that\n      // can cause uncaught promise rejections when the stream is aborted\n      return Promise.all([\n        generatorStream.pipeThrough(forwardStream).pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n              // the generator stream controller is automatically closed when it's consumed\n            },\n          }),\n        ),\n        toolResultsStream.pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n              controller.close();\n            },\n          }),\n        ),\n      ]);\n    },\n  });\n}\n","import { Span } from '@opentelemetry/api';\nimport { ServerResponse } from 'node:http';\nimport {\n  AIStreamCallbacksAndOptions,\n  StreamingTextResponse,\n  formatStreamPart,\n} from '../../streams';\nimport { CallSettings } from '../prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../prompt/convert-to-language-model-prompt';\nimport { getValidatedPrompt } from '../prompt/get-validated-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { prepareToolsAndToolChoice } from '../prompt/prepare-tools-and-tool-choice';\nimport { Prompt } from '../prompt/prompt';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { CoreTool } from '../tool';\nimport {\n  CallWarning,\n  CoreToolChoice,\n  FinishReason,\n  LanguageModel,\n  LogProbs,\n} from '../types';\nimport { CompletionTokenUsage } from '../types/token-usage';\nimport {\n  AsyncIterableStream,\n  createAsyncIterableStream,\n} from '../util/async-iterable-stream';\nimport { prepareResponseHeaders } from '../util/prepare-response-headers';\nimport { retryWithExponentialBackoff } from '../util/retry-with-exponential-backoff';\nimport { runToolsTransformation } from './run-tools-transformation';\nimport { ToToolCall } from './tool-call';\nimport { ToToolResult } from './tool-result';\n\n/**\nGenerate a text and call tools for a given prompt using a language model.\n\nThis function streams the output. If you do not want to stream the output, use `generateText` instead.\n\n@param model - The language model to use.\n@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param presencePenalty - Presence penalty setting. \nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@param onFinish - Callback that is called when the LLM response and all request tool executions \n(for tools that have an `execute` function) are finished.\n\n@return\nA result object for accessing different stream types and additional information.\n */\nexport async function streamText<TOOLS extends Record<string, CoreTool>>({\n  model,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  experimental_telemetry: telemetry,\n  onFinish,\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe tools that the model can call. The model needs to support calling tools.\n    */\n    tools?: TOOLS;\n\n    /**\nThe tool choice strategy. Default: 'auto'.\n     */\n    toolChoice?: CoreToolChoice<TOOLS>;\n\n    /**\n     * Optional telemetry configuration (experimental).\n     */\n    experimental_telemetry?: TelemetrySettings;\n\n    /**\nCallback that is called when the LLM response and all request tool executions \n(for tools that have an `execute` function) are finished.\n     */\n    onFinish?: (event: {\n      /**\nThe reason why the generation finished.\n       */\n      finishReason: FinishReason;\n\n      /**\nThe token usage of the generated response.\n */\n      usage: CompletionTokenUsage;\n\n      /**\nThe full text that has been generated.\n       */\n      text: string;\n\n      /**\nThe tool calls that have been executed.\n       */\n      toolCalls?: ToToolCall<TOOLS>[];\n\n      /**\nThe tool results that have been generated.\n       */\n      toolResults?: ToToolResult<TOOLS>[];\n\n      /**\nOptional raw response data.\n       */\n      rawResponse?: {\n        /**\nResponse headers.\n         */\n        headers?: Record<string, string>;\n      };\n\n      /**\nWarnings from the model provider (e.g. unsupported settings).\n       */\n      warnings?: CallWarning[];\n    }) => Promise<void> | void;\n  }): Promise<StreamTextResult<TOOLS>> {\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    operationName: 'ai.streamText',\n    model,\n    telemetry,\n    headers,\n    settings: { ...settings, maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n\n  return recordSpan({\n    name: 'ai.streamText',\n    attributes: {\n      ...baseTelemetryAttributes,\n      // specific settings that only make sense on the outer level:\n      'ai.prompt': JSON.stringify({ system, prompt, messages }),\n    },\n    tracer,\n    endWhenDone: false,\n    fn: async rootSpan => {\n      const retry = retryWithExponentialBackoff({ maxRetries });\n      const validatedPrompt = getValidatedPrompt({ system, prompt, messages });\n      const promptMessages = convertToLanguageModelPrompt(validatedPrompt);\n      const {\n        result: { stream, warnings, rawResponse },\n        doStreamSpan,\n      } = await retry(() =>\n        recordSpan({\n          name: 'ai.streamText.doStream',\n          attributes: {\n            ...baseTelemetryAttributes,\n            'ai.prompt.format': validatedPrompt.type,\n            'ai.prompt.messages': JSON.stringify(promptMessages),\n          },\n          tracer,\n          endWhenDone: false,\n          fn: async doStreamSpan => {\n            return {\n              result: await model.doStream({\n                mode: {\n                  type: 'regular',\n                  ...prepareToolsAndToolChoice({ tools, toolChoice }),\n                },\n                ...prepareCallSettings(settings),\n                inputFormat: validatedPrompt.type,\n                prompt: promptMessages,\n                abortSignal,\n                headers,\n              }),\n              doStreamSpan,\n            };\n          },\n        }),\n      );\n\n      return new StreamTextResult({\n        stream: runToolsTransformation({\n          tools,\n          generatorStream: stream,\n          tracer,\n        }),\n        warnings,\n        rawResponse,\n        onFinish,\n        rootSpan,\n        doStreamSpan,\n      });\n    },\n  });\n}\n\nexport type TextStreamPart<TOOLS extends Record<string, CoreTool>> =\n  | {\n      type: 'text-delta';\n      textDelta: string;\n    }\n  | ({\n      type: 'tool-call';\n    } & ToToolCall<TOOLS>)\n  | {\n      type: 'error';\n      error: unknown;\n    }\n  | ({\n      type: 'tool-result';\n    } & ToToolResult<TOOLS>)\n  | {\n      type: 'finish';\n      finishReason: FinishReason;\n      logprobs?: LogProbs;\n      usage: {\n        promptTokens: number;\n        completionTokens: number;\n        totalTokens: number;\n      };\n    };\n\n/**\nA result object for accessing different stream types and additional information.\n */\nexport class StreamTextResult<TOOLS extends Record<string, CoreTool>> {\n  private originalStream: ReadableStream<TextStreamPart<TOOLS>>;\n  private onFinish?: Parameters<typeof streamText>[0]['onFinish'];\n\n  /**\nWarnings from the model provider (e.g. unsupported settings).\n   */\n  readonly warnings: CallWarning[] | undefined;\n\n  /**\nThe token usage of the generated response. Resolved when the response is finished.\n   */\n  readonly usage: Promise<CompletionTokenUsage>;\n\n  /**\nThe reason why the generation finished. Resolved when the response is finished.\n   */\n  readonly finishReason: Promise<FinishReason>;\n\n  /**\nThe full text that has been generated. Resolved when the response is finished.\n   */\n  readonly text: Promise<string>;\n\n  /**\nThe tool calls that have been executed. Resolved when the response is finished.\n   */\n  readonly toolCalls: Promise<ToToolCall<TOOLS>[]>;\n\n  /**\nThe tool results that have been generated. Resolved when the all tool executions are finished.\n   */\n  readonly toolResults: Promise<ToToolResult<TOOLS>[]>;\n\n  /**\nOptional raw response data.\n   */\n  readonly rawResponse?: {\n    /**\nResponse headers.\n     */\n    headers?: Record<string, string>;\n  };\n\n  constructor({\n    stream,\n    warnings,\n    rawResponse,\n    onFinish,\n    rootSpan,\n    doStreamSpan,\n  }: {\n    stream: ReadableStream<TextStreamPart<TOOLS>>;\n    warnings: CallWarning[] | undefined;\n    rawResponse?: {\n      headers?: Record<string, string>;\n    };\n    onFinish?: Parameters<typeof streamText>[0]['onFinish'];\n    rootSpan: Span;\n    doStreamSpan: Span;\n  }) {\n    this.warnings = warnings;\n    this.rawResponse = rawResponse;\n    this.onFinish = onFinish;\n\n    // initialize usage promise\n    let resolveUsage: (\n      value: CompletionTokenUsage | PromiseLike<CompletionTokenUsage>,\n    ) => void;\n    this.usage = new Promise<CompletionTokenUsage>(resolve => {\n      resolveUsage = resolve;\n    });\n\n    // initialize finish reason promise\n    let resolveFinishReason: (\n      value: FinishReason | PromiseLike<FinishReason>,\n    ) => void;\n    this.finishReason = new Promise<FinishReason>(resolve => {\n      resolveFinishReason = resolve;\n    });\n\n    // initialize text promise\n    let resolveText: (value: string | PromiseLike<string>) => void;\n    this.text = new Promise<string>(resolve => {\n      resolveText = resolve;\n    });\n\n    // initialize toolCalls promise\n    let resolveToolCalls: (\n      value: ToToolCall<TOOLS>[] | PromiseLike<ToToolCall<TOOLS>[]>,\n    ) => void;\n    this.toolCalls = new Promise<ToToolCall<TOOLS>[]>(resolve => {\n      resolveToolCalls = resolve;\n    });\n\n    // initialize toolResults promise\n    let resolveToolResults: (\n      value: ToToolResult<TOOLS>[] | PromiseLike<ToToolResult<TOOLS>[]>,\n    ) => void;\n    this.toolResults = new Promise<ToToolResult<TOOLS>[]>(resolve => {\n      resolveToolResults = resolve;\n    });\n\n    // store information for onFinish callback:\n    let finishReason: FinishReason | undefined;\n    let usage: CompletionTokenUsage | undefined;\n    let text = '';\n    const toolCalls: ToToolCall<TOOLS>[] = [];\n    const toolResults: ToToolResult<TOOLS>[] = [];\n    let firstChunk = true;\n\n    // pipe chunks through a transformation stream that extracts metadata:\n    const self = this;\n    this.originalStream = stream.pipeThrough(\n      new TransformStream<TextStreamPart<TOOLS>, TextStreamPart<TOOLS>>({\n        async transform(chunk, controller): Promise<void> {\n          controller.enqueue(chunk);\n\n          // Telemetry event for first chunk:\n          if (firstChunk) {\n            firstChunk = false;\n            doStreamSpan.addEvent('ai.stream.firstChunk');\n          }\n\n          const chunkType = chunk.type;\n          switch (chunkType) {\n            case 'text-delta':\n              // create the full text from text deltas (for onFinish callback and text promise):\n              text += chunk.textDelta;\n              break;\n\n            case 'tool-call':\n              // store tool calls for onFinish callback and toolCalls promise:\n              toolCalls.push(chunk);\n              break;\n\n            case 'tool-result':\n              // store tool results for onFinish callback and toolResults promise:\n              toolResults.push(chunk);\n              break;\n\n            case 'finish':\n              // Note: tool executions might not be finished yet when the finish event is emitted.\n              // store usage and finish reason for promises and onFinish callback:\n              usage = chunk.usage;\n              finishReason = chunk.finishReason;\n\n              // resolve promises that can be resolved now:\n              resolveUsage(usage);\n              resolveFinishReason(finishReason);\n              resolveText(text);\n              resolveToolCalls(toolCalls);\n              break;\n\n            case 'error':\n              // ignored\n              break;\n\n            default: {\n              const exhaustiveCheck: never = chunkType;\n              throw new Error(`Unknown chunk type: ${exhaustiveCheck}`);\n            }\n          }\n        },\n\n        // invoke onFinish callback and resolve toolResults promise when the stream is about to close:\n        async flush(controller) {\n          try {\n            const finalUsage = usage ?? {\n              promptTokens: NaN,\n              completionTokens: NaN,\n              totalTokens: NaN,\n            };\n            const finalFinishReason = finishReason ?? 'unknown';\n            const telemetryToolCalls =\n              toolCalls.length > 0 ? JSON.stringify(toolCalls) : undefined;\n\n            doStreamSpan.setAttributes({\n              'ai.finishReason': finalFinishReason,\n              'ai.usage.promptTokens': finalUsage.promptTokens,\n              'ai.usage.completionTokens': finalUsage.completionTokens,\n              'ai.result.text': text,\n              'ai.result.toolCalls': telemetryToolCalls,\n            });\n\n            // finish doStreamSpan before other operations for correct timing:\n            doStreamSpan.end();\n\n            // Add response information to the root span:\n            rootSpan.setAttributes({\n              'ai.finishReason': finalFinishReason,\n              'ai.usage.promptTokens': finalUsage.promptTokens,\n              'ai.usage.completionTokens': finalUsage.completionTokens,\n              'ai.result.text': text,\n              'ai.result.toolCalls': telemetryToolCalls,\n            });\n\n            // resolve toolResults promise:\n            resolveToolResults(toolResults);\n\n            // call onFinish callback:\n            await self.onFinish?.({\n              finishReason: finalFinishReason,\n              usage: finalUsage,\n              text,\n              toolCalls,\n              // The tool results are inferred as a never[] type, because they are\n              // optional and the execute method with an inferred result type is\n              // optional as well. Therefore we need to cast the toolResults to any.\n              // The type exposed to the users will be correctly inferred.\n              toolResults: toolResults as any,\n              rawResponse,\n              warnings,\n            });\n          } catch (error) {\n            controller.error(error);\n          } finally {\n            rootSpan.end();\n          }\n        },\n      }),\n    );\n  }\n\n  /**\nSplit out a new stream from the original stream.\nThe original stream is replaced to allow for further splitting,\nsince we do not know how many times the stream will be split.\n\nNote: this leads to buffering the stream content on the server.\nHowever, the LLM results are expected to be small enough to not cause issues.\n   */\n  private teeStream() {\n    const [stream1, stream2] = this.originalStream.tee();\n    this.originalStream = stream2;\n    return stream1;\n  }\n\n  /**\nA text stream that returns only the generated text deltas. You can use it\nas either an AsyncIterable or a ReadableStream. When an error occurs, the\nstream will throw the error.\n   */\n  get textStream(): AsyncIterableStream<string> {\n    return createAsyncIterableStream(this.teeStream(), {\n      transform(chunk, controller) {\n        if (chunk.type === 'text-delta') {\n          // do not stream empty text deltas:\n          if (chunk.textDelta.length > 0) {\n            controller.enqueue(chunk.textDelta);\n          }\n        } else if (chunk.type === 'error') {\n          controller.error(chunk.error);\n        }\n      },\n    });\n  }\n\n  /**\nA stream with all events, including text deltas, tool calls, tool results, and\nerrors.\nYou can use it as either an AsyncIterable or a ReadableStream. When an error occurs, the\nstream will throw the error.\n   */\n  get fullStream(): AsyncIterableStream<TextStreamPart<TOOLS>> {\n    return createAsyncIterableStream(this.teeStream(), {\n      transform(chunk, controller) {\n        if (chunk.type === 'text-delta') {\n          // do not stream empty text deltas:\n          if (chunk.textDelta.length > 0) {\n            controller.enqueue(chunk);\n          }\n        } else {\n          controller.enqueue(chunk);\n        }\n      },\n    });\n  }\n\n  /**\nConverts the result to an `AIStream` object that is compatible with `StreamingTextResponse`.\nIt can be used with the `useChat` and `useCompletion` hooks.\n\n@param callbacks \nStream callbacks that will be called when the stream emits events.\n\n@returns an `AIStream` object.\n   */\n  toAIStream(callbacks: AIStreamCallbacksAndOptions = {}) {\n    let aggregatedResponse = '';\n\n    const callbackTransformer = new TransformStream<\n      TextStreamPart<TOOLS>,\n      TextStreamPart<TOOLS>\n    >({\n      async start(): Promise<void> {\n        if (callbacks.onStart) await callbacks.onStart();\n      },\n\n      async transform(chunk, controller): Promise<void> {\n        controller.enqueue(chunk);\n\n        if (chunk.type === 'text-delta') {\n          const textDelta = chunk.textDelta;\n\n          aggregatedResponse += textDelta;\n\n          if (callbacks.onToken) await callbacks.onToken(textDelta);\n          if (callbacks.onText) await callbacks.onText(textDelta);\n        }\n      },\n\n      async flush(): Promise<void> {\n        if (callbacks.onCompletion)\n          await callbacks.onCompletion(aggregatedResponse);\n        if (callbacks.onFinal) await callbacks.onFinal(aggregatedResponse);\n      },\n    });\n\n    const streamDataTransformer = new TransformStream<\n      TextStreamPart<TOOLS>,\n      string\n    >({\n      transform: async (chunk, controller) => {\n        switch (chunk.type) {\n          case 'text-delta':\n            controller.enqueue(formatStreamPart('text', chunk.textDelta));\n            break;\n          case 'tool-call':\n            controller.enqueue(\n              formatStreamPart('tool_call', {\n                toolCallId: chunk.toolCallId,\n                toolName: chunk.toolName,\n                args: chunk.args,\n              }),\n            );\n            break;\n          case 'tool-result':\n            controller.enqueue(\n              formatStreamPart('tool_result', {\n                toolCallId: chunk.toolCallId,\n                toolName: chunk.toolName,\n                args: chunk.args,\n                result: chunk.result,\n              }),\n            );\n            break;\n          case 'error':\n            controller.enqueue(\n              formatStreamPart('error', JSON.stringify(chunk.error)),\n            );\n            break;\n        }\n      },\n    });\n\n    return this.fullStream\n      .pipeThrough(callbackTransformer)\n      .pipeThrough(streamDataTransformer)\n      .pipeThrough(new TextEncoderStream());\n  }\n\n  /**\nWrites stream data output to a Node.js response-like object.\nIt sets a `Content-Type` header to `text/plain; charset=utf-8` and \nwrites each stream data part as a separate chunk.\n\n@param response A Node.js response-like object (ServerResponse).\n@param init Optional headers and status code.\n   */\n  pipeAIStreamToResponse(\n    response: ServerResponse,\n    init?: { headers?: Record<string, string>; status?: number },\n  ) {\n    response.writeHead(init?.status ?? 200, {\n      'Content-Type': 'text/plain; charset=utf-8',\n      ...init?.headers,\n    });\n\n    const reader = this.toAIStream().getReader();\n\n    const read = async () => {\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n          if (done) break;\n          response.write(value);\n        }\n      } catch (error) {\n        throw error;\n      } finally {\n        response.end();\n      }\n    };\n\n    read();\n  }\n\n  /**\nWrites text delta output to a Node.js response-like object.\nIt sets a `Content-Type` header to `text/plain; charset=utf-8` and \nwrites each text delta as a separate chunk.\n\n@param response A Node.js response-like object (ServerResponse).\n@param init Optional headers and status code.\n   */\n  pipeTextStreamToResponse(\n    response: ServerResponse,\n    init?: { headers?: Record<string, string>; status?: number },\n  ) {\n    response.writeHead(init?.status ?? 200, {\n      'Content-Type': 'text/plain; charset=utf-8',\n      ...init?.headers,\n    });\n\n    const reader = this.textStream\n      .pipeThrough(new TextEncoderStream())\n      .getReader();\n\n    const read = async () => {\n      try {\n        while (true) {\n          const { done, value } = await reader.read();\n          if (done) break;\n          response.write(value);\n        }\n      } catch (error) {\n        throw error;\n      } finally {\n        response.end();\n      }\n    };\n\n    read();\n  }\n\n  /**\nConverts the result to a streamed response object with a stream data part stream.\nIt can be used with the `useChat` and `useCompletion` hooks.\n\n@param init Optional headers.\n\n@return A response object.\n   */\n  toAIStreamResponse(init?: ResponseInit): Response {\n    return new StreamingTextResponse(this.toAIStream(), init);\n  }\n\n  /**\nCreates a simple text stream response.\nEach text delta is encoded as UTF-8 and sent as a separate chunk.\nNon-text-delta events are ignored.\n\n@param init Optional headers and status code.\n   */\n  toTextStreamResponse(init?: ResponseInit): Response {\n    return new Response(this.textStream.pipeThrough(new TextEncoderStream()), {\n      status: init?.status ?? 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n    });\n  }\n}\n\n/**\n * @deprecated Use `streamText` instead.\n */\nexport const experimental_streamText = streamText;\n","import { ToolResult } from '../generate-text/tool-result';\nimport { CoreMessage } from '../prompt';\n\n/**\nConverts an array of messages from useChat into an array of CoreMessages that can be used\nwith the AI core functions (e.g. `streamText`).\n */\nexport function convertToCoreMessages(\n  messages: Array<{\n    role: 'user' | 'assistant';\n    content: string;\n    toolInvocations?: Array<ToolResult<string, unknown, unknown>>;\n  }>,\n) {\n  const coreMessages: CoreMessage[] = [];\n\n  for (const { role, content, toolInvocations } of messages) {\n    switch (role) {\n      case 'user': {\n        coreMessages.push({ role: 'user', content });\n        break;\n      }\n\n      case 'assistant': {\n        if (toolInvocations == null) {\n          coreMessages.push({ role: 'assistant', content });\n          break;\n        }\n\n        // assistant message with tool calls\n        coreMessages.push({\n          role: 'assistant',\n          content: [\n            { type: 'text', text: content },\n            ...toolInvocations.map(({ toolCallId, toolName, args }) => ({\n              type: 'tool-call' as const,\n              toolCallId,\n              toolName,\n              args,\n            })),\n          ],\n        });\n\n        // tool message with tool results\n        coreMessages.push({\n          role: 'tool',\n          content: toolInvocations.map(\n            ({ toolCallId, toolName, args, result }) => ({\n              type: 'tool-result' as const,\n              toolCallId,\n              toolName,\n              args,\n              result,\n            }),\n          ),\n        });\n\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new Error(`Unhandled role: ${_exhaustiveCheck}`);\n      }\n    }\n  }\n\n  return coreMessages;\n}\n","export class InvalidModelIdError extends Error {\n  readonly id: string;\n\n  constructor({\n    id,\n    message = `Invalid model id: ${id}`,\n  }: {\n    id: string;\n    message?: string;\n  }) {\n    super(message);\n\n    this.name = 'AI_InvalidModelIdError';\n\n    this.id = id;\n  }\n\n  static isInvalidModelIdError(error: unknown): error is InvalidModelIdError {\n    return (\n      error instanceof Error &&\n      error.name === 'AI_InvalidModelIdError' &&\n      typeof (error as InvalidModelIdError).id === 'string'\n    );\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      id: this.id,\n    };\n  }\n}\n","export class NoSuchModelError extends Error {\n  readonly modelId: string;\n  readonly modelType: string;\n\n  constructor({\n    modelId,\n    modelType,\n    message = `No such ${modelType}: ${modelId}`,\n  }: {\n    modelId: string;\n    modelType: string;\n    message?: string;\n  }) {\n    super(message);\n\n    this.name = 'AI_NoSuchModelError';\n\n    this.modelId = modelId;\n    this.modelType = modelType;\n  }\n\n  static isNoSuchModelError(error: unknown): error is NoSuchModelError {\n    return (\n      error instanceof Error &&\n      error.name === 'AI_NoSuchModelError' &&\n      typeof (error as NoSuchModelError).modelId === 'string' &&\n      typeof (error as NoSuchModelError).modelType === 'string'\n    );\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      modelId: this.modelId,\n      modelType: this.modelType,\n    };\n  }\n}\n","export class NoSuchProviderError extends Error {\n  readonly providerId: string;\n  readonly availableProviders: string[];\n\n  constructor({\n    providerId,\n    availableProviders,\n    message = `No such provider: ${providerId} (available providers: ${availableProviders.join()})`,\n  }: {\n    providerId: string;\n    availableProviders: string[];\n    message?: string;\n  }) {\n    super(message);\n\n    this.name = 'AI_NoSuchProviderError';\n\n    this.providerId = providerId;\n    this.availableProviders = availableProviders;\n  }\n\n  static isNoSuchProviderError(error: unknown): error is NoSuchProviderError {\n    return (\n      error instanceof Error &&\n      error.name === 'AI_NoSuchProviderError' &&\n      typeof (error as NoSuchProviderError).providerId === 'string' &&\n      Array.isArray((error as NoSuchProviderError).availableProviders)\n    );\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      providerId: this.providerId,\n      availableProviders: this.availableProviders,\n    };\n  }\n}\n","import { EmbeddingModel, LanguageModel } from '../types';\nimport { InvalidModelIdError } from './invalid-model-id-error';\nimport { NoSuchModelError } from './no-such-model-error';\nimport { NoSuchProviderError } from './no-such-provider-error';\n\n/**\nRegistry for managing models. It enables getting a model with a string id.\n */\nexport type experimental_ProviderRegistry = {\n  /**\nReturns the language model with the given id in the format `providerId:modelId`.\nThe model id is then passed to the provider function to get the model.\n\n@param {string} id - The id of the model to return.\n\n@throws {NoSuchModelError} If no model with the given id exists.\n@throws {NoSuchProviderError} If no provider with the given id exists.\n\n@returns {LanguageModel} The language model associated with the id.\n   */\n  languageModel(id: string): LanguageModel;\n\n  /**\nReturns the text embedding model with the given id in the format `providerId:modelId`.\nThe model id is then passed to the provider function to get the model.\n\n@param {string} id - The id of the model to return.\n\n@throws {NoSuchModelError} If no model with the given id exists.\n@throws {NoSuchProviderError} If no provider with the given id exists.\n\n@returns {LanguageModel} The language model associated with the id.\n   */\n  textEmbeddingModel(id: string): EmbeddingModel<string>;\n};\n\n/**\n * @deprecated Use `experimental_ProviderRegistry` instead.\n */\nexport type experimental_ModelRegistry = experimental_ProviderRegistry;\n\n/**\n * Provider for language and text embedding models. Compatible with the\n * provider registry.\n */\ninterface Provider {\n  /**\n   * Returns a language model with the given id.\n   */\n  languageModel?: (modelId: string) => LanguageModel;\n\n  /**\n   * Returns a text embedding model with the given id.\n   */\n  textEmbedding?: (modelId: string) => EmbeddingModel<string>;\n}\n\n/**\n * Creates a registry for the given providers.\n */\nexport function experimental_createProviderRegistry(\n  providers: Record<string, Provider>,\n): experimental_ProviderRegistry {\n  const registry = new DefaultProviderRegistry();\n\n  for (const [id, provider] of Object.entries(providers)) {\n    registry.registerProvider({ id, provider });\n  }\n\n  return registry;\n}\n\n/**\n * @deprecated Use `experimental_createProviderRegistry` instead.\n */\nexport const experimental_createModelRegistry =\n  experimental_createProviderRegistry;\n\nclass DefaultProviderRegistry implements experimental_ProviderRegistry {\n  private providers: Record<string, Provider> = {};\n\n  registerProvider({ id, provider }: { id: string; provider: Provider }): void {\n    this.providers[id] = provider;\n  }\n\n  private getProvider(id: string): Provider {\n    const provider = this.providers[id];\n\n    if (provider == null) {\n      throw new NoSuchProviderError({\n        providerId: id,\n        availableProviders: Object.keys(this.providers),\n      });\n    }\n\n    return provider;\n  }\n\n  private splitId(id: string): [string, string] {\n    const index = id.indexOf(':');\n\n    if (index === -1) {\n      throw new InvalidModelIdError({ id });\n    }\n\n    return [id.slice(0, index), id.slice(index + 1)];\n  }\n\n  languageModel(id: string): LanguageModel {\n    const [providerId, modelId] = this.splitId(id);\n    const model = this.getProvider(providerId).languageModel?.(modelId);\n\n    if (model == null) {\n      throw new NoSuchModelError({ modelId: id, modelType: 'language model' });\n    }\n\n    return model;\n  }\n\n  textEmbeddingModel(id: string): EmbeddingModel<string> {\n    const [providerId, modelId] = this.splitId(id);\n    const model = this.getProvider(providerId).textEmbedding?.(modelId);\n\n    if (model == null) {\n      throw new NoSuchModelError({\n        modelId: id,\n        modelType: 'text embedding model',\n      });\n    }\n\n    return model;\n  }\n}\n","import { z } from 'zod';\n\n/**\nA tool contains the description and the schema of the input that the tool expects.\nThis enables the language model to generate the input.\n\nThe tool can also contain an optional execute function for the actual execution function of the tool.\n */\nexport interface CoreTool<PARAMETERS extends z.ZodTypeAny = any, RESULT = any> {\n  /**\nAn optional description of what the tool does. Will be used by the language model to decide whether to use the tool.\n   */\n  description?: string;\n\n  /**\nThe schema of the input that the tool expects. The language model will use this to generate the input.\nIt is also used to validate the output of the language model. \nUse descriptions to make the input understandable for the language model.\n   */\n  parameters: PARAMETERS;\n\n  /**\nAn async function that is called with the arguments from the tool call and produces a result. \nIf not provided, the tool will not be executed automatically.\n   */\n  execute?: (args: z.infer<PARAMETERS>) => PromiseLike<RESULT>;\n}\n\n/**\nHelper function for inferring the execute args of a tool.\n */\n// Note: special type inference is needed for the execute function args to make sure they are inferred correctly.\nexport function tool<PARAMETERS extends z.ZodTypeAny, RESULT>(\n  tool: CoreTool<PARAMETERS, RESULT> & {\n    execute: (args: z.infer<PARAMETERS>) => PromiseLike<RESULT>;\n  },\n): CoreTool<PARAMETERS, RESULT> & {\n  execute: (args: z.infer<PARAMETERS>) => PromiseLike<RESULT>;\n};\nexport function tool<PARAMETERS extends z.ZodTypeAny, RESULT>(\n  tool: CoreTool<PARAMETERS, RESULT> & {\n    execute?: undefined;\n  },\n): CoreTool<PARAMETERS, RESULT> & {\n  execute: undefined;\n};\nexport function tool<PARAMETERS extends z.ZodTypeAny, RESULT = any>(\n  tool: CoreTool<PARAMETERS, RESULT>,\n): CoreTool<PARAMETERS, RESULT> {\n  return tool;\n}\n\n/**\n * @deprecated Use `CoreTool` instead.\n */\nexport type ExperimentalTool = CoreTool;\n","export {\n  APICallError,\n  EmptyResponseBodyError,\n  InvalidArgumentError,\n  InvalidDataContentError,\n  InvalidPromptError,\n  InvalidResponseDataError,\n  InvalidToolArgumentsError,\n  JSONParseError,\n  LoadAPIKeyError,\n  NoObjectGeneratedError,\n  NoSuchToolError,\n  RetryError,\n  ToolCallParseError,\n  TypeValidationError,\n  UnsupportedFunctionalityError,\n  UnsupportedJSONSchemaError,\n} from '@ai-sdk/provider';\n","/**\n * Calculates the cosine similarity between two vectors. This is a useful metric for\n * comparing the similarity of two vectors such as embeddings.\n *\n * @param vector1 - The first vector.\n * @param vector2 - The second vector.\n *\n * @returns The cosine similarity between vector1 and vector2.\n * @throws {Error} If the vectors do not have the same length.\n */\nexport function cosineSimilarity(vector1: number[], vector2: number[]) {\n  if (vector1.length !== vector2.length) {\n    throw new Error(\n      `Vectors must have the same length (vector1: ${vector1.length} elements, vector2: ${vector2.length} elements)`,\n    );\n  }\n\n  return (\n    dotProduct(vector1, vector2) / (magnitude(vector1) * magnitude(vector2))\n  );\n}\n\n/**\n * Calculates the dot product of two vectors.\n * @param vector1 - The first vector.\n * @param vector2 - The second vector.\n * @returns The dot product of vector1 and vector2.\n */\nfunction dotProduct(vector1: number[], vector2: number[]) {\n  return vector1.reduce(\n    (accumulator: number, value: number, index: number) =>\n      accumulator + value * vector2[index]!,\n    0,\n  );\n}\n\n/**\n * Calculates the magnitude of a vector.\n * @param vector - The vector.\n * @returns The magnitude of the vector.\n */\nfunction magnitude(vector: number[]) {\n  return Math.sqrt(dotProduct(vector, vector));\n}\n","import {\n  createParser,\n  type EventSourceParser,\n  type ParsedEvent,\n  type ReconnectInterval,\n} from 'eventsource-parser';\nimport { OpenAIStreamCallbacks } from './openai-stream';\n\nexport interface FunctionCallPayload {\n  name: string;\n  arguments: Record<string, unknown>;\n}\nexport interface ToolCallPayload {\n  tools: {\n    id: string;\n    type: 'function';\n    func: {\n      name: string;\n      arguments: Record<string, unknown>;\n    };\n  }[];\n}\n\n/**\n * Configuration options and helper callback methods for AIStream stream lifecycle events.\n * @interface\n */\nexport interface AIStreamCallbacksAndOptions {\n  /** `onStart`: Called once when the stream is initialized. */\n  onStart?: () => Promise<void> | void;\n  /** `onCompletion`: Called for each tokenized message. */\n  onCompletion?: (completion: string) => Promise<void> | void;\n  /** `onFinal`: Called once when the stream is closed with the final completion message. */\n  onFinal?: (completion: string) => Promise<void> | void;\n  /** `onToken`: Called for each tokenized message. */\n  onToken?: (token: string) => Promise<void> | void;\n  /** `onText`: Called for each text chunk. */\n  onText?: (text: string) => Promise<void> | void;\n  /**\n   * @deprecated This flag is no longer used and only retained for backwards compatibility.\n   * You can remove it from your code.\n   */\n  experimental_streamData?: boolean;\n}\n\n/**\n * Options for the AIStreamParser.\n * @interface\n * @property {string} event - The event (type) from the server side event stream.\n */\nexport interface AIStreamParserOptions {\n  event?: string;\n}\n\n/**\n * Custom parser for AIStream data.\n * @interface\n * @param {string} data - The data to be parsed.\n * @param {AIStreamParserOptions} options - The options for the parser.\n * @returns {string | void} The parsed data or void.\n */\nexport interface AIStreamParser {\n  (data: string, options: AIStreamParserOptions):\n    | string\n    | void\n    | { isText: false; content: string };\n}\n\n/**\n * Creates a TransformStream that parses events from an EventSource stream using a custom parser.\n * @param {AIStreamParser} customParser - Function to handle event data.\n * @returns {TransformStream<Uint8Array, string>} TransformStream parsing events.\n */\nexport function createEventStreamTransformer(\n  customParser?: AIStreamParser,\n): TransformStream<Uint8Array, string | { isText: false; content: string }> {\n  const textDecoder = new TextDecoder();\n  let eventSourceParser: EventSourceParser;\n\n  return new TransformStream({\n    async start(controller): Promise<void> {\n      eventSourceParser = createParser(\n        (event: ParsedEvent | ReconnectInterval) => {\n          if (\n            ('data' in event &&\n              event.type === 'event' &&\n              event.data === '[DONE]') ||\n            // Replicate doesn't send [DONE] but does send a 'done' event\n            // @see https://replicate.com/docs/streaming\n            (event as any).event === 'done'\n          ) {\n            controller.terminate();\n            return;\n          }\n\n          if ('data' in event) {\n            const parsedMessage = customParser\n              ? customParser(event.data, {\n                  event: event.event,\n                })\n              : event.data;\n            if (parsedMessage) controller.enqueue(parsedMessage);\n          }\n        },\n      );\n    },\n\n    transform(chunk) {\n      eventSourceParser.feed(textDecoder.decode(chunk));\n    },\n  });\n}\n\n/**\n * Creates a transform stream that encodes input messages and invokes optional callback functions.\n * The transform stream uses the provided callbacks to execute custom logic at different stages of the stream's lifecycle.\n * - `onStart`: Called once when the stream is initialized.\n * - `onToken`: Called for each tokenized message.\n * - `onCompletion`: Called every time an AIStream completion message is received. This can occur multiple times when using e.g. OpenAI functions\n * - `onFinal`: Called once when the stream is closed with the final completion message.\n *\n * This function is useful when you want to process a stream of messages and perform specific actions during the stream's lifecycle.\n *\n * @param {AIStreamCallbacksAndOptions} [callbacks] - An object containing the callback functions.\n * @return {TransformStream<string, Uint8Array>} A transform stream that encodes input messages as Uint8Array and allows the execution of custom logic through callbacks.\n *\n * @example\n * const callbacks = {\n *   onStart: async () => console.log('Stream started'),\n *   onToken: async (token) => console.log(`Token: ${token}`),\n *   onCompletion: async (completion) => console.log(`Completion: ${completion}`)\n *   onFinal: async () => data.close()\n * };\n * const transformer = createCallbacksTransformer(callbacks);\n */\nexport function createCallbacksTransformer(\n  cb: AIStreamCallbacksAndOptions | OpenAIStreamCallbacks | undefined,\n): TransformStream<string | { isText: false; content: string }, Uint8Array> {\n  const textEncoder = new TextEncoder();\n  let aggregatedResponse = '';\n  const callbacks = cb || {};\n\n  return new TransformStream({\n    async start(): Promise<void> {\n      if (callbacks.onStart) await callbacks.onStart();\n    },\n\n    async transform(message, controller): Promise<void> {\n      const content = typeof message === 'string' ? message : message.content;\n\n      controller.enqueue(textEncoder.encode(content));\n\n      aggregatedResponse += content;\n\n      if (callbacks.onToken) await callbacks.onToken(content);\n      if (callbacks.onText && typeof message === 'string') {\n        await callbacks.onText(message);\n      }\n    },\n\n    async flush(): Promise<void> {\n      const isOpenAICallbacks = isOfTypeOpenAIStreamCallbacks(callbacks);\n      // If it's OpenAICallbacks, it has an experimental_onFunctionCall which means that the createFunctionCallTransformer\n      // will handle calling onComplete.\n      if (callbacks.onCompletion) {\n        await callbacks.onCompletion(aggregatedResponse);\n      }\n\n      if (callbacks.onFinal && !isOpenAICallbacks) {\n        await callbacks.onFinal(aggregatedResponse);\n      }\n    },\n  });\n}\n\nfunction isOfTypeOpenAIStreamCallbacks(\n  callbacks: AIStreamCallbacksAndOptions | OpenAIStreamCallbacks,\n): callbacks is OpenAIStreamCallbacks {\n  return 'experimental_onFunctionCall' in callbacks;\n}\n/**\n * Returns a stateful function that, when invoked, trims leading whitespace\n * from the input text. The trimming only occurs on the first invocation, ensuring that\n * subsequent calls do not alter the input text. This is particularly useful in scenarios\n * where a text stream is being processed and only the initial whitespace should be removed.\n *\n * @return {function(string): string} A function that takes a string as input and returns a string\n * with leading whitespace removed if it is the first invocation; otherwise, it returns the input unchanged.\n *\n * @example\n * const trimStart = trimStartOfStreamHelper();\n * const output1 = trimStart(\"   text\"); // \"text\"\n * const output2 = trimStart(\"   text\"); // \"   text\"\n *\n */\nexport function trimStartOfStreamHelper(): (text: string) => string {\n  let isStreamStart = true;\n\n  return (text: string): string => {\n    if (isStreamStart) {\n      text = text.trimStart();\n      if (text) isStreamStart = false;\n    }\n    return text;\n  };\n}\n\n/**\n * Returns a ReadableStream created from the response, parsed and handled with custom logic.\n * The stream goes through two transformation stages, first parsing the events and then\n * invoking the provided callbacks.\n *\n * For 2xx HTTP responses:\n * - The function continues with standard stream processing.\n *\n * For non-2xx HTTP responses:\n * - If the response body is defined, it asynchronously extracts and decodes the response body.\n * - It then creates a custom ReadableStream to propagate a detailed error message.\n *\n * @param {Response} response - The response.\n * @param {AIStreamParser} customParser - The custom parser function.\n * @param {AIStreamCallbacksAndOptions} callbacks - The callbacks.\n * @return {ReadableStream} The AIStream.\n * @throws Will throw an error if the response is not OK.\n */\nexport function AIStream(\n  response: Response,\n  customParser?: AIStreamParser,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream<Uint8Array> {\n  if (!response.ok) {\n    if (response.body) {\n      const reader = response.body.getReader();\n      return new ReadableStream({\n        async start(controller) {\n          const { done, value } = await reader.read();\n          if (!done) {\n            const errorText = new TextDecoder().decode(value);\n            controller.error(new Error(`Response error: ${errorText}`));\n          }\n        },\n      });\n    } else {\n      return new ReadableStream({\n        start(controller) {\n          controller.error(new Error('Response error: No response body'));\n        },\n      });\n    }\n  }\n\n  const responseBodyStream = response.body || createEmptyReadableStream();\n\n  return responseBodyStream\n    .pipeThrough(createEventStreamTransformer(customParser))\n    .pipeThrough(createCallbacksTransformer(callbacks));\n}\n\n// outputs lines like\n// 0: chunk\n// 0: more chunk\n// 1: a fct call\n// z: added data from Data\n\n/**\n * Creates an empty ReadableStream that immediately closes upon creation.\n * This function is used as a fallback for creating a ReadableStream when the response body is null or undefined,\n * ensuring that the subsequent pipeline processing doesn't fail due to a lack of a stream.\n *\n * @returns {ReadableStream} An empty and closed ReadableStream instance.\n */\nfunction createEmptyReadableStream(): ReadableStream {\n  return new ReadableStream({\n    start(controller) {\n      controller.close();\n    },\n  });\n}\n\n/**\n * Implements ReadableStream.from(asyncIterable), which isn't documented in MDN and isn't implemented in node.\n * https://github.com/whatwg/streams/commit/8d7a0bf26eb2cc23e884ddbaac7c1da4b91cf2bc\n */\nexport function readableFromAsyncIterable<T>(iterable: AsyncIterable<T>) {\n  let it = iterable[Symbol.asyncIterator]();\n  return new ReadableStream<T>({\n    async pull(controller) {\n      const { done, value } = await it.next();\n      if (done) controller.close();\n      else controller.enqueue(value);\n    },\n\n    async cancel(reason) {\n      await it.return?.(reason);\n    },\n  });\n}\n","import { JSONValue, formatStreamPart } from '@ai-sdk/ui-utils';\n\n/**\n * A stream wrapper to send custom JSON-encoded data back to the client.\n */\nexport class StreamData {\n  private encoder = new TextEncoder();\n\n  private controller: ReadableStreamController<Uint8Array> | null = null;\n  public stream: ReadableStream<Uint8Array>;\n\n  private isClosed: boolean = false;\n  private warningTimeout: NodeJS.Timeout | null = null;\n\n  constructor() {\n    const self = this;\n\n    this.stream = new ReadableStream({\n      start: async controller => {\n        self.controller = controller;\n\n        // Set a timeout to show a warning if the stream is not closed within 3 seconds\n        if (process.env.NODE_ENV === 'development') {\n          self.warningTimeout = setTimeout(() => {\n            console.warn(\n              'The data stream is hanging. Did you forget to close it with `data.close()`?',\n            );\n          }, 3000);\n        }\n      },\n      pull: controller => {\n        // No-op: we don't need to do anything special on pull\n      },\n      cancel: reason => {\n        this.isClosed = true;\n      },\n    });\n  }\n\n  async close(): Promise<void> {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.close();\n    this.isClosed = true;\n\n    // Clear the warning timeout if the stream is closed\n    if (this.warningTimeout) {\n      clearTimeout(this.warningTimeout);\n    }\n  }\n\n  append(value: JSONValue): void {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.enqueue(\n      this.encoder.encode(formatStreamPart('data', [value])),\n    );\n  }\n\n  appendMessageAnnotation(value: JSONValue): void {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.enqueue(\n      this.encoder.encode(formatStreamPart('message_annotations', [value])),\n    );\n  }\n}\n\n/**\n * A TransformStream for LLMs that do not have their own transform stream handlers managing encoding (e.g. OpenAIStream has one for function call handling).\n * This assumes every chunk is a 'text' chunk.\n */\nexport function createStreamDataTransformer() {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n  return new TransformStream({\n    transform: async (chunk, controller) => {\n      const message = decoder.decode(chunk);\n      controller.enqueue(encoder.encode(formatStreamPart('text', message)));\n    },\n  });\n}\n\n/**\n@deprecated Use `StreamData` instead.\n */\nexport class experimental_StreamData extends StreamData {}\n","import {\n  AIStream,\n  readableFromAsyncIterable,\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\n// from anthropic sdk (Completion)\ninterface CompletionChunk {\n  /**\n   * Unique object identifier.\n   *\n   * The format and length of IDs may change over time.\n   */\n  id: string;\n\n  /**\n   * The resulting completion up to and excluding the stop sequences.\n   */\n  completion: string;\n\n  /**\n   * The model that handled the request.\n   */\n  model: string;\n\n  /**\n   * The reason that we stopped.\n   *\n   * This may be one the following values:\n   *\n   * - `\"stop_sequence\"`: we reached a stop sequence — either provided by you via the\n   *   `stop_sequences` parameter, or a stop sequence built into the model\n   * - `\"max_tokens\"`: we exceeded `max_tokens_to_sample` or the model's maximum\n   */\n  stop_reason: string | null;\n\n  /**\n   * Object type.\n   *\n   * For Text Completions, this is always `\"completion\"`.\n   */\n  type: 'completion';\n}\n\ninterface StreamError {\n  error: {\n    type: string;\n    message: string;\n  };\n}\n\ninterface StreamPing {}\n\ntype StreamData = CompletionChunk | StreamError | StreamPing;\n\ninterface Message {\n  id: string;\n  content: Array<ContentBlock>;\n  model: string;\n  role: 'assistant';\n  stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | null;\n  stop_sequence: string | null;\n  type: 'message';\n}\n\ninterface ContentBlock {\n  text: string;\n  type: 'text';\n}\n\ninterface TextDelta {\n  text: string;\n  type: 'text_delta';\n}\n\ninterface ContentBlockDeltaEvent {\n  delta: TextDelta;\n  index: number;\n  type: 'content_block_delta';\n}\n\ninterface ContentBlockStartEvent {\n  content_block: ContentBlock;\n  index: number;\n  type: 'content_block_start';\n}\n\ninterface ContentBlockStopEvent {\n  index: number;\n  type: 'content_block_stop';\n}\n\ninterface MessageDeltaEventDelta {\n  stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | null;\n  stop_sequence: string | null;\n}\n\ninterface MessageDeltaEvent {\n  delta: MessageDeltaEventDelta;\n  type: 'message_delta';\n}\n\ntype MessageStreamEvent =\n  | MessageStartEvent\n  | MessageDeltaEvent\n  | MessageStopEvent\n  | ContentBlockStartEvent\n  | ContentBlockDeltaEvent\n  | ContentBlockStopEvent;\n\ninterface MessageStartEvent {\n  message: Message;\n  type: 'message_start';\n}\n\ninterface MessageStopEvent {\n  type: 'message_stop';\n}\n\nfunction parseAnthropicStream(): (data: string) => string | void {\n  let previous = '';\n\n  return data => {\n    const json = JSON.parse(data as string) as StreamData;\n\n    // error event\n    if ('error' in json) {\n      throw new Error(`${json.error.type}: ${json.error.message}`);\n    }\n\n    // ping event\n    if (!('completion' in json)) {\n      return;\n    }\n\n    // On API versions older than 2023-06-01,\n    // Anthropic's `completion` field is cumulative unlike OpenAI's\n    // deltas. In order to compute the delta, we must slice out the text\n    // we previously received.\n    const text = json.completion;\n    if (\n      !previous ||\n      (text.length > previous.length && text.startsWith(previous))\n    ) {\n      const delta = text.slice(previous.length);\n      previous = text;\n\n      return delta;\n    }\n\n    return text;\n  };\n}\n\nasync function* streamable(\n  stream: AsyncIterable<CompletionChunk> | AsyncIterable<MessageStreamEvent>,\n) {\n  for await (const chunk of stream) {\n    if ('completion' in chunk) {\n      // completion stream\n      const text = chunk.completion;\n      if (text) yield text;\n    } else if ('delta' in chunk) {\n      // messge stream\n      const { delta } = chunk;\n      if ('text' in delta) {\n        const text = delta.text;\n        if (text) yield text;\n      }\n    }\n  }\n}\n\n/**\n * Accepts either a fetch Response from the Anthropic `POST /v1/complete` endpoint,\n * or the return value of `await client.completions.create({ stream: true })`\n * from the `@anthropic-ai/sdk` package.\n *\n * @deprecated Use the [Anthropic provider](https://sdk.vercel.ai/providers/ai-sdk-providers/anthropic) instead.\n */\nexport function AnthropicStream(\n  res:\n    | Response\n    | AsyncIterable<CompletionChunk>\n    | AsyncIterable<MessageStreamEvent>,\n  cb?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  if (Symbol.asyncIterator in res) {\n    return readableFromAsyncIterable(streamable(res))\n      .pipeThrough(createCallbacksTransformer(cb))\n      .pipeThrough(createStreamDataTransformer());\n  } else {\n    return AIStream(res, parseAnthropicStream(), cb).pipeThrough(\n      createStreamDataTransformer(),\n    );\n  }\n}\n","import {\n  AssistantMessage,\n  DataMessage,\n  formatStreamPart,\n} from '@ai-sdk/ui-utils';\nimport { type AssistantStream } from 'openai/lib/AssistantStream';\nimport { Run } from 'openai/resources/beta/threads/runs/runs';\n\n/**\nYou can pass the thread and the latest message into the `AssistantResponse`. This establishes the context for the response.\n */\ntype AssistantResponseSettings = {\n  /**\nThe thread ID that the response is associated with.\n   */\n  threadId: string;\n\n  /**\nThe ID of the latest message that the response is associated with.\n */\n  messageId: string;\n};\n\n/**\nThe process parameter is a callback in which you can run the assistant on threads, and send messages and data messages to the client.\n */\ntype AssistantResponseCallback = (options: {\n  /**\n@deprecated use variable from outer scope instead.\n   */\n  threadId: string;\n\n  /**\n@deprecated use variable from outer scope instead.\n   */\n  messageId: string;\n\n  /**\nForwards an assistant message (non-streaming) to the client.\n   */\n  sendMessage: (message: AssistantMessage) => void;\n\n  /**\nSend a data message to the client. You can use this to provide information for rendering custom UIs while the assistant is processing the thread.\n */\n  sendDataMessage: (message: DataMessage) => void;\n\n  /**\nForwards the assistant response stream to the client. Returns the `Run` object after it completes, or when it requires an action.\n   */\n  forwardStream: (stream: AssistantStream) => Promise<Run | undefined>;\n}) => Promise<void>;\n\n/**\nThe `AssistantResponse` allows you to send a stream of assistant update to `useAssistant`.\nIt is designed to facilitate streaming assistant responses to the `useAssistant` hook.\nIt receives an assistant thread and a current message, and can send messages and data messages to the client.\n */\nexport function AssistantResponse(\n  { threadId, messageId }: AssistantResponseSettings,\n  process: AssistantResponseCallback,\n): Response {\n  const stream = new ReadableStream({\n    async start(controller) {\n      const textEncoder = new TextEncoder();\n\n      const sendMessage = (message: AssistantMessage) => {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('assistant_message', message)),\n        );\n      };\n\n      const sendDataMessage = (message: DataMessage) => {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('data_message', message)),\n        );\n      };\n\n      const sendError = (errorMessage: string) => {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('error', errorMessage)),\n        );\n      };\n\n      const forwardStream = async (stream: AssistantStream) => {\n        let result: Run | undefined = undefined;\n\n        for await (const value of stream) {\n          switch (value.event) {\n            case 'thread.message.created': {\n              controller.enqueue(\n                textEncoder.encode(\n                  formatStreamPart('assistant_message', {\n                    id: value.data.id,\n                    role: 'assistant',\n                    content: [{ type: 'text', text: { value: '' } }],\n                  }),\n                ),\n              );\n              break;\n            }\n\n            case 'thread.message.delta': {\n              const content = value.data.delta.content?.[0];\n\n              if (content?.type === 'text' && content.text?.value != null) {\n                controller.enqueue(\n                  textEncoder.encode(\n                    formatStreamPart('text', content.text.value),\n                  ),\n                );\n              }\n\n              break;\n            }\n\n            case 'thread.run.completed':\n            case 'thread.run.requires_action': {\n              result = value.data;\n              break;\n            }\n          }\n        }\n\n        return result;\n      };\n\n      // send the threadId and messageId as the first message:\n      controller.enqueue(\n        textEncoder.encode(\n          formatStreamPart('assistant_control_data', {\n            threadId,\n            messageId,\n          }),\n        ),\n      );\n\n      try {\n        await process({\n          threadId,\n          messageId,\n          sendMessage,\n          sendDataMessage,\n          forwardStream,\n        });\n      } catch (error) {\n        sendError((error as any).message ?? `${error}`);\n      } finally {\n        controller.close();\n      }\n    },\n    pull(controller) {},\n    cancel() {},\n  });\n\n  return new Response(stream, {\n    status: 200,\n    headers: {\n      'Content-Type': 'text/plain; charset=utf-8',\n    },\n  });\n}\n\n/**\n@deprecated Use `AssistantResponse` instead.\n */\nexport const experimental_AssistantResponse = AssistantResponse;\n","import {\n  AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\ninterface AWSBedrockResponse {\n  body?: AsyncIterable<{\n    chunk?: { bytes?: Uint8Array };\n  }>;\n}\n\nasync function* asDeltaIterable(\n  response: AWSBedrockResponse,\n  extractTextDeltaFromChunk: (chunk: any) => string,\n) {\n  const decoder = new TextDecoder();\n  for await (const chunk of response.body ?? []) {\n    const bytes = chunk.chunk?.bytes;\n\n    if (bytes != null) {\n      const chunkText = decoder.decode(bytes);\n      const chunkJSON = JSON.parse(chunkText);\n      const delta = extractTextDeltaFromChunk(chunkJSON);\n\n      if (delta != null) {\n        yield delta;\n      }\n    }\n  }\n}\n\nexport function AWSBedrockAnthropicMessagesStream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk.delta?.text);\n}\n\nexport function AWSBedrockAnthropicStream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk.completion);\n}\n\nexport function AWSBedrockCohereStream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk?.text);\n}\n\nexport function AWSBedrockLlama2Stream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk.generation);\n}\n\nexport function AWSBedrockStream(\n  response: AWSBedrockResponse,\n  callbacks: AIStreamCallbacksAndOptions | undefined,\n  extractTextDeltaFromChunk: (chunk: any) => string,\n) {\n  return readableFromAsyncIterable(\n    asDeltaIterable(response, extractTextDeltaFromChunk),\n  )\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n","import {\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\nconst utf8Decoder = new TextDecoder('utf-8');\n\n// Full types\n// @see: https://github.com/cohere-ai/cohere-typescript/blob/c2eceb4a845098240ba0bc44e3787ccf75e268e8/src/api/types/StreamedChatResponse.ts\ninterface StreamChunk {\n  text?: string;\n  eventType:\n    | 'stream-start'\n    | 'search-queries-generation'\n    | 'search-results'\n    | 'text-generation'\n    | 'citation-generation'\n    | 'stream-end';\n}\n\nasync function processLines(\n  lines: string[],\n  controller: ReadableStreamDefaultController<string>,\n) {\n  for (const line of lines) {\n    const { text, is_finished } = JSON.parse(line);\n\n    // closing the reader is handed in readAndProcessLines\n    if (!is_finished) {\n      controller.enqueue(text);\n    }\n  }\n}\n\nasync function readAndProcessLines(\n  reader: ReadableStreamDefaultReader<Uint8Array>,\n  controller: ReadableStreamDefaultController<string>,\n) {\n  let segment = '';\n\n  while (true) {\n    const { value: chunk, done } = await reader.read();\n    if (done) {\n      break;\n    }\n\n    segment += utf8Decoder.decode(chunk, { stream: true });\n\n    const linesArray = segment.split(/\\r\\n|\\n|\\r/g);\n    segment = linesArray.pop() || '';\n\n    await processLines(linesArray, controller);\n  }\n\n  if (segment) {\n    const linesArray = [segment];\n    await processLines(linesArray, controller);\n  }\n\n  controller.close();\n}\n\nfunction createParser(res: Response) {\n  const reader = res.body?.getReader();\n\n  return new ReadableStream<string>({\n    async start(controller): Promise<void> {\n      if (!reader) {\n        controller.close();\n        return;\n      }\n\n      await readAndProcessLines(reader, controller);\n    },\n  });\n}\n\nasync function* streamable(stream: AsyncIterable<StreamChunk>) {\n  for await (const chunk of stream) {\n    if (chunk.eventType === 'text-generation') {\n      const text = chunk.text;\n      if (text) yield text;\n    }\n  }\n}\n\nexport function CohereStream(\n  reader: Response | AsyncIterable<StreamChunk>,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  if (Symbol.asyncIterator in reader) {\n    return readableFromAsyncIterable(streamable(reader))\n      .pipeThrough(createCallbacksTransformer(callbacks))\n      .pipeThrough(createStreamDataTransformer());\n  } else {\n    return createParser(reader)\n      .pipeThrough(createCallbacksTransformer(callbacks))\n      .pipeThrough(createStreamDataTransformer());\n  }\n}\n","import {\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n  type AIStreamCallbacksAndOptions,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\ninterface GenerateContentResponse {\n  candidates?: GenerateContentCandidate[];\n}\n\ninterface GenerateContentCandidate {\n  index: number;\n  content: Content;\n}\n\ninterface Content {\n  role: string;\n  parts: Part[];\n}\n\ntype Part = TextPart | InlineDataPart;\n\ninterface InlineDataPart {\n  text?: never;\n}\n\ninterface TextPart {\n  text: string;\n  inlineData?: never;\n}\n\nasync function* streamable(response: {\n  stream: AsyncIterable<GenerateContentResponse>;\n}) {\n  for await (const chunk of response.stream) {\n    const parts = chunk.candidates?.[0]?.content?.parts;\n\n    if (parts === undefined) {\n      continue;\n    }\n\n    const firstPart = parts[0];\n\n    if (typeof firstPart.text === 'string') {\n      yield firstPart.text;\n    }\n  }\n}\n\n/**\n * @deprecated Use the [Google Generative AI provider](https://sdk.vercel.ai/providers/ai-sdk-providers/google-generative-ai) instead.\n */\nexport function GoogleGenerativeAIStream(\n  response: {\n    stream: AsyncIterable<GenerateContentResponse>;\n  },\n  cb?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return readableFromAsyncIterable(streamable(response))\n    .pipeThrough(createCallbacksTransformer(cb))\n    .pipeThrough(createStreamDataTransformer());\n}\n","import {\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n  trimStartOfStreamHelper,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\nfunction createParser(res: AsyncGenerator<any>) {\n  const trimStartOfStream = trimStartOfStreamHelper();\n  return new ReadableStream<string>({\n    async pull(controller): Promise<void> {\n      const { value, done } = await res.next();\n\n      if (done) {\n        controller.close();\n        return;\n      }\n\n      const text = trimStartOfStream(value.token?.text ?? '');\n      if (!text) return;\n\n      // some HF models return generated_text instead of a real ending token\n      if (value.generated_text != null && value.generated_text.length > 0) {\n        return;\n      }\n\n      // <|endoftext|> is for https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n      // <|end|> is for https://huggingface.co/HuggingFaceH4/starchat-beta\n      // </s> is also often last token in the stream depending on the model\n      if (text === '</s>' || text === '<|endoftext|>' || text === '<|end|>') {\n        return;\n      }\n\n      controller.enqueue(text);\n    },\n  });\n}\n\nexport function HuggingFaceStream(\n  res: AsyncGenerator<any>,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return createParser(res)\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n","// packages/core/streams/inkeep-stream.ts\nimport {\n  AIStream,\n  type AIStreamCallbacksAndOptions,\n  AIStreamParser,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\nexport type InkeepOnFinalMetadata = {\n  chat_session_id: string;\n  records_cited: any;\n};\n\nexport type InkeepChatResultCallbacks = {\n  onFinal?: (\n    completion: string,\n    metadata?: InkeepOnFinalMetadata,\n  ) => Promise<void> | void;\n  onRecordsCited?: (\n    records_cited: InkeepOnFinalMetadata['records_cited'],\n  ) => void;\n};\n\nexport type InkeepAIStreamCallbacksAndOptions = AIStreamCallbacksAndOptions &\n  InkeepChatResultCallbacks;\n\nexport function InkeepStream(\n  res: Response,\n  callbacks?: InkeepAIStreamCallbacksAndOptions,\n): ReadableStream {\n  if (!res.body) {\n    throw new Error('Response body is null');\n  }\n\n  let chat_session_id = '';\n  let records_cited: any;\n\n  const inkeepEventParser: AIStreamParser = (data: string, options) => {\n    const { event } = options;\n\n    if (event === 'records_cited') {\n      records_cited = JSON.parse(data) as any;\n      callbacks?.onRecordsCited?.(records_cited);\n    }\n\n    if (event === 'message_chunk') {\n      const inkeepMessageChunk = JSON.parse(data);\n      chat_session_id = inkeepMessageChunk.chat_session_id ?? chat_session_id;\n      return inkeepMessageChunk.content_chunk;\n    }\n    return;\n  };\n\n  let { onRecordsCited, ...passThroughCallbacks } = callbacks || {};\n\n  // extend onFinal callback with Inkeep specific metadata\n  passThroughCallbacks = {\n    ...passThroughCallbacks,\n    onFinal: completion => {\n      const inkeepOnFinalMetadata: InkeepOnFinalMetadata = {\n        chat_session_id,\n        records_cited,\n      };\n      callbacks?.onFinal?.(completion, inkeepOnFinalMetadata);\n    },\n  };\n\n  return AIStream(res, inkeepEventParser, passThroughCallbacks).pipeThrough(\n    createStreamDataTransformer(),\n  );\n}\n","import {\n  AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\ntype LangChainImageDetail = 'auto' | 'low' | 'high';\n\ntype LangChainMessageContentText = {\n  type: 'text';\n  text: string;\n};\n\ntype LangChainMessageContentImageUrl = {\n  type: 'image_url';\n  image_url:\n    | string\n    | {\n        url: string;\n        detail?: LangChainImageDetail;\n      };\n};\n\ntype LangChainMessageContentComplex =\n  | LangChainMessageContentText\n  | LangChainMessageContentImageUrl\n  | (Record<string, any> & {\n      type?: 'text' | 'image_url' | string;\n    })\n  | (Record<string, any> & {\n      type?: never;\n    });\n\ntype LangChainMessageContent = string | LangChainMessageContentComplex[];\n\ntype LangChainAIMessageChunk = {\n  content: LangChainMessageContent;\n};\n\n/**\nConverts LangChain output streams to AIStream. \n\nThe following streams are supported:\n- `LangChainAIMessageChunk` streams (LangChain `model.stream` output)\n- `string` streams (LangChain `StringOutputParser` output)\n */\nexport function toAIStream(\n  stream: ReadableStream<LangChainAIMessageChunk> | ReadableStream<string>,\n  callbacks?: AIStreamCallbacksAndOptions,\n) {\n  return stream\n    .pipeThrough(\n      new TransformStream<LangChainAIMessageChunk | string>({\n        transform: async (chunk, controller) => {\n          if (typeof chunk === 'string') {\n            controller.enqueue(chunk);\n          } else if (typeof chunk.content === 'string') {\n            controller.enqueue(chunk.content);\n          } else {\n            const content: LangChainMessageContentComplex[] = chunk.content;\n            for (const item of content) {\n              if (item.type === 'text') {\n                controller.enqueue(item.text);\n              }\n            }\n          }\n        },\n      }),\n    )\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n","import {\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\n/**\n * @deprecated Use [LangChainAdapter](https://sdk.vercel.ai/providers/adapters/langchain) instead.\n */\nexport function LangChainStream(callbacks?: AIStreamCallbacksAndOptions) {\n  const stream = new TransformStream();\n  const writer = stream.writable.getWriter();\n\n  const runs = new Set();\n\n  const handleError = async (e: Error, runId: string) => {\n    runs.delete(runId);\n    await writer.ready;\n    await writer.abort(e);\n  };\n\n  const handleStart = async (runId: string) => {\n    runs.add(runId);\n  };\n\n  const handleEnd = async (runId: string) => {\n    runs.delete(runId);\n\n    if (runs.size === 0) {\n      await writer.ready;\n      await writer.close();\n    }\n  };\n\n  return {\n    stream: stream.readable\n      .pipeThrough(createCallbacksTransformer(callbacks))\n      .pipeThrough(createStreamDataTransformer()),\n    writer,\n    handlers: {\n      handleLLMNewToken: async (token: string) => {\n        await writer.ready;\n        await writer.write(token);\n      },\n      handleLLMStart: async (_llm: any, _prompts: string[], runId: string) => {\n        handleStart(runId);\n      },\n      handleLLMEnd: async (_output: any, runId: string) => {\n        await handleEnd(runId);\n      },\n      handleLLMError: async (e: Error, runId: string) => {\n        await handleError(e, runId);\n      },\n      handleChainStart: async (_chain: any, _inputs: any, runId: string) => {\n        handleStart(runId);\n      },\n      handleChainEnd: async (_outputs: any, runId: string) => {\n        await handleEnd(runId);\n      },\n      handleChainError: async (e: Error, runId: string) => {\n        await handleError(e, runId);\n      },\n      handleToolStart: async (_tool: any, _input: string, runId: string) => {\n        handleStart(runId);\n      },\n      handleToolEnd: async (_output: string, runId: string) => {\n        await handleEnd(runId);\n      },\n      handleToolError: async (e: Error, runId: string) => {\n        await handleError(e, runId);\n      },\n    },\n  };\n}\n","import {\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n  type AIStreamCallbacksAndOptions,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\ninterface ChatCompletionResponseChunk {\n  id: string;\n  object: 'chat.completion.chunk';\n  created: number;\n  model: string;\n  choices: ChatCompletionResponseChunkChoice[];\n}\n\ninterface ChatCompletionResponseChunkChoice {\n  index: number;\n  delta: {\n    role?: string;\n    content?: string;\n    tool_calls?: ToolCalls[];\n  };\n  finish_reason: string;\n}\n\ninterface FunctionCall {\n  name: string;\n  arguments: string;\n}\n\ninterface ToolCalls {\n  id: 'null';\n  type: 'function';\n  function: FunctionCall;\n}\n\nasync function* streamable(stream: AsyncIterable<ChatCompletionResponseChunk>) {\n  for await (const chunk of stream) {\n    const content = chunk.choices[0]?.delta?.content;\n\n    if (content === undefined || content === '') {\n      continue;\n    }\n\n    yield content;\n  }\n}\n\n/*\n * @deprecated Use the [Mistral provider](https://sdk.vercel.ai/providers/ai-sdk-providers/mistral) instead.\n */\nexport function MistralStream(\n  response: AsyncGenerator<ChatCompletionResponseChunk, void, unknown>,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  const stream = readableFromAsyncIterable(streamable(response));\n  return stream\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n","import {\n  CreateMessage,\n  FunctionCall,\n  JSONValue,\n  ToolCall,\n  createChunkDecoder,\n  formatStreamPart,\n} from '@ai-sdk/ui-utils';\nimport {\n  AIStream,\n  FunctionCallPayload,\n  ToolCallPayload,\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n  trimStartOfStreamHelper,\n  type AIStreamCallbacksAndOptions,\n} from './ai-stream';\nimport { AzureChatCompletions } from './azure-openai-types';\nimport { createStreamDataTransformer } from './stream-data';\n\nexport type OpenAIStreamCallbacks = AIStreamCallbacksAndOptions & {\n  /**\n   * @example\n   * ```js\n   * const response = await openai.chat.completions.create({\n   *   model: 'gpt-3.5-turbo-0613',\n   *   stream: true,\n   *   messages,\n   *   functions,\n   * })\n   *\n   * const stream = OpenAIStream(response, {\n   *   experimental_onFunctionCall: async (functionCallPayload, createFunctionCallMessages) => {\n   *     // ... run your custom logic here\n   *     const result = await myFunction(functionCallPayload)\n   *\n   *     // Ask for another completion, or return a string to send to the client as an assistant message.\n   *     return await openai.chat.completions.create({\n   *       model: 'gpt-3.5-turbo-0613',\n   *       stream: true,\n   *       // Append the relevant \"assistant\" and \"function\" call messages\n   *       messages: [...messages, ...createFunctionCallMessages(result)],\n   *       functions,\n   *     })\n   *   }\n   * })\n   * ```\n   */\n  experimental_onFunctionCall?: (\n    functionCallPayload: FunctionCallPayload,\n    createFunctionCallMessages: (\n      functionCallResult: JSONValue,\n    ) => CreateMessage[],\n  ) => Promise<\n    Response | undefined | void | string | AsyncIterableOpenAIStreamReturnTypes\n  >;\n  /**\n   * @example\n   * ```js\n   * const response = await openai.chat.completions.create({\n   *   model: 'gpt-3.5-turbo-1106', // or gpt-4-1106-preview\n   *   stream: true,\n   *   messages,\n   *   tools,\n   *   tool_choice: \"auto\", // auto is default, but we'll be explicit\n   * })\n   *\n   * const stream = OpenAIStream(response, {\n   *   experimental_onToolCall: async (toolCallPayload, appendToolCallMessages) => {\n   *    let messages: CreateMessage[] = []\n   *    //   There might be multiple tool calls, so we need to iterate through them\n   *    for (const tool of toolCallPayload.tools) {\n   *     // ... run your custom logic here\n   *     const result = await myFunction(tool.function)\n   *    // Append the relevant \"assistant\" and \"tool\" call messages\n   *     appendToolCallMessage({tool_call_id:tool.id, function_name:tool.function.name, tool_call_result:result})\n   *    }\n   *     // Ask for another completion, or return a string to send to the client as an assistant message.\n   *     return await openai.chat.completions.create({\n   *       model: 'gpt-3.5-turbo-1106', // or gpt-4-1106-preview\n   *       stream: true,\n   *       // Append the results messages, calling appendToolCallMessage without\n   *       // any arguments will jsut return the accumulated messages\n   *       messages: [...messages, ...appendToolCallMessage()],\n   *       tools,\n   *        tool_choice: \"auto\", // auto is default, but we'll be explicit\n   *     })\n   *   }\n   * })\n   * ```\n   */\n  experimental_onToolCall?: (\n    toolCallPayload: ToolCallPayload,\n    appendToolCallMessage: (result?: {\n      tool_call_id: string;\n      function_name: string;\n      tool_call_result: JSONValue;\n    }) => CreateMessage[],\n  ) => Promise<\n    Response | undefined | void | string | AsyncIterableOpenAIStreamReturnTypes\n  >;\n};\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L28-L40\ninterface ChatCompletionChunk {\n  id: string;\n  choices: Array<ChatCompletionChunkChoice>;\n  created: number;\n  model: string;\n  object: string;\n}\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L43-L49\n// Updated for https://github.com/openai/openai-node/commit/f10c757d831d90407ba47b4659d9cd34b1a35b1d\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ChatCompletionChunkChoice {\n  delta: ChoiceDelta;\n  finish_reason:\n    | 'stop'\n    | 'length'\n    | 'tool_calls'\n    | 'content_filter'\n    | 'function_call'\n    | null;\n  index: number;\n}\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L123-L139\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ChoiceDelta {\n  /**\n   * The contents of the chunk message.\n   */\n  content?: string | null;\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  function_call?: FunctionCall;\n\n  /**\n   * The role of the author of this message.\n   */\n  role?: 'system' | 'user' | 'assistant' | 'tool';\n\n  tool_calls?: Array<DeltaToolCall>;\n}\n\n// From https://github.com/openai/openai-node/blob/master/src/resources/chat/completions.ts\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface DeltaToolCall {\n  index: number;\n\n  /**\n   * The ID of the tool call.\n   */\n  id?: string;\n\n  /**\n   * The function that the model called.\n   */\n  function?: ToolCallFunction;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type?: 'function';\n}\n\n// From https://github.com/openai/openai-node/blob/master/src/resources/chat/completions.ts\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ToolCallFunction {\n  /**\n   * The arguments to call the function with, as generated by the model in JSON\n   * format. Note that the model does not always generate valid JSON, and may\n   * hallucinate parameters not defined by your function schema. Validate the\n   * arguments in your code before calling your function.\n   */\n  arguments?: string;\n\n  /**\n   * The name of the function to call.\n   */\n  name?: string;\n}\n\n/**\n * https://github.com/openai/openai-node/blob/3ec43ee790a2eb6a0ccdd5f25faa23251b0f9b8e/src/resources/completions.ts#L28C1-L64C1\n * Completions API. Streamed and non-streamed responses are the same.\n */\ninterface Completion {\n  /**\n   * A unique identifier for the completion.\n   */\n  id: string;\n\n  /**\n   * The list of completion choices the model generated for the input prompt.\n   */\n  choices: Array<CompletionChoice>;\n\n  /**\n   * The Unix timestamp of when the completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"text_completion\"\n   */\n  object: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionUsage;\n}\n\ninterface CompletionChoice {\n  /**\n   * The reason the model stopped generating tokens. This will be `stop` if the model\n   * hit a natural stop point or a provided stop sequence, or `length` if the maximum\n   * number of tokens specified in the request was reached.\n   */\n  finish_reason: 'stop' | 'length' | 'content_filter';\n\n  index: number;\n\n  // edited: Removed CompletionChoice.logProbs and replaced with any\n  logprobs: any | null;\n\n  text: string;\n}\n\nexport interface CompletionUsage {\n  /**\n   * Usage statistics for the completion request.\n   */\n\n  /**\n   * Number of tokens in the generated completion.\n   */\n  completion_tokens: number;\n\n  /**\n   * Number of tokens in the prompt.\n   */\n  prompt_tokens: number;\n\n  /**\n   * Total number of tokens used in the request (prompt + completion).\n   */\n  total_tokens: number;\n}\n\n/**\n * Creates a parser function for processing the OpenAI stream data.\n * The parser extracts and trims text content from the JSON data. This parser\n * can handle data for chat or completion models.\n *\n * @return {(data: string) => string | void| { isText: false; content: string }}\n * A parser function that takes a JSON string as input and returns the extracted text content,\n * a complex object with isText: false for function/tool calls, or nothing.\n */\nfunction parseOpenAIStream(): (\n  data: string,\n) => string | void | { isText: false; content: string } {\n  const extract = chunkToText();\n  return data => extract(JSON.parse(data) as OpenAIStreamReturnTypes);\n}\n\n/**\n * Reads chunks from OpenAI's new Streamable interface, which is essentially\n * the same as the old Response body interface with an included SSE parser\n * doing the parsing for us.\n */\nasync function* streamable(stream: AsyncIterableOpenAIStreamReturnTypes) {\n  const extract = chunkToText();\n\n  for await (let chunk of stream) {\n    // convert chunk if it is an Azure chat completion. Azure does not expose all\n    // properties in the interfaces, and also uses camelCase instead of snake_case\n    if ('promptFilterResults' in chunk) {\n      chunk = {\n        id: chunk.id,\n        created: chunk.created.getDate(),\n        object: (chunk as any).object, // not exposed by Azure API\n        model: (chunk as any).model, // not exposed by Azure API\n        choices: chunk.choices.map(choice => ({\n          delta: {\n            content: choice.delta?.content,\n            function_call: choice.delta?.functionCall,\n            role: choice.delta?.role as any,\n            tool_calls: choice.delta?.toolCalls?.length\n              ? choice.delta?.toolCalls?.map((toolCall, index) => ({\n                  index,\n                  id: toolCall.id,\n                  function: toolCall.function,\n                  type: toolCall.type,\n                }))\n              : undefined,\n          },\n          finish_reason: choice.finishReason as any,\n          index: choice.index,\n        })),\n      } satisfies ChatCompletionChunk;\n    }\n\n    const text = extract(chunk);\n\n    if (text) yield text;\n  }\n}\n\nfunction chunkToText(): (\n  chunk: OpenAIStreamReturnTypes,\n) => string | { isText: false; content: string } | void {\n  const trimStartOfStream = trimStartOfStreamHelper();\n  let isFunctionStreamingIn: boolean;\n  return json => {\n    if (isChatCompletionChunk(json)) {\n      const delta = json.choices[0]?.delta;\n      if (delta.function_call?.name) {\n        isFunctionStreamingIn = true;\n        return {\n          isText: false,\n          content: `{\"function_call\": {\"name\": \"${delta.function_call.name}\", \"arguments\": \"`,\n        };\n      } else if (delta.tool_calls?.[0]?.function?.name) {\n        isFunctionStreamingIn = true;\n        const toolCall = delta.tool_calls[0];\n        if (toolCall.index === 0) {\n          return {\n            isText: false,\n            content: `{\"tool_calls\":[ {\"id\": \"${toolCall.id}\", \"type\": \"function\", \"function\": {\"name\": \"${toolCall.function?.name}\", \"arguments\": \"`,\n          };\n        } else {\n          return {\n            isText: false,\n            content: `\"}}, {\"id\": \"${toolCall.id}\", \"type\": \"function\", \"function\": {\"name\": \"${toolCall.function?.name}\", \"arguments\": \"`,\n          };\n        }\n      } else if (delta.function_call?.arguments) {\n        return {\n          isText: false,\n          content: cleanupArguments(delta.function_call?.arguments),\n        };\n      } else if (delta.tool_calls?.[0]?.function?.arguments) {\n        return {\n          isText: false,\n          content: cleanupArguments(delta.tool_calls?.[0]?.function?.arguments),\n        };\n      } else if (\n        isFunctionStreamingIn &&\n        (json.choices[0]?.finish_reason === 'function_call' ||\n          json.choices[0]?.finish_reason === 'stop')\n      ) {\n        isFunctionStreamingIn = false; // Reset the flag\n        return {\n          isText: false,\n          content: '\"}}',\n        };\n      } else if (\n        isFunctionStreamingIn &&\n        json.choices[0]?.finish_reason === 'tool_calls'\n      ) {\n        isFunctionStreamingIn = false; // Reset the flag\n        return {\n          isText: false,\n          content: '\"}}]}',\n        };\n      }\n    }\n\n    const text = trimStartOfStream(\n      isChatCompletionChunk(json) && json.choices[0].delta.content\n        ? json.choices[0].delta.content\n        : isCompletion(json)\n        ? json.choices[0].text\n        : '',\n    );\n\n    return text;\n  };\n\n  function cleanupArguments(argumentChunk: string) {\n    let escapedPartialJson = argumentChunk\n      .replace(/\\\\/g, '\\\\\\\\') // Replace backslashes first to prevent double escaping\n      .replace(/\\//g, '\\\\/') // Escape slashes\n      .replace(/\"/g, '\\\\\"') // Escape double quotes\n      .replace(/\\n/g, '\\\\n') // Escape new lines\n      .replace(/\\r/g, '\\\\r') // Escape carriage returns\n      .replace(/\\t/g, '\\\\t') // Escape tabs\n      .replace(/\\f/g, '\\\\f'); // Escape form feeds\n\n    return `${escapedPartialJson}`;\n  }\n}\n\nconst __internal__OpenAIFnMessagesSymbol = Symbol(\n  'internal_openai_fn_messages',\n);\n\ntype AsyncIterableOpenAIStreamReturnTypes =\n  | AsyncIterable<ChatCompletionChunk>\n  | AsyncIterable<Completion>\n  | AsyncIterable<AzureChatCompletions>;\n\ntype ExtractType<T> = T extends AsyncIterable<infer U> ? U : never;\n\ntype OpenAIStreamReturnTypes =\n  ExtractType<AsyncIterableOpenAIStreamReturnTypes>;\n\nfunction isChatCompletionChunk(\n  data: OpenAIStreamReturnTypes,\n): data is ChatCompletionChunk {\n  return (\n    'choices' in data &&\n    data.choices &&\n    data.choices[0] &&\n    'delta' in data.choices[0]\n  );\n}\n\nfunction isCompletion(data: OpenAIStreamReturnTypes): data is Completion {\n  return (\n    'choices' in data &&\n    data.choices &&\n    data.choices[0] &&\n    'text' in data.choices[0]\n  );\n}\n\n/**\n * @deprecated Use the [OpenAI provider](https://sdk.vercel.ai/providers/ai-sdk-providers/openai) instead.\n */\nexport function OpenAIStream(\n  res: Response | AsyncIterableOpenAIStreamReturnTypes,\n  callbacks?: OpenAIStreamCallbacks,\n): ReadableStream {\n  // Annotate the internal `messages` property for recursive function calls\n  const cb:\n    | undefined\n    | (OpenAIStreamCallbacks & {\n        [__internal__OpenAIFnMessagesSymbol]?: CreateMessage[];\n      }) = callbacks;\n\n  let stream: ReadableStream<Uint8Array>;\n  if (Symbol.asyncIterator in res) {\n    stream = readableFromAsyncIterable(streamable(res)).pipeThrough(\n      createCallbacksTransformer(\n        cb?.experimental_onFunctionCall || cb?.experimental_onToolCall\n          ? {\n              ...cb,\n              onFinal: undefined,\n            }\n          : {\n              ...cb,\n            },\n      ),\n    );\n  } else {\n    stream = AIStream(\n      res,\n      parseOpenAIStream(),\n      cb?.experimental_onFunctionCall || cb?.experimental_onToolCall\n        ? {\n            ...cb,\n            onFinal: undefined,\n          }\n        : {\n            ...cb,\n          },\n    );\n  }\n\n  if (cb && (cb.experimental_onFunctionCall || cb.experimental_onToolCall)) {\n    const functionCallTransformer = createFunctionCallTransformer(cb);\n    return stream.pipeThrough(functionCallTransformer);\n  } else {\n    return stream.pipeThrough(createStreamDataTransformer());\n  }\n}\n\nfunction createFunctionCallTransformer(\n  callbacks: OpenAIStreamCallbacks & {\n    [__internal__OpenAIFnMessagesSymbol]?: CreateMessage[];\n  },\n): TransformStream<Uint8Array, Uint8Array> {\n  const textEncoder = new TextEncoder();\n  let isFirstChunk = true;\n  let aggregatedResponse = '';\n  let aggregatedFinalCompletionResponse = '';\n  let isFunctionStreamingIn = false;\n\n  let functionCallMessages: CreateMessage[] =\n    callbacks[__internal__OpenAIFnMessagesSymbol] || [];\n\n  const decode = createChunkDecoder();\n\n  return new TransformStream({\n    async transform(chunk, controller): Promise<void> {\n      const message = decode(chunk);\n      aggregatedFinalCompletionResponse += message;\n\n      const shouldHandleAsFunction =\n        isFirstChunk &&\n        (message.startsWith('{\"function_call\":') ||\n          message.startsWith('{\"tool_calls\":'));\n\n      if (shouldHandleAsFunction) {\n        isFunctionStreamingIn = true;\n        aggregatedResponse += message;\n        isFirstChunk = false;\n        return;\n      }\n\n      // Stream as normal\n      if (!isFunctionStreamingIn) {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('text', message)),\n        );\n        return;\n      } else {\n        aggregatedResponse += message;\n      }\n    },\n    async flush(controller): Promise<void> {\n      try {\n        if (\n          !isFirstChunk &&\n          isFunctionStreamingIn &&\n          (callbacks.experimental_onFunctionCall ||\n            callbacks.experimental_onToolCall)\n        ) {\n          isFunctionStreamingIn = false;\n          const payload = JSON.parse(aggregatedResponse);\n          // Append the function call message to the list\n          let newFunctionCallMessages: CreateMessage[] = [\n            ...functionCallMessages,\n          ];\n\n          let functionResponse:\n            | Response\n            | undefined\n            | void\n            | string\n            | AsyncIterableOpenAIStreamReturnTypes\n            | undefined = undefined;\n          // This callbacks.experimental_onFunctionCall check should not be necessary but TS complains\n          if (callbacks.experimental_onFunctionCall) {\n            // If the user is using the experimental_onFunctionCall callback, they should not be using tools\n            // if payload.function_call is not defined by time we get here we must have gotten a tool response\n            // and the user had defined experimental_onToolCall\n            if (payload.function_call === undefined) {\n              console.warn(\n                'experimental_onFunctionCall should not be defined when using tools',\n              );\n            }\n\n            const argumentsPayload = JSON.parse(\n              payload.function_call.arguments,\n            );\n\n            functionResponse = await callbacks.experimental_onFunctionCall(\n              {\n                name: payload.function_call.name,\n                arguments: argumentsPayload,\n              },\n              result => {\n                // Append the function call request and result messages to the list\n                newFunctionCallMessages = [\n                  ...functionCallMessages,\n                  {\n                    role: 'assistant',\n                    content: '',\n                    function_call: payload.function_call,\n                  },\n                  {\n                    role: 'function',\n                    name: payload.function_call.name,\n                    content: JSON.stringify(result),\n                  },\n                ];\n                // Return it to the user\n                return newFunctionCallMessages;\n              },\n            );\n          }\n          if (callbacks.experimental_onToolCall) {\n            const toolCalls: ToolCallPayload = {\n              tools: [],\n            };\n            for (const tool of payload.tool_calls) {\n              toolCalls.tools.push({\n                id: tool.id,\n                type: 'function',\n                func: {\n                  name: tool.function.name,\n                  arguments: JSON.parse(tool.function.arguments),\n                },\n              });\n            }\n            let responseIndex = 0;\n            try {\n              functionResponse = await callbacks.experimental_onToolCall(\n                toolCalls,\n                result => {\n                  if (result) {\n                    const { tool_call_id, function_name, tool_call_result } =\n                      result;\n                    // Append the function call request and result messages to the list\n                    newFunctionCallMessages = [\n                      ...newFunctionCallMessages,\n                      // Only append the assistant message if it's the first response\n                      ...(responseIndex === 0\n                        ? [\n                            {\n                              role: 'assistant' as const,\n                              content: '',\n                              tool_calls: payload.tool_calls.map(\n                                (tc: ToolCall) => ({\n                                  id: tc.id,\n                                  type: 'function',\n                                  function: {\n                                    name: tc.function.name,\n                                    // we send the arguments an object to the user, but as the API expects a string, we need to stringify it\n                                    arguments: JSON.stringify(\n                                      tc.function.arguments,\n                                    ),\n                                  },\n                                }),\n                              ),\n                            },\n                          ]\n                        : []),\n                      // Append the function call result message\n                      {\n                        role: 'tool',\n                        tool_call_id,\n                        name: function_name,\n                        content: JSON.stringify(tool_call_result),\n                      },\n                    ];\n                    responseIndex++;\n                  }\n                  // Return it to the user\n                  return newFunctionCallMessages;\n                },\n              );\n            } catch (e) {\n              console.error('Error calling experimental_onToolCall:', e);\n            }\n          }\n\n          if (!functionResponse) {\n            // The user didn't do anything with the function call on the server and wants\n            // to either do nothing or run it on the client\n            // so we just return the function call as a message\n            controller.enqueue(\n              textEncoder.encode(\n                formatStreamPart(\n                  payload.function_call ? 'function_call' : 'tool_calls',\n                  // parse to prevent double-encoding:\n                  JSON.parse(aggregatedResponse),\n                ),\n              ),\n            );\n            return;\n          } else if (typeof functionResponse === 'string') {\n            // The user returned a string, so we just return it as a message\n            controller.enqueue(\n              textEncoder.encode(formatStreamPart('text', functionResponse)),\n            );\n            aggregatedFinalCompletionResponse = functionResponse;\n            return;\n          }\n\n          // Recursively:\n\n          // We don't want to trigger onStart or onComplete recursively\n          // so we remove them from the callbacks\n          // see https://github.com/vercel/ai/issues/351\n          const filteredCallbacks: OpenAIStreamCallbacks = {\n            ...callbacks,\n            onStart: undefined,\n          };\n          // We only want onFinal to be called the _last_ time\n          callbacks.onFinal = undefined;\n\n          const openAIStream = OpenAIStream(functionResponse, {\n            ...filteredCallbacks,\n            [__internal__OpenAIFnMessagesSymbol]: newFunctionCallMessages,\n          } as AIStreamCallbacksAndOptions);\n\n          const reader = openAIStream.getReader();\n\n          while (true) {\n            const { done, value } = await reader.read();\n            if (done) {\n              break;\n            }\n            controller.enqueue(value);\n          }\n        }\n      } finally {\n        if (callbacks.onFinal && aggregatedFinalCompletionResponse) {\n          await callbacks.onFinal(aggregatedFinalCompletionResponse);\n        }\n      }\n    },\n  });\n}\n","import { AIStream, type AIStreamCallbacksAndOptions } from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\n// from replicate SDK\ninterface Prediction {\n  id: string;\n  status: 'starting' | 'processing' | 'succeeded' | 'failed' | 'canceled';\n  version: string;\n  input: object;\n  output?: any;\n  source: 'api' | 'web';\n  error?: any;\n  logs?: string;\n  metrics?: {\n    predict_time?: number;\n  };\n  webhook?: string;\n  webhook_events_filter?: ('start' | 'output' | 'logs' | 'completed')[];\n  created_at: string;\n  updated_at?: string;\n  completed_at?: string;\n  urls: {\n    get: string;\n    cancel: string;\n    stream?: string;\n  };\n}\n\n/**\n * Stream predictions from Replicate.\n * Only certain models are supported and you must pass `stream: true` to\n * replicate.predictions.create().\n * @see https://github.com/replicate/replicate-javascript#streaming\n *\n * @example\n * const response = await replicate.predictions.create({\n *  stream: true,\n *  input: {\n *    prompt: messages.join('\\n')\n *  },\n *  version: '2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1'\n * })\n *\n * const stream = await ReplicateStream(response)\n * return new StreamingTextResponse(stream)\n *\n */\nexport async function ReplicateStream(\n  res: Prediction,\n  cb?: AIStreamCallbacksAndOptions,\n  options?: {\n    headers?: Record<string, string>;\n  },\n): Promise<ReadableStream> {\n  const url = res.urls?.stream;\n\n  if (!url) {\n    if (res.error) throw new Error(res.error);\n    else throw new Error('Missing stream URL in Replicate response');\n  }\n\n  const eventStream = await fetch(url, {\n    method: 'GET',\n    headers: {\n      Accept: 'text/event-stream',\n      ...options?.headers,\n    },\n  });\n\n  return AIStream(eventStream, undefined, cb).pipeThrough(\n    createStreamDataTransformer(),\n  );\n}\n","/**\n * Merges two readable streams into a single readable stream, emitting values\n * from each stream as they become available.\n *\n * The first stream is prioritized over the second stream. If both streams have\n * values available, the first stream's value is emitted first.\n *\n * @template VALUE1 - The type of values emitted by the first stream.\n * @template VALUE2 - The type of values emitted by the second stream.\n * @param {ReadableStream<VALUE1>} stream1 - The first readable stream.\n * @param {ReadableStream<VALUE2>} stream2 - The second readable stream.\n * @returns {ReadableStream<VALUE1 | VALUE2>} A new readable stream that emits values from both input streams.\n */\nexport function mergeStreams<VALUE1, VALUE2>(\n  stream1: ReadableStream<VALUE1>,\n  stream2: ReadableStream<VALUE2>,\n): ReadableStream<VALUE1 | VALUE2> {\n  const reader1 = stream1.getReader();\n  const reader2 = stream2.getReader();\n\n  let lastRead1: Promise<ReadableStreamReadResult<VALUE1>> | undefined =\n    undefined;\n  let lastRead2: Promise<ReadableStreamReadResult<VALUE2>> | undefined =\n    undefined;\n\n  let stream1Done = false;\n  let stream2Done = false;\n\n  // only use when stream 2 is done:\n  async function readStream1(\n    controller: ReadableStreamDefaultController<VALUE1 | VALUE2>,\n  ) {\n    try {\n      if (lastRead1 == null) {\n        lastRead1 = reader1.read();\n      }\n\n      const result = await lastRead1;\n      lastRead1 = undefined;\n\n      if (!result.done) {\n        controller.enqueue(result.value);\n      } else {\n        controller.close();\n      }\n    } catch (error) {\n      controller.error(error);\n    }\n  }\n\n  // only use when stream 1 is done:\n  async function readStream2(\n    controller: ReadableStreamDefaultController<VALUE1 | VALUE2>,\n  ) {\n    try {\n      if (lastRead2 == null) {\n        lastRead2 = reader2.read();\n      }\n\n      const result = await lastRead2;\n      lastRead2 = undefined;\n\n      if (!result.done) {\n        controller.enqueue(result.value);\n      } else {\n        controller.close();\n      }\n    } catch (error) {\n      controller.error(error);\n    }\n  }\n\n  return new ReadableStream<VALUE1 | VALUE2>({\n    async pull(controller) {\n      try {\n        // stream 1 is done, we can only read from stream 2:\n        if (stream1Done) {\n          readStream2(controller);\n          return;\n        }\n\n        // stream 2 is done, we can only read from stream 1:\n        if (stream2Done) {\n          readStream1(controller);\n          return;\n        }\n\n        // pull the next value from the stream that was read last:\n        if (lastRead1 == null) {\n          lastRead1 = reader1.read();\n        }\n        if (lastRead2 == null) {\n          lastRead2 = reader2.read();\n        }\n\n        // Note on Promise.race (prioritizing stream 1 over stream 2):\n        // If the iterable contains one or more non-promise values and/or an already settled promise,\n        // then Promise.race() will settle to the first of these values found in the iterable.\n        const { result, reader } = await Promise.race([\n          lastRead1.then(result => ({ result, reader: reader1 })),\n          lastRead2.then(result => ({ result, reader: reader2 })),\n        ]);\n\n        if (!result.done) {\n          controller.enqueue(result.value);\n        }\n\n        if (reader === reader1) {\n          lastRead1 = undefined;\n          if (result.done) {\n            // stream 1 is done, we can only read from stream 2:\n            readStream2(controller);\n            stream1Done = true;\n          }\n        } else {\n          lastRead2 = undefined;\n          // stream 2 is done, we can only read from stream 1:\n          if (result.done) {\n            stream2Done = true;\n            readStream1(controller);\n          }\n        }\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n    cancel() {\n      reader1.cancel();\n      reader2.cancel();\n    },\n  });\n}\n","import type { ServerResponse } from 'node:http';\nimport { StreamData } from './stream-data';\nimport { mergeStreams } from '../core/util/merge-streams';\n\n/**\n * A utility function to stream a ReadableStream to a Node.js response-like object.\n */\nexport function streamToResponse(\n  res: ReadableStream,\n  response: ServerResponse,\n  init?: { headers?: Record<string, string>; status?: number },\n  data?: StreamData,\n) {\n  response.writeHead(init?.status ?? 200, {\n    'Content-Type': 'text/plain; charset=utf-8',\n    ...init?.headers,\n  });\n\n  let processedStream = res;\n\n  if (data) {\n    processedStream = mergeStreams(data.stream, res);\n  }\n\n  const reader = processedStream.getReader();\n  function read() {\n    reader.read().then(({ done, value }: { done: boolean; value?: any }) => {\n      if (done) {\n        response.end();\n        return;\n      }\n      response.write(value);\n      read();\n    });\n  }\n  read();\n}\n","import { mergeStreams } from '../core/util/merge-streams';\nimport { prepareResponseHeaders } from '../core/util/prepare-response-headers';\nimport { StreamData } from './stream-data';\n\n/**\n * A utility class for streaming text responses.\n */\nexport class StreamingTextResponse extends Response {\n  constructor(res: ReadableStream, init?: ResponseInit, data?: StreamData) {\n    let processedStream = res;\n\n    if (data) {\n      processedStream = mergeStreams(data.stream, res);\n    }\n\n    super(processedStream as any, {\n      ...init,\n      status: 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n    });\n  }\n}\n"],"names":["getErrorMessage","error","tool","safeParseJSON","_a","span","result","NoSuchToolError","doStreamSpan","formatStreamPart","process","stream","createParser","streamable","generateId"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AEAA,eAAsB,MAAM,SAAA;IAC1B,OAAO,IAAI,QAAQ,CAAA,UAAW,WAAW,SAAS;AACpD;;ADUO,IAAM,8BACX,CAAC,EACC,aAAa,CAAA,EACb,mBAAmB,GAAA,EACnB,gBAAgB,CAAA,EAClB,GAAI,CAAC,CAAA,GACL,OAAe,IACb,6BAA6B,GAAG;YAC9B;YACA,WAAW;YACX;QACF;AAEJ,eAAe,6BACb,CAAA,EACA,EACE,UAAA,EACA,SAAA,EACA,aAAA,EACF,EACA,SAAoB,EAAC;IAErB,IAAI;QACF,OAAO,MAAM;IACf,EAAA,OAAS,OAAO;QACd,IAAI,CAAA,GAAA,wLAAA,CAAA,eAAA,EAAa,QAAQ;YACvB,MAAM;QACR;QAEA,IAAI,eAAe,GAAG;YACpB,MAAM;QACR;QAEA,MAAM,eAAe,CAAA,GAAA,wLAAA,CAAA,kBAAA,EAAgB;QACrC,MAAM,YAAY;eAAI;YAAQ;SAAK;QACnC,MAAM,YAAY,UAAU,MAAA;QAE5B,IAAI,YAAY,YAAY;YAC1B,MAAM,IAAI,+KAAA,CAAA,aAAA,CAAW;gBACnB,SAAS,CAAA,aAAA,EAAgB,UAAS,uBAAA,EAA0B,aAAY,CAAA;gBACxE,QAAQ;gBACR,QAAQ;YACV;QACF;QAEA,IACE,iBAAiB,SACjB,+KAAA,CAAA,eAAA,CAAa,cAAA,CAAe,UAC5B,MAAM,WAAA,KAAgB,QACtB,aAAa,YACb;YACA,MAAM,MAAM;YACZ,OAAO,6BACL,GACA;gBAAE;gBAAY,WAAW,gBAAgB;gBAAW;YAAc,GAClE;QAEJ;QAEA,IAAI,cAAc,GAAG;YACnB,MAAM;QACR;QAEA,MAAM,IAAI,+KAAA,CAAA,aAAA,CAAW;YACnB,SAAS,CAAA,aAAA,EAAgB,UAAS,qCAAA,EAAwC,aAAY,CAAA,CAAA;YACtF,QAAQ;YACR,QAAQ;QACV;IACF;AACF;;AEjEA,eAAsB,MAAa,EACjC,KAAA,EACA,KAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACF;IAtBA,IAAA;IAmDE,MAAM,QAAQ,4BAA4B;QAAE;IAAW;IAEvD,MAAM,gBAAgB,MAAM,MAAM,IAChC,MAAM,OAAA,CAAQ;YAAE,QAAQ;gBAAC;aAAK;YAAG;YAAa;QAAQ;IAGxD,OAAO,IAAI,YAAY;QACrB;QACA,WAAW,cAAc,UAAA,CAAW,EAAC;QACrC,OAAA,CAAO,KAAA,cAAc,KAAA,KAAd,OAAA,KAAuB;YAAE,QAAQ;QAAI;QAC5C,aAAa,cAAc,WAAA;IAC7B;AACF;AAMO,IAAM,cAAN;IA0BL,YAAY,OAAA,CAKT;QACD,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,SAAA,GAAY,QAAQ,SAAA;QACzB,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,WAAA,GAAc,QAAQ,WAAA;IAC7B;AACF;;AClGO,SAAS,WAAc,KAAA,EAAY,SAAA;IACxC,IAAI,aAAa,GAAG;QAClB,MAAM,IAAI,MAAM;IAClB;IAEA,MAAM,SAAS,EAAC;IAChB,IAAA,IAAS,IAAI,GAAG,IAAI,MAAM,MAAA,EAAQ,KAAK,UAAW;QAChD,OAAO,IAAA,CAAK,MAAM,KAAA,CAAM,GAAG,IAAI;IACjC;IAEA,OAAO;AACT;;ACEA,eAAsB,UAAiB,EACrC,KAAA,EACA,MAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACF;IA3BA,IAAA,IAAA,IAAA;IAwDE,MAAM,QAAQ,4BAA4B;QAAE;IAAW;IACvD,MAAM,uBAAuB,MAAM,oBAAA;IAInC,IAAI,wBAAwB,MAAM;QAChC,MAAM,gBAAgB,MAAM,MAAM,IAChC,MAAM,OAAA,CAAQ;gBAAE;gBAAQ;gBAAa;YAAQ;QAG/C,OAAO,IAAI,gBAAgB;YACzB;YACA,YAAY,cAAc,UAAA;YAC1B,OAAA,CAAO,KAAA,cAAc,KAAA,KAAd,OAAA,KAAuB;gBAAE,QAAQ;YAAI;QAC9C;IACF;IAGA,MAAM,cAAc,WAAW,QAAQ;IAGvC,MAAM,aAAa,EAAC;IACpB,IAAI,SAAS;IAEb,KAAA,MAAW,SAAS,YAAa;QAC/B,MAAM,gBAAgB,MAAM,MAAM,IAChC,MAAM,OAAA,CAAQ;gBAAE,QAAQ;gBAAO;gBAAa;YAAQ;QAEtD,WAAW,IAAA,IAAQ,cAAc,UAAU;QAC3C,UAAA,CAAU,KAAA,CAAA,KAAA,cAAc,KAAA,KAAd,OAAA,KAAA,IAAA,GAAqB,MAAA,KAArB,OAAA,KAA+B;IAC3C;IAEA,OAAO,IAAI,gBAAgB;QAAE;QAAQ;QAAY,OAAO;YAAE;QAAO;IAAE;AACrE;AAMO,IAAM,kBAAN;IAgBL,YAAY,OAAA,CAIT;QACD,IAAA,CAAK,MAAA,GAAS,QAAQ,MAAA;QACtB,IAAA,CAAK,UAAA,GAAa,QAAQ,UAAA;QAC1B,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;IACvB;AACF;;;;AExHA,IAAM,qBAAqB;IACzB;QAAE,UAAU;QAAsB,OAAO;YAAC;YAAM;YAAM;SAAI;IAAE;IAC5D;QAAE,UAAU;QAAsB,OAAO;YAAC;YAAM;YAAM;YAAM;SAAI;IAAE;IAClE;QAAE,UAAU;QAAuB,OAAO;YAAC;YAAM;SAAI;IAAE;IACvD;QAAE,UAAU;QAAuB,OAAO;YAAC;YAAM;YAAM;YAAM;SAAI;IAAE;CACrE;AAEO,SAAS,oBACd,KAAA;IAEA,KAAA,MAAW,EAAE,KAAA,EAAO,QAAA,EAAS,IAAK,mBAAoB;QACpD,IACE,MAAM,MAAA,IAAU,MAAM,MAAA,IACtB,MAAM,KAAA,CAAM,CAAC,MAAM,QAAU,KAAA,CAAM,MAAK,KAAM,OAC9C;YACA,OAAO;QACT;IACF;IAEA,OAAO,KAAA;AACT;;;ACHO,SAAS,iCAAiC,OAAA;IAC/C,IAAI,OAAO,YAAY,UAAU;QAC/B,OAAO;IACT;IAEA,IAAI,mBAAmB,aAAa;QAClC,OAAO,CAAA,GAAA,wLAAA,CAAA,4BAAA,EAA0B,IAAI,WAAW;IAClD;IAEA,OAAO,CAAA,GAAA,wLAAA,CAAA,4BAAA,EAA0B;AACnC;AAQO,SAAS,+BACd,OAAA;IAEA,IAAI,mBAAmB,YAAY;QACjC,OAAO;IACT;IAEA,IAAI,OAAO,YAAY,UAAU;QAC/B,IAAI;YACF,OAAO,CAAA,GAAA,wLAAA,CAAA,4BAAA,EAA0B;QACnC,EAAA,OAAS,OAAO;YACd,MAAM,IAAI,+KAAA,CAAA,0BAAA,CAAwB;gBAChC,SACE;gBACF;gBACA,OAAO;YACT;QACF;IACF;IAEA,IAAI,mBAAmB,aAAa;QAClC,OAAO,IAAI,WAAW;IACxB;IAEA,MAAM,IAAI,+KAAA,CAAA,0BAAA,CAAwB;QAAE;IAAQ;AAC9C;;AC5DO,IAAM,0BAAN,cAAsC;IAG3C,YAAY,EACV,IAAA,EACA,UAAU,CAAA,uBAAA,EAA0B,KAAI,yDAAA,CAAA,EAC1C,CAGG;QACD,KAAA,CAAM;QAEN,IAAA,CAAK,IAAA,GAAO;QAEZ,IAAA,CAAK,IAAA,GAAO;IACd;IAEA,OAAO,0BACL,KAAA,EACkC;QAClC,OACE,iBAAiB,SACjB,MAAM,IAAA,KAAS,gCACf,OAAQ,MAAkC,IAAA,KAAS;IAEvD;IAEA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,MAAM,IAAA,CAAK,IAAA;QACb;IACF;AACF;;ACvBO,SAAS,6BACd,MAAA;IAEA,MAAM,wBAA+C,EAAC;IAEtD,IAAI,OAAO,MAAA,IAAU,MAAM;QACzB,sBAAsB,IAAA,CAAK;YAAE,MAAM;YAAU,SAAS,OAAO,MAAA;QAAO;IACtE;IAEA,MAAM,aAAa,OAAO,IAAA;IAC1B,OAAQ;QACN,KAAK;YAAU;gBACb,sBAAsB,IAAA,CAAK;oBACzB,MAAM;oBACN,SAAS;wBAAC;4BAAE,MAAM;4BAAQ,MAAM,OAAO,MAAA;wBAAO;qBAAC;gBACjD;gBACA;YACF;QAEA,KAAK;YAAY;gBACf,sBAAsB,IAAA,IACjB,OAAO,QAAA,CAAS,GAAA,CAAI;gBAEzB;YACF;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAA,yBAAA,EAA4B,iBAAgB,CAAE;YAChE;IACF;IAEA,OAAO;AACT;AAEO,SAAS,8BACd,OAAA;IAEA,MAAM,OAAO,QAAQ,IAAA;IACrB,OAAQ;QACN,KAAK;YAAU;gBACb,OAAO;oBAAE,MAAM;oBAAU,SAAS,QAAQ,OAAA;gBAAQ;YACpD;QAEA,KAAK;YAAQ;gBACX,IAAI,OAAO,QAAQ,OAAA,KAAY,UAAU;oBACvC,OAAO;wBACL,MAAM;wBACN,SAAS;4BAAC;gCAAE,MAAM;gCAAQ,MAAM,QAAQ,OAAA;4BAAQ;yBAAC;oBACnD;gBACF;gBAEA,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA,CAAQ,GAAA,CACvB,CAAC;wBApEX,IAAA;wBAqEY,OAAQ,KAAK,IAAA;4BACX,KAAK;gCAAQ;oCACX,OAAO;gCACT;4BAEA,KAAK;gCAAS;oCACZ,IAAI,KAAK,KAAA,YAAiB,KAAK;wCAC7B,OAAO;4CACL,MAAM;4CACN,OAAO,KAAK,KAAA;4CACZ,UAAU,KAAK,QAAA;wCACjB;oCACF;oCAGA,IAAI,OAAO,KAAK,KAAA,KAAU,UAAU;wCAClC,IAAI;4CACF,MAAM,MAAM,IAAI,IAAI,KAAK,KAAK;4CAE9B,OAAQ,IAAI,QAAA;gDACV,KAAK;gDACL,KAAK;oDAAU;wDACb,OAAO;4DACL,MAAM;4DACN,OAAO;4DACP,UAAU,KAAK,QAAA;wDACjB;oDACF;gDACA,KAAK;oDAAS;wDACZ,IAAI;4DACF,MAAM,CAAC,QAAQ,cAAa,GAAI,KAAK,KAAA,CAAM,KAAA,CAAM;4DACjD,MAAM,WAAW,OAAO,KAAA,CAAM,IAAG,CAAE,EAAC,CAAE,KAAA,CAAM,IAAG,CAAE,EAAC;4DAElD,IAAI,YAAY,QAAQ,iBAAiB,MAAM;gEAC7C,MAAM,IAAI,MAAM;4DAClB;4DAEA,OAAO;gEACL,MAAM;gEACN,OACE,+BAA+B;gEACjC;4DACF;wDACF,EAAA,OAAS,OAAO;4DACd,MAAM,IAAI,MACR,CAAA,2BAAA,EAA8BA,CAAAA,GAAAA,wLAAAA,CAAAA,kBAAAA,EAC5B,SACD,CAAA;wDAEL;oDACF;gDACA;oDAAS;wDACP,MAAM,IAAI,MACR,CAAA,0BAAA,EAA6B,IAAI,QAAQ,CAAA,CAAA;oDAE7C;4CACF;wCACF,EAAA,OAAS,UAAU,CAEnB;oCACF;oCAEA,MAAM,aAAa,+BAA+B,KAAK,KAAK;oCAE5D,OAAO;wCACL,MAAM;wCACN,OAAO;wCACP,UAAA,CAAU,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,oBAAoB;oCACjD;gCACF;wBACF;oBACF;gBAEJ;YACF;QAEA,KAAK;YAAa;gBAChB,IAAI,OAAO,QAAQ,OAAA,KAAY,UAAU;oBACvC,OAAO;wBACL,MAAM;wBACN,SAAS;4BAAC;gCAAE,MAAM;gCAAQ,MAAM,QAAQ,OAAA;4BAAQ;yBAAC;oBACnD;gBACF;gBAEA,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA,CAAQ,MAAA,CAAA,2BAAA;oBAEvB,CAAA,OAAQ,KAAK,IAAA,KAAS,UAAU,KAAK,IAAA,KAAS;gBAElD;YACF;QAEA,KAAK;YAAQ;gBACX,OAAO;YACT;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,wBAAwB;oBAAE,MAAM;gBAAiB;YAC7D;IACF;AACF;;ACzJO,SAAS,mBAAmB,MAAA;IACjC,IAAI,OAAO,MAAA,IAAU,QAAQ,OAAO,QAAA,IAAY,MAAM;QACpD,MAAM,IAAI,+KAAA,CAAA,qBAAA,CAAmB;YAC3B;YACA,SAAS;QACX;IACF;IAEA,IAAI,OAAO,MAAA,IAAU,QAAQ,OAAO,QAAA,IAAY,MAAM;QACpD,MAAM,IAAI,+KAAA,CAAA,qBAAA,CAAmB;YAC3B;YACA,SAAS;QACX;IACF;IAEA,IAAI,OAAO,QAAA,IAAY,MAAM;QAC3B,KAAA,MAAW,WAAW,OAAO,QAAA,CAAU;YACrC,IAAI,QAAQ,IAAA,KAAS,YAAY,OAAO,QAAQ,OAAA,KAAY,UAAU;gBACpE,MAAM,IAAI,+KAAA,CAAA,qBAAA,CAAmB;oBAC3B;oBACA,SAAS;gBACX;YACF;QACF;IACF;IAEA,OAAO,OAAO,MAAA,IAAU,OACpB;QACE,MAAM;QACN,QAAQ,OAAO,MAAA;QACf,UAAU,KAAA;QACV,QAAQ,OAAO,MAAA;IACjB,IACA;QACE,MAAM;QACN,QAAQ,KAAA;QACR,UAAU,OAAO,QAAA;QAAA,wCAAA;QACjB,QAAQ,OAAO,MAAA;IACjB;AACN;;ACnDO,SAAS,oBAAoB,EAClC,SAAA,EACA,WAAA,EACA,IAAA,EACA,eAAA,EACA,gBAAA,EACA,IAAA,EACA,UAAA,EACF;IACE,IAAI,aAAa,MAAM;QACrB,IAAI,CAAC,OAAO,SAAA,CAAU,YAAY;YAChC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,YAAY,GAAG;YACjB,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,eAAe,MAAM;QACvB,IAAI,OAAO,gBAAgB,UAAU;YACnC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,QAAQ,MAAM;QAChB,IAAI,OAAO,SAAS,UAAU;YAC5B,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,mBAAmB,MAAM;QAC3B,IAAI,OAAO,oBAAoB,UAAU;YACvC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,oBAAoB,MAAM;QAC5B,IAAI,OAAO,qBAAqB,UAAU;YACxC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,QAAQ,MAAM;QAChB,IAAI,CAAC,OAAO,SAAA,CAAU,OAAO;YAC3B,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,cAAc,MAAM;QACtB,IAAI,CAAC,OAAO,SAAA,CAAU,aAAa;YACjC,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,aAAa,GAAG;YAClB,MAAM,IAAI,+KAAA,CAAA,uBAAA,CAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,OAAO;QACL;QACA,aAAa,eAAA,OAAA,cAAe;QAC5B;QACA;QACA;QACA;QACA,YAAY,cAAA,OAAA,aAAc;IAC5B;AACF;;AChFO,SAAS,8BAA8B,KAAA;IAI5C,OAAO;QACL,cAAc,MAAM,YAAA;QACpB,kBAAkB,MAAM,gBAAA;QACxB,aAAa,MAAM,YAAA,GAAe,MAAM,gBAAA;IAC1C;AACF;;ACnCO,SAAS,uBACd,SAAA;IAGA,OAAO,CAAA,GAAA,mLAAA,CAAA,UAAA,EAAgB;AACzB;;ACTO,SAAS,uBACd,IAAA,EACA,EAAE,WAAA,EAAY;IAFhB,IAAA;IAIE,MAAM,UAAU,IAAI,QAAA,CAAQ,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA,KAAN,OAAA,KAAiB,CAAC;IAE9C,IAAI,CAAC,QAAQ,GAAA,CAAI,iBAAiB;QAChC,QAAQ,GAAA,CAAI,gBAAgB;IAC9B;IAEA,OAAO;AACT;;ACTA,IAAM,wBAAwB;AAC9B,IAAM,wBACJ;AAEK,SAAS,2BAA2B,EACzC,MAAA,EACA,MAAA,EACA,eAAe,qBAAA,EACf,eAAe,qBAAA,EACjB;IAME,OAAO;QACL;QACA,UAAU,OAAO,KAAK;QAAA,sCAAA;QACtB;QACA,KAAK,SAAA,CAAU;QACf;KACF,CACG,MAAA,CAAO,CAAA,OAAQ,QAAQ,MACvB,IAAA,CAAK;AACV;;AV6BA,eAAsB,eAAkB,EACtC,KAAA,EACA,MAAA,EACA,IAAA,EACA,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,GAAG,UACL;IAlEA,IAAA,IAAA;IA8FE,MAAM,QAAQ,4BAA4B;QAAE;IAAW;IACvD,MAAM,aAAa,uBAAuB;IAG1C,IAAI,SAAS,UAAU,QAAQ,MAAM;QACnC,OAAO,MAAM,2BAAA;IACf;IAEA,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,IAAI;IACJ,IAAI;IAEJ,OAAQ;QACN,KAAK;YAAQ;gBACX,MAAM,kBAAkB,mBAAmB;oBACzC,QAAQ,2BAA2B;wBAAE;wBAAQ,QAAQ;oBAAW;oBAChE;oBACA;gBACF;gBAEA,MAAM,iBAAiB,MAAM,MAAM;oBACjC,OAAO,MAAM,UAAA,CAAW;wBACtB,MAAM;4BAAE,MAAM;wBAAc;wBAC5B,GAAG,oBAAoB,SAAQ;wBAC/B,aAAa,gBAAgB,IAAA;wBAC7B,QAAQ,6BAA6B;wBACrC;wBACA;oBACF;gBACF;gBAEA,IAAI,eAAe,IAAA,KAAS,KAAA,GAAW;oBACrC,MAAM,IAAI,+KAAA,CAAA,yBAAA;gBACZ;gBAEA,SAAS,eAAe,IAAA;gBACxB,eAAe,eAAe,YAAA;gBAC9B,QAAQ,eAAe,KAAA;gBACvB,WAAW,eAAe,QAAA;gBAC1B,cAAc,eAAe,WAAA;gBAC7B,WAAW,eAAe,QAAA;gBAE1B;YACF;QAEA,KAAK;YAAW;gBACd,MAAM,kBAAkB,mBAAmB;oBACzC,QAAQ,2BAA2B;wBAAE;wBAAQ,QAAQ;oBAAW;oBAChE;oBACA;gBACF;gBAEA,MAAM,iBAAiB,MAAM,MAAM,IACjC,MAAM,UAAA,CAAW;wBACf,MAAM;4BAAE,MAAM;4BAAkB,QAAQ;wBAAW;wBACnD,GAAG,oBAAoB,SAAQ;wBAC/B,aAAa,gBAAgB,IAAA;wBAC7B,QAAQ,6BAA6B;wBACrC;oBACF;gBAGF,IAAI,eAAe,IAAA,KAAS,KAAA,GAAW;oBACrC,MAAM,IAAI,+KAAA,CAAA,yBAAA;gBACZ;gBAEA,SAAS,eAAe,IAAA;gBACxB,eAAe,eAAe,YAAA;gBAC9B,QAAQ,eAAe,KAAA;gBACvB,WAAW,eAAe,QAAA;gBAC1B,cAAc,eAAe,WAAA;gBAC7B,WAAW,eAAe,QAAA;gBAE1B;YACF;QAEA,KAAK;YAAQ;gBACX,MAAM,kBAAkB,mBAAmB;oBACzC;oBACA;oBACA;gBACF;gBAEA,MAAM,iBAAiB,MAAM,MAAM,IACjC,MAAM,UAAA,CAAW;wBACf,MAAM;4BACJ,MAAM;4BACN,MAAM;gCACJ,MAAM;gCACN,MAAM;gCACN,aAAa;gCACb,YAAY;4BACd;wBACF;wBACA,GAAG,oBAAoB,SAAQ;wBAC/B,aAAa,gBAAgB,IAAA;wBAC7B,QAAQ,6BAA6B;wBACrC;oBACF;gBAGF,MAAM,eAAA,CAAe,KAAA,CAAA,KAAA,eAAe,SAAA,KAAf,OAAA,KAAA,IAAA,EAAA,CAA2B,EAAA,KAA3B,OAAA,KAAA,IAAA,GAA+B,IAAA;gBAEpD,IAAI,iBAAiB,KAAA,GAAW;oBAC9B,MAAM,IAAI,+KAAA,CAAA,yBAAA;gBACZ;gBAEA,SAAS;gBACT,eAAe,eAAe,YAAA;gBAC9B,QAAQ,eAAe,KAAA;gBACvB,WAAW,eAAe,QAAA;gBAC1B,cAAc,eAAe,WAAA;gBAC7B,WAAW,eAAe,QAAA;gBAE1B;YACF;QAEA,KAAK,KAAA;YAAW;gBACd,MAAM,IAAI,MAAM;YAClB;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;YACzD;IACF;IAEA,MAAM,cAAc,CAAA,GAAA,wLAAA,CAAA,gBAAA,EAAc;QAAE,MAAM;QAAQ;IAAO;IAEzD,IAAI,CAAC,YAAY,OAAA,EAAS;QACxB,MAAM,YAAY,KAAA;IACpB;IAEA,OAAO,IAAI,qBAAqB;QAC9B,QAAQ,YAAY,KAAA;QACpB;QACA,OAAO,8BAA8B;QACrC;QACA;QACA;IACF;AACF;AAKO,IAAM,uBAAN;IAqCL,YAAY,OAAA,CAST;QACD,IAAA,CAAK,MAAA,GAAS,QAAQ,MAAA;QACtB,IAAA,CAAK,YAAA,GAAe,QAAQ,YAAA;QAC5B,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;QACxB,IAAA,CAAK,WAAA,GAAc,QAAQ,WAAA;QAC3B,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;IAC1B;IAAA;;;KAAA,GAMA,eAAe,IAAA,EAA+B;QA9ShD,IAAA;QA+SI,OAAO,IAAI,SAAS,KAAK,SAAA,CAAU,IAAA,CAAK,MAAM,GAAG;YAC/C,QAAA,CAAQ,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAA,KAAgB;YACxB,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;AAKO,IAAM,8BAA8B;;;;AYzTpC,SAAS,0BACd,MAAA,EACA,WAAA;IAEA,MAAM,oBAAyB,OAAO,WAAA,CACpC,IAAI,gBAAgB;IAGtB,iBAAA,CAAkB,OAAO,aAAa,CAAA,GAAI;QACxC,MAAM,SAAS,kBAAkB,SAAA;QACjC,OAAO;YACL,MAAM;gBACJ,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;gBACrC,OAAO,OAAO;oBAAE,MAAM;oBAAM,OAAO,KAAA;gBAAU,IAAI;oBAAE,MAAM;oBAAO;gBAAM;YACxE;QACF;IACF;IAEA,OAAO;AACT;;AD+CA,eAAsB,aAAgB,EACpC,KAAA,EACA,MAAA,EACA,IAAA,EACA,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,QAAA,EACA,GAAG,UACL;IA+DE,MAAM,QAAQ,4BAA4B;QAAE;IAAW;IACvD,MAAM,aAAa,uBAAuB;IAG1C,IAAI,SAAS,UAAU,QAAQ,MAAM;QACnC,OAAO,MAAM,2BAAA;IACf;IAEA,IAAI;IACJ,IAAI;IAKJ,OAAQ;QACN,KAAK;YAAQ;gBACX,MAAM,kBAAkB,mBAAmB;oBACzC,QAAQ,2BAA2B;wBAAE;wBAAQ,QAAQ;oBAAW;oBAChE;oBACA;gBACF;gBAEA,cAAc;oBACZ,MAAM;wBAAE,MAAM;oBAAc;oBAC5B,GAAG,oBAAoB,SAAQ;oBAC/B,aAAa,gBAAgB,IAAA;oBAC7B,QAAQ,6BAA6B;oBACrC;oBACA;gBACF;gBAEA,cAAc;oBACZ,WAAW,CAAC,OAAO;wBACjB,OAAQ,MAAM,IAAA;4BACZ,KAAK;gCACH,WAAW,OAAA,CAAQ,MAAM,SAAS;gCAClC;4BACF,KAAK;4BACL,KAAK;gCACH,WAAW,OAAA,CAAQ;gCACnB;wBACJ;oBACF;gBACF;gBAEA;YACF;QAEA,KAAK;YAAW;gBACd,MAAM,kBAAkB,mBAAmB;oBACzC,QAAQ,2BAA2B;wBAAE;wBAAQ,QAAQ;oBAAW;oBAChE;oBACA;gBACF;gBAEA,cAAc;oBACZ,MAAM;wBAAE,MAAM;wBAAkB,QAAQ;oBAAW;oBACnD,GAAG,oBAAoB,SAAQ;oBAC/B,aAAa,gBAAgB,IAAA;oBAC7B,QAAQ,6BAA6B;oBACrC;oBACA;gBACF;gBAEA,cAAc;oBACZ,WAAW,CAAC,OAAO;wBACjB,OAAQ,MAAM,IAAA;4BACZ,KAAK;gCACH,WAAW,OAAA,CAAQ,MAAM,SAAS;gCAClC;4BACF,KAAK;4BACL,KAAK;gCACH,WAAW,OAAA,CAAQ;gCACnB;wBACJ;oBACF;gBACF;gBAEA;YACF;QAEA,KAAK;YAAQ;gBACX,MAAM,kBAAkB,mBAAmB;oBACzC;oBACA;oBACA;gBACF;gBAEA,cAAc;oBACZ,MAAM;wBACJ,MAAM;wBACN,MAAM;4BACJ,MAAM;4BACN,MAAM;4BACN,aAAa;4BACb,YAAY;wBACd;oBACF;oBACA,GAAG,oBAAoB,SAAQ;oBAC/B,aAAa,gBAAgB,IAAA;oBAC7B,QAAQ,6BAA6B;oBACrC;oBACA;gBACF;gBAEA,cAAc;oBACZ,WAAU,KAAA,EAAO,UAAA;wBACf,OAAQ,MAAM,IAAA;4BACZ,KAAK;gCACH,WAAW,OAAA,CAAQ,MAAM,aAAa;gCACtC;4BACF,KAAK;4BACL,KAAK;gCACH,WAAW,OAAA,CAAQ;gCACnB;wBACJ;oBACF;gBACF;gBAEA;YACF;QAEA,KAAK,KAAA;YAAW;gBACd,MAAM,IAAI,MAAM;YAClB;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;YACzD;IACF;IAEA,MAAM,SAAS,MAAM,MAAM,IAAM,MAAM,QAAA,CAAS;IAEhD,OAAO,IAAI,mBAAmB;QAC5B,QAAQ,OAAO,MAAA,CAAO,WAAA,CAAY,IAAI,gBAAgB;QACtD,UAAU,OAAO,QAAA;QACjB,aAAa,OAAO,WAAA;QACpB;QACA;IACF;AACF;AAgCO,IAAM,qBAAN;IA4BL,YAAY,EACV,MAAA,EACA,QAAA,EACA,WAAA,EACA,MAAA,EACA,QAAA,EACF,CAUG;QACD,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,WAAA,GAAc;QAGnB,IAAI;QACJ,IAAI;QACJ,IAAA,CAAK,MAAA,GAAS,IAAI,QAAW,CAAC,SAAS;YACrC,gBAAgB;YAChB,eAAe;QACjB;QAGA,IAAI;QAGJ,IAAA,CAAK,KAAA,GAAQ,IAAI,QAA8B,CAAA;YAC7C,eAAe;QACjB;QAGA,IAAI;QACJ,IAAI;QACJ,IAAI;QAGJ,IAAI,kBAAkB;QACtB,IAAI,QAAQ;QACZ,IAAI,eAA2C,KAAA;QAE/C,IAAA,CAAK,cAAA,GAAiB,OAAO,WAAA,CAC3B,IAAI,gBAAqE;YACvE,MAAM,WAAU,KAAA,EAAO,UAAA;gBAErB,IAAI,OAAO,UAAU,UAAU;oBAC7B,mBAAmB;oBACnB,SAAS;oBAET,MAAM,gBAAgB,CAAA,GAAA,4KAAA,CAAA,mBAAA,EACpB;oBAGF,IAAI,CAAC,CAAA,GAAA,4KAAA,CAAA,kBAAA,EAAgB,cAAc,gBAAgB;wBACjD,eAAe;wBAEf,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,QAAQ;wBACV;wBAEA,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,WAAW;wBACb;wBAEA,QAAQ;oBACV;oBAEA;gBACF;gBAEA,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBAAU;4BAEb,IAAI,UAAU,IAAI;gCAChB,WAAW,OAAA,CAAQ;oCACjB,MAAM;oCACN,WAAW;gCACb;4BACF;4BAGA,QAAQ,8BAA8B,MAAM,KAAK;4BAEjD,WAAW,OAAA,CAAQ;gCAAE,GAAG,KAAA;gCAAO;4BAAM;4BAGrC,aAAa;4BAGb,MAAM,mBAAmB,CAAA,GAAA,wLAAA,CAAA,oBAAA,EAAkB;gCACzC,OAAO;gCACP;4BACF;4BAEA,IAAI,iBAAiB,OAAA,EAAS;gCAC5B,SAAS,iBAAiB,KAAA;gCAC1B,cAAc;4BAChB,OAAO;gCACL,QAAQ,iBAAiB,KAAA;gCACzB,aAAa;4BACf;4BAEA;wBACF;oBAEA;wBAAS;4BACP,WAAW,OAAA,CAAQ;4BACnB;wBACF;gBACF;YACF;YAAA,8FAAA;YAGA,MAAM,OAAM,UAAA;gBACV,IAAI;oBAEF,MAAA,CAAM,YAAA,OAAA,KAAA,IAAA,SAAW;wBACf,OAAO,SAAA,OAAA,QAAS;4BACd,cAAc;4BACd,kBAAkB;4BAClB,aAAa;wBACf;wBACA;wBACA;wBACA;wBACA;oBACF,EAAA;gBACF,EAAA,OAASC,QAAO;oBACd,WAAW,KAAA,CAAMA;gBACnB;YACF;QACF;IAEJ;IAAA;;;;;KAAA,GAQA,IAAI,sBAA2D;QAC7D,OAAO,0BAA0B,IAAA,CAAK,cAAA,EAAgB;YACpD,WAAU,KAAA,EAAO,UAAA;gBACf,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBACH,WAAW,OAAA,CAAQ,MAAM,MAAM;wBAC/B;oBAEF,KAAK;oBACL,KAAK;wBACH;oBAEF,KAAK;wBACH,WAAW,KAAA,CAAM,MAAM,KAAK;wBAC5B;oBAEF;wBAAS;4BACP,MAAM,mBAA0B;4BAChC,MAAM,IAAI,MAAM,CAAA,wBAAA,EAA2B,iBAAgB,CAAE;wBAC/D;gBACF;YACF;QACF;IACF;IAAA;;;KAAA,GAMA,IAAI,aAA0C;QAC5C,OAAO,0BAA0B,IAAA,CAAK,cAAA,EAAgB;YACpD,WAAU,KAAA,EAAO,UAAA;gBACf,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBACH,WAAW,OAAA,CAAQ,MAAM,SAAS;wBAClC;oBAEF,KAAK;oBACL,KAAK;wBACH;oBAEF,KAAK;wBACH,WAAW,KAAA,CAAM,MAAM,KAAK;wBAC5B;oBAEF;wBAAS;4BACP,MAAM,mBAA0B;4BAChC,MAAM,IAAI,MAAM,CAAA,wBAAA,EAA2B,iBAAgB,CAAE;wBAC/D;gBACF;YACF;QACF;IACF;IAAA;;KAAA,GAKA,IAAI,aAAuD;QACzD,OAAO,0BAA0B,IAAA,CAAK,cAAA,EAAgB;YACpD,WAAU,KAAA,EAAO,UAAA;gBACf,WAAW,OAAA,CAAQ;YACrB;QACF;IACF;IAAA;;;;;;;KAAA,GAUA,yBACE,QAAA,EACA,IAAA,EACA;QAxjBJ,IAAA;QAyjBI,SAAS,SAAA,CAAA,CAAU,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAA,KAAgB,KAAK;YACtC,gBAAgB;YAChB,GAAG,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA;QACX;QAEA,MAAM,SAAS,IAAA,CAAK,UAAA,CACjB,WAAA,CAAY,IAAI,qBAChB,SAAA;QAEH,MAAM,OAAO;YACX,IAAI;gBACF,MAAO,KAAM;oBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;oBACrC,IAAI,MAAM;oBACV,SAAS,KAAA,CAAM;gBACjB;YACF,EAAA,OAAS,OAAO;gBACd,MAAM;YACR,SAAE;gBACA,SAAS,GAAA;YACX;QACF;QAEA;IACF;IAAA;;;;;;;KAAA,GAUA,qBAAqB,IAAA,EAA+B;QA3lBtD,IAAA;QA4lBI,OAAO,IAAI,SAAS,IAAA,CAAK,UAAA,CAAW,WAAA,CAAY,IAAI,sBAAsB;YACxE,QAAA,CAAQ,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAA,KAAgB;YACxB,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;AAKO,IAAM,4BAA4B;;AExmBlC,SAAS,iBACd,MAAA;IAEA,OAAO,UAAU,QAAQ,OAAO,IAAA,CAAK,QAAQ,MAAA,GAAS;AACxD;;ACKO,SAAS,0BAEd,EACA,KAAA,EACA,UAAA,EACF;IAOE,IAAI,CAAC,iBAAiB,QAAQ;QAC5B,OAAO;YACL,OAAO,KAAA;YACP,YAAY,KAAA;QACd;IACF;IAEA,OAAO;QACL,OAAO,OAAO,OAAA,CAAQ,OAAO,GAAA,CAAI,CAAC,CAAC,MAAMC,MAAI,GAAA,CAAO;gBAClD,MAAM;gBACN;gBACA,aAAaA,MAAK,WAAA;gBAClB,YAAY,uBAAuBA,MAAK,UAAU;YACpD,CAAA;QACA,YACE,cAAc,OACV;YAAE,MAAM;QAAO,IACf,OAAO,eAAe,WACtB;YAAE,MAAM;QAAW,IACnB;YAAE,MAAM;YAAiB,UAAU,WAAW,QAAA;QAAmB;IACzE;AACF;;ACrCO,SAAS,2BAA2B,EACzC,aAAA,EACA,KAAA,EACA,QAAA,EACA,SAAA,EACA,OAAA,EACF;IAXA,IAAA;IAkBE,OAAO;QACL,qBAAqB,MAAM,QAAA;QAC3B,eAAe,MAAM,OAAA;QAAA,YAAA;QAGrB,GAAG,OAAO,OAAA,CAAQ,UAAU,MAAA,CAAO,CAAC,YAAY,CAAC,KAAK,MAAK;YACzD,UAAA,CAAW,CAAA,YAAA,EAAe,IAAG,CAAE,CAAA,GAAI;YACnC,OAAO;QACT,GAAG,CAAC,EAAe;QAAA,gCAAA;QAGnB,kBAAkB;QAClB,iBAAiB,aAAA,OAAA,KAAA,IAAA,UAAW,UAAA;QAC5B,2BAA2B,aAAA,OAAA,KAAA,IAAA,UAAW,UAAA;QAAA,8BAAA;QAGtC,GAAG,OAAO,OAAA,CAAA,CAAQ,KAAA,aAAA,OAAA,KAAA,IAAA,UAAW,QAAA,KAAX,OAAA,KAAuB,CAAC,GAAG,MAAA,CAC3C,CAAC,YAAY,CAAC,KAAK,MAAK;YACtB,UAAA,CAAW,CAAA,sBAAA,EAAyB,IAAG,CAAE,CAAA,GAAI;YAC7C,OAAO;QACT,GACA,CAAC,EACH;QAAA,kBAAA;QAGA,GAAG,OAAO,OAAA,CAAQ,WAAA,OAAA,UAAW,CAAC,GAAG,MAAA,CAAO,CAAC,YAAY,CAAC,KAAK,MAAK;YAC9D,IAAI,UAAU,KAAA,GAAW;gBACvB,UAAA,CAAW,CAAA,mBAAA,EAAsB,IAAG,CAAE,CAAA,GAAI;YAC5C;YACA,OAAO;QACT,GAAG,CAAC,EAAe;IACrB;AACF;;;AE7CO,IAAM,aAAqB;IAChC;QACE,OAAO;IACT;IAEA,iBACE,IAAA,EACA,IAAA,EACA,IAAA,EACA,IAAA;QAEA,IAAI,OAAO,SAAS,YAAY;YAC9B,OAAO,KAAK;QACd;QACA,IAAI,OAAO,SAAS,YAAY;YAC9B,OAAO,KAAK;QACd;QACA,IAAI,OAAO,SAAS,YAAY;YAC9B,OAAO,KAAK;QACd;IACF;AACF;AAEA,IAAM,WAAiB;IACrB;QACE,OAAO;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO;IACT;IACA;QACE,OAAO,IAAA;IACT;AACF;AAEA,IAAM,kBAA+B;IACnC,SAAS;IACT,QAAQ;IACR,YAAY;AACd;;AD9DA,IAAI,aAAiC,KAAA;AAM9B,SAAS,UAAU,EAAE,SAAA,EAAU;IACpC,IAAI,CAAC,WAAW;QACd,OAAO;IACT;IAEA,IAAI,YAAY;QACd,OAAO;IACT;IAEA,OAAO,2DAAA,CAAA,QAAA,CAAM,SAAA,CAAU;AACzB;;AEpBO,SAAS,WAAc,EAC5B,IAAA,EACA,MAAA,EACA,UAAA,EACA,EAAA,EACA,cAAc,IAAA,EAChB;IAOE,OAAO,OAAO,eAAA,CAAgB,MAAM;QAAE;IAAW,GAAG,OAAM;QACxD,IAAI;YACF,MAAM,SAAS,MAAM,GAAG;YAExB,IAAI,aAAa;gBACf,KAAK,GAAA;YACP;YAEA,OAAO;QACT,EAAA,OAAS,OAAO;YACd,IAAI;gBACF,IAAI,iBAAiB,OAAO;oBAC1B,KAAK,eAAA,CAAgB;wBACnB,MAAM,MAAM,IAAA;wBACZ,SAAS,MAAM,OAAA;wBACf,OAAO,MAAM,KAAA;oBACf;oBACA,KAAK,SAAA,CAAU;wBACb,MAAM,2DAAA,CAAA,iBAAA,CAAe,KAAA;wBACrB,SAAS,MAAM,OAAA;oBACjB;gBACF,OAAO;oBACL,KAAK,SAAA,CAAU;wBAAE,MAAM,2DAAA,CAAA,iBAAA,CAAe,KAAA;oBAAM;gBAC9C;YACF,SAAE;gBAEA,KAAK,GAAA;YACP;YAEA,MAAM;QACR;IACF;AACF;;;ACFO,SAAS,cAAsD,EACpE,QAAA,EACA,KAAA,EACF;IAIE,MAAM,WAAW,SAAS,QAAA;IAE1B,IAAI,SAAS,MAAM;QACjB,MAAM,IAAI,+KAAA,CAAA,kBAAA,CAAgB;YAAE,UAAU,SAAS,QAAA;QAAS;IAC1D;IAEA,MAAMA,QAAO,KAAA,CAAM,SAAQ;IAE3B,IAAIA,SAAQ,MAAM;QAChB,MAAM,IAAI,+KAAA,CAAA,kBAAA,CAAgB;YACxB,UAAU,SAAS,QAAA;YACnB,gBAAgB,OAAO,IAAA,CAAK;QAC9B;IACF;IAEA,MAAM,cAAcC,CAAAA,GAAAA,wLAAAA,CAAAA,gBAAAA,EAAc;QAChC,MAAM,SAAS,IAAA;QACf,QAAQD,MAAK,UAAA;IACf;IAEA,IAAI,YAAY,OAAA,KAAY,OAAO;QACjC,MAAM,IAAI,+KAAA,CAAA,4BAAA,CAA0B;YAClC;YACA,UAAU,SAAS,IAAA;YACnB,OAAO,YAAY,KAAA;QACrB;IACF;IAEA,OAAO;QACL,MAAM;QACN,YAAY,SAAS,UAAA;QACrB;QACA,MAAM,YAAY,KAAA;IACpB;AACF;;AChBA,eAAsB,aAAqD,EACzE,KAAA,EACA,KAAA,EACA,UAAA,EACA,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,yBAAyB,CAAA,EACzB,oBAAoB,sBAAA,EACpB,wBAAwB,SAAA,EACxB,GAAG,UACL;IApFA,IAAA;IA6HE,MAAM,0BAA0B,2BAA2B;QACzD,eAAe;QACf;QACA;QACA;QACA,UAAU;YAAE,GAAG,QAAA;YAAU;QAAW;IACtC;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAW,KAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAA,KAAwB;IAAM;IACpE,OAAO,WAAW;QAChB,MAAM;QACN,YAAY;YACV,GAAG,uBAAA;YAAA,6DAAA;YAEH,aAAa,KAAK,SAAA,CAAU;gBAAE;gBAAQ;gBAAQ;YAAS;YACvD,iCAAiC;QACnC;QACA;QACA,IAAI,OAAM;YA/Id,IAAAE,KAAA,IAAA;YAgJM,MAAM,QAAQ,4BAA4B;gBAAE;YAAW;YACvD,MAAM,kBAAkB,mBAAmB;gBACzC;gBACA;gBACA;YACF;YAEA,MAAM,OAAO;gBACX,MAAM;gBACN,GAAG,0BAA0B;oBAAE;oBAAO;gBAAW,EAAC;YACpD;YACA,MAAM,eAAe,oBAAoB;YACzC,MAAM,iBAAiB,6BAA6B;YAEpD,IAAI;YAGJ,IAAI,mBAA2C,EAAC;YAChD,IAAI,qBAA+C,EAAC;YACpD,IAAI,aAAa;YACjB,MAAM,mBACJ,EAAC;YAEH,GAAG;gBAED,MAAM,qBACJ,eAAe,IAAI,gBAAgB,IAAA,GAAO;gBAE5C,uBAAuB,MAAM,MAAM,IACjC,WAAW;wBACT,MAAM;wBACN,YAAY;4BACV,GAAG,uBAAA;4BACH,oBAAoB;4BACpB,sBAAsB,KAAK,SAAA,CAAU;wBACvC;wBACA;wBACA,IAAI,OAAMC;4BACR,MAAM,SAAS,MAAM,MAAM,UAAA,CAAW;gCACpC;gCACA,GAAG,YAAA;gCACH,aAAa;gCACb,QAAQ;gCACR;gCACA;4BACF;4BAGAA,MAAK,aAAA,CAAc;gCACjB,mBAAmB,OAAO,YAAA;gCAC1B,yBAAyB,OAAO,KAAA,CAAM,YAAA;gCACtC,6BAA6B,OAAO,KAAA,CAAM,gBAAA;gCAC1C,kBAAkB,OAAO,IAAA;gCACzB,uBAAuB,KAAK,SAAA,CAAU,OAAO,SAAS;4BACxD;4BAEA,OAAO;wBACT;oBACF;gBAIF,mBAAA,CAAA,CAAoBD,MAAA,qBAAqB,SAAA,KAArB,OAAAA,MAAkC,EAAC,EAAG,GAAA,CACxD,CAAA,gBAAiB,cAAc;wBAAE,UAAU;wBAAe;oBAAM;gBAIlE,qBACE,SAAS,OACL,EAAC,GACD,MAAM,aAAa;oBACjB,WAAW;oBACX;oBACA;gBACF;gBAGN,MAAM,sBAAsB,mBAAmB;oBAC7C,MAAA,CAAM,KAAA,qBAAqB,IAAA,KAArB,OAAA,KAA6B;oBACnC,WAAW;oBACX,aAAa;gBACf;gBACA,iBAAiB,IAAA,IAAQ;gBACzB,eAAe,IAAA,IACV,oBAAoB,GAAA,CAAI;YAE/B,QAAA,wBAAA;YAEE,iBAAiB,MAAA,GAAS,KAAA,uCAAA;YAE1B,mBAAmB,MAAA,KAAW,iBAAiB,MAAA,IAAA,qDAAA;YAE/C,eAAe,kBAAA;YAIjB,KAAK,aAAA,CAAc;gBACjB,mBAAmB,qBAAqB,YAAA;gBACxC,yBAAyB,qBAAqB,KAAA,CAAM,YAAA;gBACpD,6BACE,qBAAqB,KAAA,CAAM,gBAAA;gBAC7B,kBAAkB,qBAAqB,IAAA;gBACvC,uBAAuB,KAAK,SAAA,CAAU,qBAAqB,SAAS;YACtE;YAEA,OAAO,IAAI,mBAAmB;gBAAA,iFAAA;gBAAA,8DAAA;gBAAA,2CAAA;gBAI5B,MAAA,CAAM,KAAA,qBAAqB,IAAA,KAArB,OAAA,KAA6B;gBACnC,WAAW;gBACX,aAAa;gBACb,cAAc,qBAAqB,YAAA;gBACnC,OAAO,8BAA8B,qBAAqB,KAAK;gBAC/D,UAAU,qBAAqB,QAAA;gBAC/B,aAAa,qBAAqB,WAAA;gBAClC,UAAU,qBAAqB,QAAA;gBAC/B;YACF;QACF;IACF;AACF;AAEA,eAAe,aAAqD,EAClE,SAAA,EACA,KAAA,EACA,MAAA,EACF;IAKE,MAAM,cAAc,MAAM,QAAQ,GAAA,CAChC,UAAU,GAAA,CAAI,OAAM;QAClB,MAAMF,QAAO,KAAA,CAAM,SAAS,QAAQ,CAAA;QAEpC,IAAA,CAAIA,SAAA,OAAA,KAAA,IAAAA,MAAM,OAAA,KAAW,MAAM;YACzB,OAAO,KAAA;QACT;QAEA,MAAM,SAAS,MAAM,WAAW;YAC9B,MAAM;YACN,YAAY;gBACV,oBAAoB,SAAS,QAAA;gBAC7B,kBAAkB,SAAS,UAAA;gBAC3B,oBAAoB,KAAK,SAAA,CAAU,SAAS,IAAI;YAClD;YACA;YACA,IAAI,OAAM;gBACR,MAAMI,UAAS,MAAMJ,MAAK,OAAA,CAAS,SAAS,IAAI;gBAEhD,IAAI;oBACF,KAAK,aAAA,CAAc;wBACjB,sBAAsB,KAAK,SAAA,CAAUI;oBACvC;gBACF,EAAA,OAAS,SAAS,CAKlB;gBAEA,OAAOA;YACT;QACF;QAEA,OAAO;YACL,YAAY,SAAS,UAAA;YACrB,UAAU,SAAS,QAAA;YACnB,MAAM,SAAS,IAAA;YACf;QACF;IACF;IAGF,OAAO,YAAY,MAAA,CACjB,CAAC,SAAiD,UAAU;AAEhE;AAMO,IAAM,qBAAN;IAwDL,YAAY,OAAA,CAYT;QACD,IAAA,CAAK,IAAA,GAAO,QAAQ,IAAA;QACpB,IAAA,CAAK,SAAA,GAAY,QAAQ,SAAA;QACzB,IAAA,CAAK,WAAA,GAAc,QAAQ,WAAA;QAC3B,IAAA,CAAK,YAAA,GAAe,QAAQ,YAAA;QAC5B,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;QACxB,IAAA,CAAK,WAAA,GAAc,QAAQ,WAAA;QAC3B,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;QACxB,IAAA,CAAK,gBAAA,GAAmB,QAAQ,gBAAA;IAClC;AACF;AAKA,SAAS,mBAA2D,EAClE,IAAA,EACA,SAAA,EACA,WAAA,EACF;IAKE,MAAM,mBAAkE,EAAC;IAEzE,iBAAiB,IAAA,CAAK;QACpB,MAAM;QACN,SAAS;YAAC;gBAAE,MAAM;gBAAQ;YAAK;eAAM;SAAS;IAChD;IAEA,IAAI,YAAY,MAAA,GAAS,GAAG;QAC1B,iBAAiB,IAAA,CAAK;YACpB,MAAM;YACN,SAAS,YAAY,GAAA,CAAI,CAAA,SAAA,CAAW;oBAClC,MAAM;oBACN,YAAY,OAAO,UAAA;oBACnB,UAAU,OAAO,QAAA;oBACjB,QAAQ,OAAO,MAAA;gBACjB,CAAA;QACF;IACF;IAEA,OAAO;AACT;AAKO,IAAM,4BAA4B;;;ACrblC,SAAS,uBAA+D,EAC7E,KAAA,EACA,eAAA,EACA,MAAA,EACF;IAKE,IAAI,WAAW;IACf,MAAM,uBAAuB,aAAA,GAAA,IAAI;IAGjC,IAAI,8BAEO;IACX,MAAM,oBAAoB,IAAI,eAAsC;QAClE,OAAM,UAAA;YACJ,8BAA8B;QAChC;IACF;IAGA,MAAM,gBAAgB,IAAI,gBAGxB;QACA,WACE,KAAA,EACA,UAAA;YAEA,MAAM,YAAY,MAAM,IAAA;YAExB,OAAQ;gBAEN,KAAK;gBACL,KAAK;oBAAS;wBACZ,WAAW,OAAA,CAAQ;wBACnB;oBACF;gBAGA,KAAK;oBAAa;wBAChB,MAAM,WAAW,MAAM,QAAA;wBAEvB,IAAI,SAAS,MAAM;4BACjB,4BAA6B,OAAA,CAAQ;gCACnC,MAAM;gCACN,OAAO,IAAIC,+KAAAA,CAAAA,kBAAAA,CAAgB;oCAAE,UAAU,MAAM,QAAA;gCAAS;4BACxD;4BACA;wBACF;wBAEA,MAAML,QAAO,KAAA,CAAM,SAAQ;wBAE3B,IAAIA,SAAQ,MAAM;4BAChB,4BAA6B,OAAA,CAAQ;gCACnC,MAAM;gCACN,OAAO,IAAIK,+KAAAA,CAAAA,kBAAAA,CAAgB;oCACzB,UAAU,MAAM,QAAA;oCAChB,gBAAgB,OAAO,IAAA,CAAK;gCAC9B;4BACF;4BAEA;wBACF;wBAEA,IAAI;4BACF,MAAM,WAAW,cAAc;gCAC7B,UAAU;gCACV;4BACF;4BAEA,WAAW,OAAA,CAAQ;4BAEnB,IAAIL,MAAK,OAAA,IAAW,MAAM;gCACxB,MAAM,kBAAkB,CAAA,GAAA,gNAAA,CAAA,aAAA;gCACxB,qBAAqB,GAAA,CAAI;gCAKzB,WAAW;oCACT,MAAM;oCACN,YAAY;wCACV,oBAAoB,SAAS,QAAA;wCAC7B,kBAAkB,SAAS,UAAA;wCAC3B,oBAAoB,KAAK,SAAA,CAAU,SAAS,IAAI;oCAClD;oCACA;oCACA,IAAI,OAAM,OACRA,MAAK,OAAA,CAAS,SAAS,IAAI,EAAE,IAAA,CAC3B,CAAC;4CACC,4BAA6B,OAAA,CAAQ;gDACnC,GAAG,QAAA;gDACH,MAAM;gDACN;4CACF;4CAEA,qBAAqB,MAAA,CAAO;4CAG5B,IAAI,YAAY,qBAAqB,IAAA,KAAS,GAAG;gDAC/C,4BAA6B,KAAA;4CAC/B;4CAGA,IAAI;gDACF,KAAK,aAAA,CAAc;oDACjB,sBAAsB,KAAK,SAAA,CAAU;gDACvC;4CACF,EAAA,OAAS,SAAS,CAKlB;wCACF,GACA,CAAC;4CACC,4BAA6B,OAAA,CAAQ;gDACnC,MAAM;gDACN;4CACF;4CAEA,qBAAqB,MAAA,CAAO;4CAG5B,IAAI,YAAY,qBAAqB,IAAA,KAAS,GAAG;gDAC/C,4BAA6B,KAAA;4CAC/B;wCACF;gCAEN;4BACF;wBACF,EAAA,OAAS,OAAO;4BACd,4BAA6B,OAAA,CAAQ;gCACnC,MAAM;gCACN;4BACF;wBACF;wBAEA;oBACF;gBAGA,KAAK;oBAAU;wBACb,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,cAAc,MAAM,YAAA;4BACpB,UAAU,MAAM,QAAA;4BAChB,OAAO,8BAA8B,MAAM,KAAK;wBAClD;wBACA;oBACF;gBAGA,KAAK;oBAAmB;wBACtB;oBACF;gBAEA;oBAAS;wBACP,MAAM,mBAA0B;wBAChC,MAAM,IAAI,MAAM,CAAA,sBAAA,EAAyB,iBAAgB,CAAE;oBAC7D;YACF;QACF;QAEA;YACE,WAAW;YAEX,IAAI,qBAAqB,IAAA,KAAS,GAAG;gBACnC,4BAA6B,KAAA;YAC/B;QACF;IACF;IAGA,OAAO,IAAI,eAAsC;QAC/C,MAAM,OAAM,UAAA;YAGV,OAAO,QAAQ,GAAA,CAAI;gBACjB,gBAAgB,WAAA,CAAY,eAAe,MAAA,CACzC,IAAI,eAAe;oBACjB,OAAM,KAAA;wBACJ,WAAW,OAAA,CAAQ;oBACrB;oBACA,UAEA;gBACF;gBAEF,kBAAkB,MAAA,CAChB,IAAI,eAAe;oBACjB,OAAM,KAAA;wBACJ,WAAW,OAAA,CAAQ;oBACrB;oBACA;wBACE,WAAW,KAAA;oBACb;gBACF;aAEH;QACH;IACF;AACF;;AC5IA,eAAsB,WAAmD,EACvE,KAAA,EACA,KAAA,EACA,UAAA,EACA,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,wBAAwB,SAAA,EACxB,QAAA,EACA,GAAG,UACL;IAvFA,IAAA;IA2JE,MAAM,0BAA0B,2BAA2B;QACzD,eAAe;QACf;QACA;QACA;QACA,UAAU;YAAE,GAAG,QAAA;YAAU;QAAW;IACtC;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAW,KAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAA,KAAwB;IAAM;IAEpE,OAAO,WAAW;QAChB,MAAM;QACN,YAAY;YACV,GAAG,uBAAA;YAAA,6DAAA;YAEH,aAAa,KAAK,SAAA,CAAU;gBAAE;gBAAQ;gBAAQ;YAAS;QACzD;QACA;QACA,aAAa;QACb,IAAI,OAAM;YACR,MAAM,QAAQ,4BAA4B;gBAAE;YAAW;YACvD,MAAM,kBAAkB,mBAAmB;gBAAE;gBAAQ;gBAAQ;YAAS;YACtE,MAAM,iBAAiB,6BAA6B;YACpD,MAAM,EACJ,QAAQ,EAAE,MAAA,EAAQ,QAAA,EAAU,WAAA,EAAY,EACxC,YAAA,EACF,GAAI,MAAM,MAAM,IACd,WAAW;oBACT,MAAM;oBACN,YAAY;wBACV,GAAG,uBAAA;wBACH,oBAAoB,gBAAgB,IAAA;wBACpC,sBAAsB,KAAK,SAAA,CAAU;oBACvC;oBACA;oBACA,aAAa;oBACb,IAAI,OAAMM;wBACR,OAAO;4BACL,QAAQ,MAAM,MAAM,QAAA,CAAS;gCAC3B,MAAM;oCACJ,MAAM;oCACN,GAAG,0BAA0B;wCAAE;wCAAO;oCAAW,EAAC;gCACpD;gCACA,GAAG,oBAAoB,SAAQ;gCAC/B,aAAa,gBAAgB,IAAA;gCAC7B,QAAQ;gCACR;gCACA;4BACF;4BACA,cAAAA;wBACF;oBACF;gBACF;YAGF,OAAO,IAAI,iBAAiB;gBAC1B,QAAQ,uBAAuB;oBAC7B;oBACA,iBAAiB;oBACjB;gBACF;gBACA;gBACA;gBACA;gBACA;gBACA;YACF;QACF;IACF;AACF;AA+BO,IAAM,mBAAN;IA4CL,YAAY,EACV,MAAA,EACA,QAAA,EACA,WAAA,EACA,QAAA,EACA,QAAA,EACA,YAAA,EACF,CASG;QACD,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,WAAA,GAAc;QACnB,IAAA,CAAK,QAAA,GAAW;QAGhB,IAAI;QAGJ,IAAA,CAAK,KAAA,GAAQ,IAAI,QAA8B,CAAA;YAC7C,eAAe;QACjB;QAGA,IAAI;QAGJ,IAAA,CAAK,YAAA,GAAe,IAAI,QAAsB,CAAA;YAC5C,sBAAsB;QACxB;QAGA,IAAI;QACJ,IAAA,CAAK,IAAA,GAAO,IAAI,QAAgB,CAAA;YAC9B,cAAc;QAChB;QAGA,IAAI;QAGJ,IAAA,CAAK,SAAA,GAAY,IAAI,QAA6B,CAAA;YAChD,mBAAmB;QACrB;QAGA,IAAI;QAGJ,IAAA,CAAK,WAAA,GAAc,IAAI,QAA+B,CAAA;YACpD,qBAAqB;QACvB;QAGA,IAAI;QACJ,IAAI;QACJ,IAAI,OAAO;QACX,MAAM,YAAiC,EAAC;QACxC,MAAM,cAAqC,EAAC;QAC5C,IAAI,aAAa;QAGjB,MAAM,OAAO,IAAA;QACb,IAAA,CAAK,cAAA,GAAiB,OAAO,WAAA,CAC3B,IAAI,gBAA8D;YAChE,MAAM,WAAU,KAAA,EAAO,UAAA;gBACrB,WAAW,OAAA,CAAQ;gBAGnB,IAAI,YAAY;oBACd,aAAa;oBACb,aAAa,QAAA,CAAS;gBACxB;gBAEA,MAAM,YAAY,MAAM,IAAA;gBACxB,OAAQ;oBACN,KAAK;wBAEH,QAAQ,MAAM,SAAA;wBACd;oBAEF,KAAK;wBAEH,UAAU,IAAA,CAAK;wBACf;oBAEF,KAAK;wBAEH,YAAY,IAAA,CAAK;wBACjB;oBAEF,KAAK;wBAGH,QAAQ,MAAM,KAAA;wBACd,eAAe,MAAM,YAAA;wBAGrB,aAAa;wBACb,oBAAoB;wBACpB,YAAY;wBACZ,iBAAiB;wBACjB;oBAEF,KAAK;wBAEH;oBAEF;wBAAS;4BACP,MAAM,kBAAyB;4BAC/B,MAAM,IAAI,MAAM,CAAA,oBAAA,EAAuB,gBAAe,CAAE;wBAC1D;gBACF;YACF;YAAA,8FAAA;YAGA,MAAM,OAAM,UAAA;gBArapB,IAAA;gBAsaU,IAAI;oBACF,MAAM,aAAa,SAAA,OAAA,QAAS;wBAC1B,cAAc;wBACd,kBAAkB;wBAClB,aAAa;oBACf;oBACA,MAAM,oBAAoB,gBAAA,OAAA,eAAgB;oBAC1C,MAAM,qBACJ,UAAU,MAAA,GAAS,IAAI,KAAK,SAAA,CAAU,aAAa,KAAA;oBAErD,aAAa,aAAA,CAAc;wBACzB,mBAAmB;wBACnB,yBAAyB,WAAW,YAAA;wBACpC,6BAA6B,WAAW,gBAAA;wBACxC,kBAAkB;wBAClB,uBAAuB;oBACzB;oBAGA,aAAa,GAAA;oBAGb,SAAS,aAAA,CAAc;wBACrB,mBAAmB;wBACnB,yBAAyB,WAAW,YAAA;wBACpC,6BAA6B,WAAW,gBAAA;wBACxC,kBAAkB;wBAClB,uBAAuB;oBACzB;oBAGA,mBAAmB;oBAGnB,MAAA,CAAA,CAAM,KAAA,KAAK,QAAA,KAAL,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,MAAgB;wBACpB,cAAc;wBACd,OAAO;wBACP;wBACA;wBAAA,oEAAA;wBAAA,kEAAA;wBAAA,sEAAA;wBAAA,4DAAA;wBAKA;wBACA;wBACA;oBACF,EAAA;gBACF,EAAA,OAAS,OAAO;oBACd,WAAW,KAAA,CAAM;gBACnB,SAAE;oBACA,SAAS,GAAA;gBACX;YACF;QACF;IAEJ;IAAA;;;;;;;KAAA,GAUQ,YAAY;QAClB,MAAM,CAAC,SAAS,QAAO,GAAI,IAAA,CAAK,cAAA,CAAe,GAAA;QAC/C,IAAA,CAAK,cAAA,GAAiB;QACtB,OAAO;IACT;IAAA;;;;KAAA,GAOA,IAAI,aAA0C;QAC5C,OAAO,0BAA0B,IAAA,CAAK,SAAA,IAAa;YACjD,WAAU,KAAA,EAAO,UAAA;gBACf,IAAI,MAAM,IAAA,KAAS,cAAc;oBAE/B,IAAI,MAAM,SAAA,CAAU,MAAA,GAAS,GAAG;wBAC9B,WAAW,OAAA,CAAQ,MAAM,SAAS;oBACpC;gBACF,OAAA,IAAW,MAAM,IAAA,KAAS,SAAS;oBACjC,WAAW,KAAA,CAAM,MAAM,KAAK;gBAC9B;YACF;QACF;IACF;IAAA;;;;;KAAA,GAQA,IAAI,aAAyD;QAC3D,OAAO,0BAA0B,IAAA,CAAK,SAAA,IAAa;YACjD,WAAU,KAAA,EAAO,UAAA;gBACf,IAAI,MAAM,IAAA,KAAS,cAAc;oBAE/B,IAAI,MAAM,SAAA,CAAU,MAAA,GAAS,GAAG;wBAC9B,WAAW,OAAA,CAAQ;oBACrB;gBACF,OAAO;oBACL,WAAW,OAAA,CAAQ;gBACrB;YACF;QACF;IACF;IAAA;;;;;;;;KAAA,GAWA,WAAW,YAAyC,CAAC,CAAA,EAAG;QACtD,IAAI,qBAAqB;QAEzB,MAAM,sBAAsB,IAAI,gBAG9B;YACA,MAAM;gBACJ,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA;YACzC;YAEA,MAAM,WAAU,KAAA,EAAO,UAAA;gBACrB,WAAW,OAAA,CAAQ;gBAEnB,IAAI,MAAM,IAAA,KAAS,cAAc;oBAC/B,MAAM,YAAY,MAAM,SAAA;oBAExB,sBAAsB;oBAEtB,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA,CAAQ;oBAC/C,IAAI,UAAU,MAAA,EAAQ,MAAM,UAAU,MAAA,CAAO;gBAC/C;YACF;YAEA,MAAM;gBACJ,IAAI,UAAU,YAAA,EACZ,MAAM,UAAU,YAAA,CAAa;gBAC/B,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA,CAAQ;YACjD;QACF;QAEA,MAAM,wBAAwB,IAAI,gBAGhC;YACA,WAAW,OAAO,OAAO;gBACvB,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBACH,WAAW,OAAA,CAAQ,CAAA,GAAA,4KAAA,CAAA,mBAAA,EAAiB,QAAQ,MAAM,SAAS;wBAC3D;oBACF,KAAK;wBACH,WAAW,OAAA,CACT,CAAA,GAAA,4KAAA,CAAA,mBAAA,EAAiB,aAAa;4BAC5B,YAAY,MAAM,UAAA;4BAClB,UAAU,MAAM,QAAA;4BAChB,MAAM,MAAM,IAAA;wBACd;wBAEF;oBACF,KAAK;wBACH,WAAW,OAAA,CACT,CAAA,GAAA,4KAAA,CAAA,mBAAA,EAAiB,eAAe;4BAC9B,YAAY,MAAM,UAAA;4BAClB,UAAU,MAAM,QAAA;4BAChB,MAAM,MAAM,IAAA;4BACZ,QAAQ,MAAM,MAAA;wBAChB;wBAEF;oBACF,KAAK;wBACH,WAAW,OAAA,CACT,CAAA,GAAA,4KAAA,CAAA,mBAAA,EAAiB,SAAS,KAAK,SAAA,CAAU,MAAM,KAAK;wBAEtD;gBACJ;YACF;QACF;QAEA,OAAO,IAAA,CAAK,UAAA,CACT,WAAA,CAAY,qBACZ,WAAA,CAAY,uBACZ,WAAA,CAAY,IAAI;IACrB;IAAA;;;;;;;KAAA,GAUA,uBACE,QAAA,EACA,IAAA,EACA;QApnBJ,IAAA;QAqnBI,SAAS,SAAA,CAAA,CAAU,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAA,KAAgB,KAAK;YACtC,gBAAgB;YAChB,GAAG,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA;QACX;QAEA,MAAM,SAAS,IAAA,CAAK,UAAA,GAAa,SAAA;QAEjC,MAAM,OAAO;YACX,IAAI;gBACF,MAAO,KAAM;oBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;oBACrC,IAAI,MAAM;oBACV,SAAS,KAAA,CAAM;gBACjB;YACF,EAAA,OAAS,OAAO;gBACd,MAAM;YACR,SAAE;gBACA,SAAS,GAAA;YACX;QACF;QAEA;IACF;IAAA;;;;;;;KAAA,GAUA,yBACE,QAAA,EACA,IAAA,EACA;QAxpBJ,IAAA;QAypBI,SAAS,SAAA,CAAA,CAAU,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAA,KAAgB,KAAK;YACtC,gBAAgB;YAChB,GAAG,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA;QACX;QAEA,MAAM,SAAS,IAAA,CAAK,UAAA,CACjB,WAAA,CAAY,IAAI,qBAChB,SAAA;QAEH,MAAM,OAAO;YACX,IAAI;gBACF,MAAO,KAAM;oBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;oBACrC,IAAI,MAAM;oBACV,SAAS,KAAA,CAAM;gBACjB;YACF,EAAA,OAAS,OAAO;gBACd,MAAM;YACR,SAAE;gBACA,SAAS,GAAA;YACX;QACF;QAEA;IACF;IAAA;;;;;;;KAAA,GAUA,mBAAmB,IAAA,EAA+B;QAChD,OAAO,IAAI,sBAAsB,IAAA,CAAK,UAAA,IAAc;IACtD;IAAA;;;;;;KAAA,GASA,qBAAqB,IAAA,EAA+B;QAtsBtD,IAAA;QAusBI,OAAO,IAAI,SAAS,IAAA,CAAK,UAAA,CAAW,WAAA,CAAY,IAAI,sBAAsB;YACxE,QAAA,CAAQ,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAA,KAAgB;YACxB,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;AAKO,IAAM,0BAA0B;;AC5sBhC,SAAS,sBACd,QAAA;IAMA,MAAM,eAA8B,EAAC;IAErC,KAAA,MAAW,EAAE,IAAA,EAAM,OAAA,EAAS,eAAA,EAAgB,IAAK,SAAU;QACzD,OAAQ;YACN,KAAK;gBAAQ;oBACX,aAAa,IAAA,CAAK;wBAAE,MAAM;wBAAQ;oBAAQ;oBAC1C;gBACF;YAEA,KAAK;gBAAa;oBAChB,IAAI,mBAAmB,MAAM;wBAC3B,aAAa,IAAA,CAAK;4BAAE,MAAM;4BAAa;wBAAQ;wBAC/C;oBACF;oBAGA,aAAa,IAAA,CAAK;wBAChB,MAAM;wBACN,SAAS;4BACP;gCAAE,MAAM;gCAAQ,MAAM;4BAAQ;+BAC3B,gBAAgB,GAAA,CAAI,CAAC,EAAE,UAAA,EAAY,QAAA,EAAU,IAAA,EAAK,GAAA,CAAO;oCAC1D,MAAM;oCACN;oCACA;oCACA;gCACF,CAAA;yBACF;oBACF;oBAGA,aAAa,IAAA,CAAK;wBAChB,MAAM;wBACN,SAAS,gBAAgB,GAAA,CACvB,CAAC,EAAE,UAAA,EAAY,QAAA,EAAU,IAAA,EAAM,MAAA,EAAO,GAAA,CAAO;gCAC3C,MAAM;gCACN;gCACA;gCACA;gCACA;4BACF,CAAA;oBAEJ;oBAEA;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,MAAM,CAAA,gBAAA,EAAmB,iBAAgB,CAAE;gBACvD;QACF;IACF;IAEA,OAAO;AACT;;ACpEO,IAAM,sBAAN,cAAkC;IAGvC,YAAY,EACV,EAAA,EACA,UAAU,CAAA,kBAAA,EAAqB,GAAE,CAAA,EACnC,CAGG;QACD,KAAA,CAAM;QAEN,IAAA,CAAK,IAAA,GAAO;QAEZ,IAAA,CAAK,EAAA,GAAK;IACZ;IAEA,OAAO,sBAAsB,KAAA,EAA8C;QACzE,OACE,iBAAiB,SACjB,MAAM,IAAA,KAAS,4BACf,OAAQ,MAA8B,EAAA,KAAO;IAEjD;IAEA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,IAAI,IAAA,CAAK,EAAA;QACX;IACF;AACF;;AClCO,IAAM,mBAAN,cAA+B;IAIpC,YAAY,EACV,OAAA,EACA,SAAA,EACA,UAAU,CAAA,QAAA,EAAW,UAAS,EAAA,EAAK,QAAO,CAAA,EAC5C,CAIG;QACD,KAAA,CAAM;QAEN,IAAA,CAAK,IAAA,GAAO;QAEZ,IAAA,CAAK,OAAA,GAAU;QACf,IAAA,CAAK,SAAA,GAAY;IACnB;IAEA,OAAO,mBAAmB,KAAA,EAA2C;QACnE,OACE,iBAAiB,SACjB,MAAM,IAAA,KAAS,yBACf,OAAQ,MAA2B,OAAA,KAAY,YAC/C,OAAQ,MAA2B,SAAA,KAAc;IAErD;IAEA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,SAAS,IAAA,CAAK,OAAA;YACd,WAAW,IAAA,CAAK,SAAA;QAClB;IACF;AACF;;ACxCO,IAAM,sBAAN,cAAkC;IAIvC,YAAY,EACV,UAAA,EACA,kBAAA,EACA,UAAU,CAAA,kBAAA,EAAqB,WAAU,uBAAA,EAA0B,mBAAmB,IAAA,GAAM,CAAA,CAAA,EAC9F,CAIG;QACD,KAAA,CAAM;QAEN,IAAA,CAAK,IAAA,GAAO;QAEZ,IAAA,CAAK,UAAA,GAAa;QAClB,IAAA,CAAK,kBAAA,GAAqB;IAC5B;IAEA,OAAO,sBAAsB,KAAA,EAA8C;QACzE,OACE,iBAAiB,SACjB,MAAM,IAAA,KAAS,4BACf,OAAQ,MAA8B,UAAA,KAAe,YACrD,MAAM,OAAA,CAAS,MAA8B,kBAAkB;IAEnE;IAEA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,YAAY,IAAA,CAAK,UAAA;YACjB,oBAAoB,IAAA,CAAK,kBAAA;QAC3B;IACF;AACF;;ACoBO,SAAS,oCACd,SAAA;IAEA,MAAM,WAAW,IAAI;IAErB,KAAA,MAAW,CAAC,IAAI,SAAQ,IAAK,OAAO,OAAA,CAAQ,WAAY;QACtD,SAAS,gBAAA,CAAiB;YAAE;YAAI;QAAS;IAC3C;IAEA,OAAO;AACT;AAKO,IAAM,mCACX;AAEF,IAAM,0BAAN;IAAA,aAAA;QACE,IAAA,CAAQ,SAAA,GAAsC,CAAC;IAAA;IAE/C,iBAAiB,EAAE,EAAA,EAAI,QAAA,EAAS,EAA6C;QAC3E,IAAA,CAAK,SAAA,CAAU,GAAE,GAAI;IACvB;IAEQ,YAAY,EAAA,EAAsB;QACxC,MAAM,WAAW,IAAA,CAAK,SAAA,CAAU,GAAE;QAElC,IAAI,YAAY,MAAM;YACpB,MAAM,IAAI,oBAAoB;gBAC5B,YAAY;gBACZ,oBAAoB,OAAO,IAAA,CAAK,IAAA,CAAK,SAAS;YAChD;QACF;QAEA,OAAO;IACT;IAEQ,QAAQ,EAAA,EAA8B;QAC5C,MAAM,QAAQ,GAAG,OAAA,CAAQ;QAEzB,IAAI,UAAU,CAAA,GAAI;YAChB,MAAM,IAAI,oBAAoB;gBAAE;YAAG;QACrC;QAEA,OAAO;YAAC,GAAG,KAAA,CAAM,GAAG;YAAQ,GAAG,KAAA,CAAM,QAAQ;SAAE;IACjD;IAEA,cAAc,EAAA,EAA2B;QA5G3C,IAAA,IAAA;QA6GI,MAAM,CAAC,YAAY,QAAO,GAAI,IAAA,CAAK,OAAA,CAAQ;QAC3C,MAAM,QAAA,CAAQ,KAAA,CAAA,KAAA,IAAA,CAAK,WAAA,CAAY,WAAU,EAAE,aAAA,KAA7B,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,IAA6C;QAE3D,IAAI,SAAS,MAAM;YACjB,MAAM,IAAI,iBAAiB;gBAAE,SAAS;gBAAI,WAAW;YAAiB;QACxE;QAEA,OAAO;IACT;IAEA,mBAAmB,EAAA,EAAoC;QAvHzD,IAAA,IAAA;QAwHI,MAAM,CAAC,YAAY,QAAO,GAAI,IAAA,CAAK,OAAA,CAAQ;QAC3C,MAAM,QAAA,CAAQ,KAAA,CAAA,KAAA,IAAA,CAAK,WAAA,CAAY,WAAU,EAAE,aAAA,KAA7B,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,IAA6C;QAE3D,IAAI,SAAS,MAAM;YACjB,MAAM,IAAI,iBAAiB;gBACzB,SAAS;gBACT,WAAW;YACb;QACF;QAEA,OAAO;IACT;AACF;;ACtFO,SAAS,KACdN,KAAAA;IAEA,OAAOA;AACT;;;AExCO,SAAS,iBAAiB,OAAA,EAAmB,OAAA;IAClD,IAAI,QAAQ,MAAA,KAAW,QAAQ,MAAA,EAAQ;QACrC,MAAM,IAAI,MACR,CAAA,4CAAA,EAA+C,QAAQ,MAAM,CAAA,oBAAA,EAAuB,QAAQ,MAAM,CAAA,UAAA,CAAA;IAEtG;IAEA,OACE,WAAW,SAAS,WAAO,CAAK,UAAU,WAAW,UAAU,QAAO;AAE1E;AAQA,SAAS,WAAW,OAAA,EAAmB,OAAA;IACrC,OAAO,QAAQ,MAAA,CACb,CAAC,aAAqB,OAAe,QACnC,cAAc,QAAQ,OAAA,CAAQ,MAAK,EACrC;AAEJ;AAOA,SAAS,UAAU,MAAA;IACjB,OAAO,KAAK,IAAA,CAAK,WAAW,QAAQ;AACtC;;AC8BO,SAAS,6BACd,YAAA;IAEA,MAAM,cAAc,IAAI;IACxB,IAAI;IAEJ,OAAO,IAAI,gBAAgB;QACzB,MAAM,OAAM,UAAA;YACV,oBAAoB,CAAA,GAAA,sJAAA,CAAA,eAAA,EAClB,CAAC;gBACC,IACG,UAAU,SACT,MAAM,IAAA,KAAS,WACf,MAAM,IAAA,KAAS,YAAA,6DAAA;gBAAA,4CAAA;gBAGhB,MAAc,KAAA,KAAU,QACzB;oBACA,WAAW,SAAA;oBACX;gBACF;gBAEA,IAAI,UAAU,OAAO;oBACnB,MAAM,gBAAgB,eAClB,aAAa,MAAM,IAAA,EAAM;wBACvB,OAAO,MAAM,KAAA;oBACf,KACA,MAAM,IAAA;oBACV,IAAI,eAAe,WAAW,OAAA,CAAQ;gBACxC;YACF;QAEJ;QAEA,WAAU,KAAA;YACR,kBAAkB,IAAA,CAAK,YAAY,MAAA,CAAO;QAC5C;IACF;AACF;AAwBO,SAAS,2BACd,EAAA;IAEA,MAAM,cAAc,IAAI;IACxB,IAAI,qBAAqB;IACzB,MAAM,YAAY,MAAM,CAAC;IAEzB,OAAO,IAAI,gBAAgB;QACzB,MAAM;YACJ,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA;QACzC;QAEA,MAAM,WAAU,OAAA,EAAS,UAAA;YACvB,MAAM,UAAU,OAAO,YAAY,WAAW,UAAU,QAAQ,OAAA;YAEhE,WAAW,OAAA,CAAQ,YAAY,MAAA,CAAO;YAEtC,sBAAsB;YAEtB,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA,CAAQ;YAC/C,IAAI,UAAU,MAAA,IAAU,OAAO,YAAY,UAAU;gBACnD,MAAM,UAAU,MAAA,CAAO;YACzB;QACF;QAEA,MAAM;YACJ,MAAM,oBAAoB,8BAA8B;YAGxD,IAAI,UAAU,YAAA,EAAc;gBAC1B,MAAM,UAAU,YAAA,CAAa;YAC/B;YAEA,IAAI,UAAU,OAAA,IAAW,CAAC,mBAAmB;gBAC3C,MAAM,UAAU,OAAA,CAAQ;YAC1B;QACF;IACF;AACF;AAEA,SAAS,8BACP,SAAA;IAEA,OAAO,iCAAiC;AAC1C;AAgBO,SAAS;IACd,IAAI,gBAAgB;IAEpB,OAAO,CAAC;QACN,IAAI,eAAe;YACjB,OAAO,KAAK,SAAA;YACZ,IAAI,MAAM,gBAAgB;QAC5B;QACA,OAAO;IACT;AACF;AAoBO,SAAS,SACd,QAAA,EACA,YAAA,EACA,SAAA;IAEA,IAAI,CAAC,SAAS,EAAA,EAAI;QAChB,IAAI,SAAS,IAAA,EAAM;YACjB,MAAM,SAAS,SAAS,IAAA,CAAK,SAAA;YAC7B,OAAO,IAAI,eAAe;gBACxB,MAAM,OAAM,UAAA;oBACV,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;oBACrC,IAAI,CAAC,MAAM;wBACT,MAAM,YAAY,IAAI,cAAc,MAAA,CAAO;wBAC3C,WAAW,KAAA,CAAM,IAAI,MAAM,CAAA,gBAAA,EAAmB,UAAS,CAAE;oBAC3D;gBACF;YACF;QACF,OAAO;YACL,OAAO,IAAI,eAAe;gBACxB,OAAM,UAAA;oBACJ,WAAW,KAAA,CAAM,IAAI,MAAM;gBAC7B;YACF;QACF;IACF;IAEA,MAAM,qBAAqB,SAAS,IAAA,IAAQ;IAE5C,OAAO,mBACJ,WAAA,CAAY,6BAA6B,eACzC,WAAA,CAAY,2BAA2B;AAC5C;AAeA,SAAS;IACP,OAAO,IAAI,eAAe;QACxB,OAAM,UAAA;YACJ,WAAW,KAAA;QACb;IACF;AACF;AAMO,SAAS,0BAA6B,QAAA;IAC3C,IAAI,KAAK,QAAA,CAAS,OAAO,aAAa,CAAA;IACtC,OAAO,IAAI,eAAkB;QAC3B,MAAM,MAAK,UAAA;YACT,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,GAAG,IAAA;YACjC,IAAI,MAAM,WAAW,KAAA;iBAChB,WAAW,OAAA,CAAQ;QAC1B;QAEA,MAAM,QAAO,MAAA;YApSjB,IAAA;YAqSM,MAAA,CAAA,CAAM,KAAA,GAAG,MAAA,KAAH,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,IAAY,OAAA;QACpB;IACF;AACF;;ACnSO,IAAM,aAAN;IASL,aAAc;QARd,IAAA,CAAQ,OAAA,GAAU,IAAI;QAEtB,IAAA,CAAQ,UAAA,GAA0D;QAGlE,IAAA,CAAQ,QAAA,GAAoB;QAC5B,IAAA,CAAQ,cAAA,GAAwC;QAG9C,MAAM,OAAO,IAAA;QAEb,IAAA,CAAK,MAAA,GAAS,IAAI,eAAe;YAC/B,OAAO,OAAM;gBACX,KAAK,UAAA,GAAa;gBAGlB,wCAA4C;oBAC1C,KAAK,cAAA,GAAiB,WAAW;wBAC/B,QAAQ,IAAA,CACN;oBAEJ,GAAG;gBACL;YACF;YACA,MAAM,CAAA,cAEN;YACA,QAAQ,CAAA;gBACN,IAAA,CAAK,QAAA,GAAW;YAClB;QACF;IACF;IAEA,MAAM,QAAuB;QAC3B,IAAI,IAAA,CAAK,QAAA,EAAU;YACjB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAI,CAAC,IAAA,CAAK,UAAA,EAAY;YACpB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAA,CAAK,UAAA,CAAW,KAAA;QAChB,IAAA,CAAK,QAAA,GAAW;QAGhB,IAAI,IAAA,CAAK,cAAA,EAAgB;YACvB,aAAa,IAAA,CAAK,cAAc;QAClC;IACF;IAEA,OAAO,KAAA,EAAwB;QAC7B,IAAI,IAAA,CAAK,QAAA,EAAU;YACjB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAI,CAAC,IAAA,CAAK,UAAA,EAAY;YACpB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAA,CAAK,UAAA,CAAW,OAAA,CACd,IAAA,CAAK,OAAA,CAAQ,MAAA,CAAOO,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;YAAC;SAAM;IAExD;IAEA,wBAAwB,KAAA,EAAwB;QAC9C,IAAI,IAAA,CAAK,QAAA,EAAU;YACjB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAI,CAAC,IAAA,CAAK,UAAA,EAAY;YACpB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAA,CAAK,UAAA,CAAW,OAAA,CACd,IAAA,CAAK,OAAA,CAAQ,MAAA,CAAOA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,uBAAuB;YAAC;SAAM;IAEvE;AACF;AAMO,SAAS;IACd,MAAM,UAAU,IAAI;IACpB,MAAM,UAAU,IAAI;IACpB,OAAO,IAAI,gBAAgB;QACzB,WAAW,OAAO,OAAO;YACvB,MAAM,UAAU,QAAQ,MAAA,CAAO;YAC/B,WAAW,OAAA,CAAQ,QAAQ,MAAA,CAAOA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;QAC7D;IACF;AACF;AAKO,IAAM,0BAAN,cAAsC;AAAY;;ACiBzD,SAAS;IACP,IAAI,WAAW;IAEf,OAAO,CAAA;QACL,MAAM,OAAO,KAAK,KAAA,CAAM;QAGxB,IAAI,WAAW,MAAM;YACnB,MAAM,IAAI,MAAM,CAAA,EAAG,KAAK,KAAA,CAAM,IAAI,CAAA,EAAA,EAAK,KAAK,KAAA,CAAM,OAAO,CAAA,CAAE;QAC7D;QAGA,IAAI,CAAA,CAAE,gBAAgB,IAAA,GAAO;YAC3B;QACF;QAMA,MAAM,OAAO,KAAK,UAAA;QAClB,IACE,CAAC,YACA,KAAK,MAAA,GAAS,SAAS,MAAA,IAAU,KAAK,UAAA,CAAW,WAClD;YACA,MAAM,QAAQ,KAAK,KAAA,CAAM,SAAS,MAAM;YACxC,WAAW;YAEX,OAAO;QACT;QAEA,OAAO;IACT;AACF;AAEA,gBAAgB,WACd,MAAA;IAEA,WAAA,MAAiB,SAAS,OAAQ;QAChC,IAAI,gBAAgB,OAAO;YAEzB,MAAM,OAAO,MAAM,UAAA;YACnB,IAAI,MAAM,MAAM;QAClB,OAAA,IAAW,WAAW,OAAO;YAE3B,MAAM,EAAE,KAAA,EAAM,GAAI;YAClB,IAAI,UAAU,OAAO;gBACnB,MAAM,OAAO,MAAM,IAAA;gBACnB,IAAI,MAAM,MAAM;YAClB;QACF;IACF;AACF;AASO,SAAS,gBACd,GAAA,EAIA,EAAA;IAEA,IAAI,OAAO,aAAA,IAAiB,KAAK;QAC/B,OAAO,0BAA0B,WAAW,MACzC,WAAA,CAAY,2BAA2B,KACvC,WAAA,CAAY;IACjB,OAAO;QACL,OAAO,SAAS,KAAK,wBAAwB,IAAI,WAAA,CAC/C;IAEJ;AACF;;AC5IO,SAAS,kBACd,EAAE,QAAA,EAAU,SAAA,EAAU,EACtBC,QAAAA;IAEA,MAAM,SAAS,IAAI,eAAe;QAChC,MAAM,OAAM,UAAA;YA/DhB,IAAA;YAgEM,MAAM,cAAc,IAAI;YAExB,MAAM,cAAc,CAAC;gBACnB,WAAW,OAAA,CACT,YAAY,MAAA,CAAOD,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,qBAAqB;YAE7D;YAEA,MAAM,kBAAkB,CAAC;gBACvB,WAAW,OAAA,CACT,YAAY,MAAA,CAAOA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,gBAAgB;YAExD;YAEA,MAAM,YAAY,CAAC;gBACjB,WAAW,OAAA,CACT,YAAY,MAAA,CAAOA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,SAAS;YAEjD;YAEA,MAAM,gBAAgB,OAAOE;gBApFnC,IAAAP,KAAA;gBAqFQ,IAAI,SAA0B,KAAA;gBAE9B,WAAA,MAAiB,SAASO,QAAQ;oBAChC,OAAQ,MAAM,KAAA;wBACZ,KAAK;4BAA0B;gCAC7B,WAAW,OAAA,CACT,YAAY,MAAA,CACVF,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,qBAAqB;oCACpC,IAAI,MAAM,IAAA,CAAK,EAAA;oCACf,MAAM;oCACN,SAAS;wCAAC;4CAAE,MAAM;4CAAQ,MAAM;gDAAE,OAAO;4CAAG;wCAAE;qCAAC;gCACjD;gCAGJ;4BACF;wBAEA,KAAK;4BAAwB;gCAC3B,MAAM,UAAA,CAAUL,MAAA,MAAM,IAAA,CAAK,KAAA,CAAM,OAAA,KAAjB,OAAA,KAAA,IAAAA,GAAAA,CAA2B,EAAA;gCAE3C,IAAA,CAAI,WAAA,OAAA,KAAA,IAAA,QAAS,IAAA,MAAS,UAAA,CAAA,CAAU,KAAA,QAAQ,IAAA,KAAR,OAAA,KAAA,IAAA,GAAc,KAAA,KAAS,MAAM;oCAC3D,WAAW,OAAA,CACT,YAAY,MAAA,CACVK,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ,QAAQ,IAAA,CAAK,KAAK;gCAGjD;gCAEA;4BACF;wBAEA,KAAK;wBACL,KAAK;4BAA8B;gCACjC,SAAS,MAAM,IAAA;gCACf;4BACF;oBACF;gBACF;gBAEA,OAAO;YACT;YAGA,WAAW,OAAA,CACT,YAAY,MAAA,CACVA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,0BAA0B;gBACzC;gBACA;YACF;YAIJ,IAAI;gBACF,MAAMC,SAAQ;oBACZ;oBACA;oBACA;oBACA;oBACA;gBACF;YACF,EAAA,OAAS,OAAO;gBACd,UAAA,CAAW,KAAA,MAAc,OAAA,KAAd,OAAA,KAAyB,CAAA,EAAG,MAAK,CAAE;YAChD,SAAE;gBACA,WAAW,KAAA;YACb;QACF;QACA,MAAK,UAAA,GAAa;QAClB,WAAU;IACZ;IAEA,OAAO,IAAI,SAAS,QAAQ;QAC1B,QAAQ;QACR,SAAS;YACP,gBAAgB;QAClB;IACF;AACF;AAKO,IAAM,iCAAiC;;ACzJ9C,gBAAgB,gBACd,QAAA,EACA,yBAAA;IAfF,IAAA,IAAA;IAiBE,MAAM,UAAU,IAAI;IACpB,WAAA,MAAiB,SAAA,CAAS,KAAA,SAAS,IAAA,KAAT,OAAA,KAAiB,EAAC,CAAG;QAC7C,MAAM,QAAA,CAAQ,KAAA,MAAM,KAAA,KAAN,OAAA,KAAA,IAAA,GAAa,KAAA;QAE3B,IAAI,SAAS,MAAM;YACjB,MAAM,YAAY,QAAQ,MAAA,CAAO;YACjC,MAAM,YAAY,KAAK,KAAA,CAAM;YAC7B,MAAM,QAAQ,0BAA0B;YAExC,IAAI,SAAS,MAAM;gBACjB,MAAM;YACR;QACF;IACF;AACF;AAEO,SAAS,kCACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA;QArC/C,IAAA;QAqCwD,OAAA,CAAA,KAAA,MAAM,KAAA,KAAN,OAAA,KAAA,IAAA,GAAa,IAAA;IAAA;AACrE;AAEO,SAAS,0BACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA,QAAS,MAAM,UAAU;AACxE;AAEO,SAAS,uBACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA,QAAS,SAAA,OAAA,KAAA,IAAA,MAAO,IAAI;AACnE;AAEO,SAAS,uBACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA,QAAS,MAAM,UAAU;AACxE;AAEO,SAAS,iBACd,QAAA,EACA,SAAA,EACA,yBAAA;IAEA,OAAO,0BACL,gBAAgB,UAAU,4BAEzB,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;;AChEA,IAAM,cAAc,IAAI,YAAY;AAepC,eAAe,aACb,KAAA,EACA,UAAA;IAEA,KAAA,MAAW,QAAQ,MAAO;QACxB,MAAM,EAAE,IAAA,EAAM,WAAA,EAAY,GAAI,KAAK,KAAA,CAAM;QAGzC,IAAI,CAAC,aAAa;YAChB,WAAW,OAAA,CAAQ;QACrB;IACF;AACF;AAEA,eAAe,oBACb,MAAA,EACA,UAAA;IAEA,IAAI,UAAU;IAEd,MAAO,KAAM;QACX,MAAM,EAAE,OAAO,KAAA,EAAO,IAAA,EAAK,GAAI,MAAM,OAAO,IAAA;QAC5C,IAAI,MAAM;YACR;QACF;QAEA,WAAW,YAAY,MAAA,CAAO,OAAO;YAAE,QAAQ;QAAK;QAEpD,MAAM,aAAa,QAAQ,KAAA,CAAM;QACjC,UAAU,WAAW,GAAA,MAAS;QAE9B,MAAM,aAAa,YAAY;IACjC;IAEA,IAAI,SAAS;QACX,MAAM,aAAa;YAAC;SAAO;QAC3B,MAAM,aAAa,YAAY;IACjC;IAEA,WAAW,KAAA;AACb;AAEA,SAASE,cAAa,GAAA;IAhEtB,IAAA;IAiEE,MAAM,SAAA,CAAS,KAAA,IAAI,IAAA,KAAJ,OAAA,KAAA,IAAA,GAAU,SAAA;IAEzB,OAAO,IAAI,eAAuB;QAChC,MAAM,OAAM,UAAA;YACV,IAAI,CAAC,QAAQ;gBACX,WAAW,KAAA;gBACX;YACF;YAEA,MAAM,oBAAoB,QAAQ;QACpC;IACF;AACF;AAEA,gBAAgBC,YAAW,MAAA;IACzB,WAAA,MAAiB,SAAS,OAAQ;QAChC,IAAI,MAAM,SAAA,KAAc,mBAAmB;YACzC,MAAM,OAAO,MAAM,IAAA;YACnB,IAAI,MAAM,MAAM;QAClB;IACF;AACF;AAEO,SAAS,aACd,MAAA,EACA,SAAA;IAEA,IAAI,OAAO,aAAA,IAAiB,QAAQ;QAClC,OAAO,0BAA0BA,YAAW,SACzC,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;IACjB,OAAO;QACL,OAAOD,cAAa,QACjB,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;IACjB;AACF;;ACrEA,gBAAgBC,YAAW,QAAA;IAhC3B,IAAA,IAAA,IAAA;IAmCE,WAAA,MAAiB,SAAS,SAAS,MAAA,CAAQ;QACzC,MAAM,QAAA,CAAQ,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,OAAA,KAAvB,OAAA,KAAA,IAAA,GAAgC,KAAA;QAE9C,IAAI,UAAU,KAAA,GAAW;YACvB;QACF;QAEA,MAAM,YAAY,KAAA,CAAM,EAAC;QAEzB,IAAI,OAAO,UAAU,IAAA,KAAS,UAAU;YACtC,MAAM,UAAU,IAAA;QAClB;IACF;AACF;AAKO,SAAS,yBACd,QAAA,EAGA,EAAA;IAEA,OAAO,0BAA0BA,YAAW,WACzC,WAAA,CAAY,2BAA2B,KACvC,WAAA,CAAY;AACjB;;ACvDA,SAASD,cAAa,GAAA;IACpB,MAAM,oBAAoB;IAC1B,OAAO,IAAI,eAAuB;QAChC,MAAM,MAAK,UAAA;YAVf,IAAA,IAAA;YAWM,MAAM,EAAE,KAAA,EAAO,IAAA,EAAK,GAAI,MAAM,IAAI,IAAA;YAElC,IAAI,MAAM;gBACR,WAAW,KAAA;gBACX;YACF;YAEA,MAAM,OAAO,kBAAA,CAAkB,KAAA,CAAA,KAAA,MAAM,KAAA,KAAN,OAAA,KAAA,IAAA,GAAa,IAAA,KAAb,OAAA,KAAqB;YACpD,IAAI,CAAC,MAAM;YAGX,IAAI,MAAM,cAAA,IAAkB,QAAQ,MAAM,cAAA,CAAe,MAAA,GAAS,GAAG;gBACnE;YACF;YAKA,IAAI,SAAS,UAAU,SAAS,mBAAmB,SAAS,WAAW;gBACrE;YACF;YAEA,WAAW,OAAA,CAAQ;QACrB;IACF;AACF;AAEO,SAAS,kBACd,GAAA,EACA,SAAA;IAEA,OAAOA,cAAa,KACjB,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;;ACnBO,SAAS,aACd,GAAA,EACA,SAAA;IAEA,IAAI,CAAC,IAAI,IAAA,EAAM;QACb,MAAM,IAAI,MAAM;IAClB;IAEA,IAAI,kBAAkB;IACtB,IAAI;IAEJ,MAAM,oBAAoC,CAAC,MAAc;QArC3D,IAAA,IAAA;QAsCI,MAAM,EAAE,KAAA,EAAM,GAAI;QAElB,IAAI,UAAU,iBAAiB;YAC7B,gBAAgB,KAAK,KAAA,CAAM;YAC3B,CAAA,KAAA,aAAA,OAAA,KAAA,IAAA,UAAW,cAAA,KAAX,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,WAA4B;QAC9B;QAEA,IAAI,UAAU,iBAAiB;YAC7B,MAAM,qBAAqB,KAAK,KAAA,CAAM;YACtC,kBAAA,CAAkB,KAAA,mBAAmB,eAAA,KAAnB,OAAA,KAAsC;YACxD,OAAO,mBAAmB,aAAA;QAC5B;QACA;IACF;IAEA,IAAI,EAAE,cAAA,EAAgB,GAAG,sBAAqB,GAAI,aAAa,CAAC;IAGhE,uBAAuB;QACrB,GAAG,oBAAA;QACH,SAAS,CAAA;YA1Db,IAAA;YA2DM,MAAM,wBAA+C;gBACnD;gBACA;YACF;YACA,CAAA,KAAA,aAAA,OAAA,KAAA,IAAA,UAAW,OAAA,KAAX,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,WAAqB,YAAY;QACnC;IACF;IAEA,OAAO,SAAS,KAAK,mBAAmB,sBAAsB,WAAA,CAC5D;AAEJ;;ACtEA,IAAA,4BAAA,CAAA;AAAA,SAAA,2BAAA;IAAA,YAAA,IAAA;AAAA;AA8CO,SAAS,WACd,MAAA,EACA,SAAA;IAEA,OAAO,OACJ,WAAA,CACC,IAAI,gBAAkD;QACpD,WAAW,OAAO,OAAO;YACvB,IAAI,OAAO,UAAU,UAAU;gBAC7B,WAAW,OAAA,CAAQ;YACrB,OAAA,IAAW,OAAO,MAAM,OAAA,KAAY,UAAU;gBAC5C,WAAW,OAAA,CAAQ,MAAM,OAAO;YAClC,OAAO;gBACL,MAAM,UAA4C,MAAM,OAAA;gBACxD,KAAA,MAAW,QAAQ,QAAS;oBAC1B,IAAI,KAAK,IAAA,KAAS,QAAQ;wBACxB,WAAW,OAAA,CAAQ,KAAK,IAAI;oBAC9B;gBACF;YACF;QACF;IACF,IAED,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;;AC9DO,SAAS,gBAAgB,SAAA;IAC9B,MAAM,SAAS,IAAI;IACnB,MAAM,SAAS,OAAO,QAAA,CAAS,SAAA;IAE/B,MAAM,OAAO,aAAA,GAAA,IAAI;IAEjB,MAAM,cAAc,OAAO,GAAU;QACnC,KAAK,MAAA,CAAO;QACZ,MAAM,OAAO,KAAA;QACb,MAAM,OAAO,KAAA,CAAM;IACrB;IAEA,MAAM,cAAc,OAAO;QACzB,KAAK,GAAA,CAAI;IACX;IAEA,MAAM,YAAY,OAAO;QACvB,KAAK,MAAA,CAAO;QAEZ,IAAI,KAAK,IAAA,KAAS,GAAG;YACnB,MAAM,OAAO,KAAA;YACb,MAAM,OAAO,KAAA;QACf;IACF;IAEA,OAAO;QACL,QAAQ,OAAO,QAAA,CACZ,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;QACf;QACA,UAAU;YACR,mBAAmB,OAAO;gBACxB,MAAM,OAAO,KAAA;gBACb,MAAM,OAAO,KAAA,CAAM;YACrB;YACA,gBAAgB,OAAO,MAAW,UAAoB;gBACpD,YAAY;YACd;YACA,cAAc,OAAO,SAAc;gBACjC,MAAM,UAAU;YAClB;YACA,gBAAgB,OAAO,GAAU;gBAC/B,MAAM,YAAY,GAAG;YACvB;YACA,kBAAkB,OAAO,QAAa,SAAc;gBAClD,YAAY;YACd;YACA,gBAAgB,OAAO,UAAe;gBACpC,MAAM,UAAU;YAClB;YACA,kBAAkB,OAAO,GAAU;gBACjC,MAAM,YAAY,GAAG;YACvB;YACA,iBAAiB,OAAO,OAAY,QAAgB;gBAClD,YAAY;YACd;YACA,eAAe,OAAO,SAAiB;gBACrC,MAAM,UAAU;YAClB;YACA,iBAAiB,OAAO,GAAU;gBAChC,MAAM,YAAY,GAAG;YACvB;QACF;IACF;AACF;;ACrCA,gBAAgBC,YAAW,MAAA;IApC3B,IAAA,IAAA;IAqCE,WAAA,MAAiB,SAAS,OAAQ;QAChC,MAAM,UAAA,CAAU,KAAA,CAAA,KAAA,MAAM,OAAA,CAAQ,EAAC,KAAf,OAAA,KAAA,IAAA,GAAkB,KAAA,KAAlB,OAAA,KAAA,IAAA,GAAyB,OAAA;QAEzC,IAAI,YAAY,KAAA,KAAa,YAAY,IAAI;YAC3C;QACF;QAEA,MAAM;IACR;AACF;AAKO,SAAS,cACd,QAAA,EACA,SAAA;IAEA,MAAM,SAAS,0BAA0BA,YAAW;IACpD,OAAO,OACJ,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;;ACkNA,SAAS;IAGP,MAAM,UAAU;IAChB,OAAO,CAAA,OAAQ,QAAQ,KAAK,KAAA,CAAM;AACpC;AAOA,gBAAgBA,YAAW,MAAA;IACzB,MAAM,UAAU;IAEhB,WAAA,IAAe,SAAS,OAAQ;QAG9B,IAAI,yBAAyB,OAAO;YAClC,QAAQ;gBACN,IAAI,MAAM,EAAA;gBACV,SAAS,MAAM,OAAA,CAAQ,OAAA;gBACvB,QAAS,MAAc,MAAA;gBAAA,2BAAA;gBACvB,OAAQ,MAAc,KAAA;gBAAA,2BAAA;gBACtB,SAAS,MAAM,OAAA,CAAQ,GAAA,CAAI,CAAA;oBArSnC,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;oBAqS8C,OAAA;wBACpC,OAAO;4BACL,SAAA,CAAS,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,OAAA;4BACvB,eAAA,CAAe,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,YAAA;4BAC7B,MAAA,CAAM,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,IAAA;4BACpB,YAAA,CAAA,CAAY,KAAA,CAAA,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,SAAA,KAAd,OAAA,KAAA,IAAA,GAAyB,MAAA,IAAA,CACjC,KAAA,CAAA,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,SAAA,KAAd,OAAA,KAAA,IAAA,GAAyB,GAAA,CAAI,CAAC,UAAU,QAAA,CAAW;oCACjD;oCACA,IAAI,SAAS,EAAA;oCACb,UAAU,SAAS,QAAA;oCACnB,MAAM,SAAS,IAAA;gCACjB,CAAA,KACA,KAAA;wBACN;wBACA,eAAe,OAAO,YAAA;wBACtB,OAAO,OAAO,KAAA;oBAChB;gBAAA;YACF;QACF;QAEA,MAAM,OAAO,QAAQ;QAErB,IAAI,MAAM,MAAM;IAClB;AACF;AAEA,SAAS;IAGP,MAAM,oBAAoB;IAC1B,IAAI;IACJ,OAAO,CAAA;QApUT,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;QAqUI,IAAI,sBAAsB,OAAO;YAC/B,MAAM,QAAA,CAAQ,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,KAAA;YAC/B,IAAA,CAAI,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,IAAA,EAAM;gBAC7B,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS,CAAA,4BAAA,EAA+B,MAAM,aAAA,CAAc,IAAI,CAAA,iBAAA,CAAA;gBAClE;YACF,OAAA,IAAA,CAAW,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,IAAA,EAAM;gBAChD,wBAAwB;gBACxB,MAAM,WAAW,MAAM,UAAA,CAAW,EAAC;gBACnC,IAAI,SAAS,KAAA,KAAU,GAAG;oBACxB,OAAO;wBACL,QAAQ;wBACR,SAAS,CAAA,wBAAA,EAA2B,SAAS,EAAE,CAAA,6CAAA,EAAA,CAAgD,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAI,CAAA,iBAAA,CAAA;oBACxH;gBACF,OAAO;oBACL,OAAO;wBACL,QAAQ;wBACR,SAAS,CAAA,aAAA,EAAgB,SAAS,EAAE,CAAA,6CAAA,EAAA,CAAgD,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAI,CAAA,iBAAA,CAAA;oBAC7G;gBACF;YACF,OAAA,IAAA,CAAW,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,SAAA,EAAW;gBACzC,OAAO;oBACL,QAAQ;oBACR,SAAS,iBAAA,CAAiB,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,SAAS;gBAC1D;YACF,OAAA,IAAA,CAAW,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,SAAA,EAAW;gBACrD,OAAO;oBACL,QAAQ;oBACR,SAAS,iBAAA,CAAiB,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,SAAS;gBACtE;YACF,OAAA,IACE,yBAAA,CAAA,CAAA,CACC,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,mBAAA,CAAA,CAClC,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,MAAA,GACrC;gBACA,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS;gBACX;YACF,OAAA,IACE,yBAAA,CAAA,CACA,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,cACnC;gBACA,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS;gBACX;YACF;QACF;QAEA,MAAM,OAAO,kBACX,sBAAsB,SAAS,KAAK,OAAA,CAAQ,EAAC,CAAE,KAAA,CAAM,OAAA,GACjD,KAAK,OAAA,CAAQ,EAAC,CAAE,KAAA,CAAM,OAAA,GACtB,aAAa,QACb,KAAK,OAAA,CAAQ,EAAC,CAAE,IAAA,GAChB;QAGN,OAAO;IACT;IAEA,SAAS,iBAAiB,aAAA;QACxB,IAAI,qBAAqB,cACtB,OAAA,CAAQ,OAAO,QACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,MAAM,OACd,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO;QAElB,OAAO,CAAA,EAAG,mBAAkB,CAAA;IAC9B;AACF;AAEA,IAAM,qCAAqC,OACzC;AAaF,SAAS,sBACP,IAAA;IAEA,OACE,aAAa,QACb,KAAK,OAAA,IACL,KAAK,OAAA,CAAQ,EAAC,IACd,WAAW,KAAK,OAAA,CAAQ,EAAC;AAE7B;AAEA,SAAS,aAAa,IAAA;IACpB,OACE,aAAa,QACb,KAAK,OAAA,IACL,KAAK,OAAA,CAAQ,EAAC,IACd,UAAU,KAAK,OAAA,CAAQ,EAAC;AAE5B;AAKO,SAAS,aACd,GAAA,EACA,SAAA;IAGA,MAAM,KAIG;IAET,IAAI;IACJ,IAAI,OAAO,aAAA,IAAiB,KAAK;QAC/B,SAAS,0BAA0BA,YAAW,MAAM,WAAA,CAClD,2BAAA,CACE,MAAA,OAAA,KAAA,IAAA,GAAI,2BAAA,KAAA,CAA+B,MAAA,OAAA,KAAA,IAAA,GAAI,uBAAA,IACnC;YACE,GAAG,EAAA;YACH,SAAS,KAAA;QACX,IACA;YACE,GAAG,EAAA;QACL;IAGV,OAAO;QACL,SAAS,SACP,KACA,qBAAkB,CAClB,MAAA,OAAA,KAAA,IAAA,GAAI,2BAAA,KAAA,CAA+B,MAAA,OAAA,KAAA,IAAA,GAAI,uBAAA,IACnC;YACE,GAAG,EAAA;YACH,SAAS,KAAA;QACX,IACA;YACE,GAAG,EAAA;QACL;IAER;IAEA,IAAI,MAAA,CAAO,GAAG,2BAAA,IAA+B,GAAG,uBAAA,GAA0B;QACxE,MAAM,0BAA0B,8BAA8B;QAC9D,OAAO,OAAO,WAAA,CAAY;IAC5B,OAAO;QACL,OAAO,OAAO,WAAA,CAAY;IAC5B;AACF;AAEA,SAAS,8BACP,SAAA;IAIA,MAAM,cAAc,IAAI;IACxB,IAAI,eAAe;IACnB,IAAI,qBAAqB;IACzB,IAAI,oCAAoC;IACxC,IAAI,wBAAwB;IAE5B,IAAI,uBACF,SAAA,CAAU,mCAAkC,IAAK,EAAC;IAEpD,MAAM,SAAS,CAAA,GAAA,4KAAA,CAAA,qBAAA;IAEf,OAAO,IAAI,gBAAgB;QACzB,MAAM,WAAU,KAAA,EAAO,UAAA;YACrB,MAAM,UAAU,OAAO;YACvB,qCAAqC;YAErC,MAAM,yBACJ,gBAAA,CACC,QAAQ,UAAA,CAAW,wBAClB,QAAQ,UAAA,CAAW,iBAAgB;YAEvC,IAAI,wBAAwB;gBAC1B,wBAAwB;gBACxB,sBAAsB;gBACtB,eAAe;gBACf;YACF;YAGA,IAAI,CAAC,uBAAuB;gBAC1B,WAAW,OAAA,CACT,YAAY,MAAA,CAAOJ,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;gBAE9C;YACF,OAAO;gBACL,sBAAsB;YACxB;QACF;QACA,MAAM,OAAM,UAAA;YACV,IAAI;gBACF,IACE,CAAC,gBACD,yBAAA,CACC,UAAU,2BAAA,IACT,UAAU,uBAAA,GACZ;oBACA,wBAAwB;oBACxB,MAAM,UAAU,KAAK,KAAA,CAAM;oBAE3B,IAAI,0BAA2C;2BAC1C;qBACL;oBAEA,IAAI,mBAMY,KAAA;oBAEhB,IAAI,UAAU,2BAAA,EAA6B;wBAIzC,IAAI,QAAQ,aAAA,KAAkB,KAAA,GAAW;4BACvC,QAAQ,IAAA,CACN;wBAEJ;wBAEA,MAAM,mBAAmB,KAAK,KAAA,CAC5B,QAAQ,aAAA,CAAc,SAAA;wBAGxB,mBAAmB,MAAM,UAAU,2BAAA,CACjC;4BACE,MAAM,QAAQ,aAAA,CAAc,IAAA;4BAC5B,WAAW;wBACb,GACA,CAAA;4BAEE,0BAA0B;mCACrB;gCACH;oCACE,MAAM;oCACN,SAAS;oCACT,eAAe,QAAQ,aAAA;gCACzB;gCACA;oCACE,MAAM;oCACN,MAAM,QAAQ,aAAA,CAAc,IAAA;oCAC5B,SAAS,KAAK,SAAA,CAAU;gCAC1B;6BACF;4BAEA,OAAO;wBACT;oBAEJ;oBACA,IAAI,UAAU,uBAAA,EAAyB;wBACrC,MAAM,YAA6B;4BACjC,OAAO,EAAC;wBACV;wBACA,KAAA,MAAWP,SAAQ,QAAQ,UAAA,CAAY;4BACrC,UAAU,KAAA,CAAM,IAAA,CAAK;gCACnB,IAAIA,MAAK,EAAA;gCACT,MAAM;gCACN,MAAM;oCACJ,MAAMA,MAAK,QAAA,CAAS,IAAA;oCACpB,WAAW,KAAK,KAAA,CAAMA,MAAK,QAAA,CAAS,SAAS;gCAC/C;4BACF;wBACF;wBACA,IAAI,gBAAgB;wBACpB,IAAI;4BACF,mBAAmB,MAAM,UAAU,uBAAA,CACjC,WACA,CAAA;gCACE,IAAI,QAAQ;oCACV,MAAM,EAAE,YAAA,EAAc,aAAA,EAAe,gBAAA,EAAiB,GACpD;oCAEF,0BAA0B;2CACrB;wCAAA,+DAAA;2CAEC,kBAAkB,IAClB;4CACE;gDACE,MAAM;gDACN,SAAS;gDACT,YAAY,QAAQ,UAAA,CAAW,GAAA,CAC7B,CAAC,KAAA,CAAkB;wDACjB,IAAI,GAAG,EAAA;wDACP,MAAM;wDACN,UAAU;4DACR,MAAM,GAAG,QAAA,CAAS,IAAA;4DAAA,wGAAA;4DAElB,WAAW,KAAK,SAAA,CACd,GAAG,QAAA,CAAS,SAAA;wDAEhB;oDACF,CAAA;4CAEJ;yCACF,GACA,EAAC;wCAAA,0CAAA;wCAEL;4CACE,MAAM;4CACN;4CACA,MAAM;4CACN,SAAS,KAAK,SAAA,CAAU;wCAC1B;qCACF;oCACA;gCACF;gCAEA,OAAO;4BACT;wBAEJ,EAAA,OAAS,GAAG;4BACV,QAAQ,KAAA,CAAM,0CAA0C;wBAC1D;oBACF;oBAEA,IAAI,CAAC,kBAAkB;wBAIrB,WAAW,OAAA,CACT,YAAY,MAAA,CACVO,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EACE,QAAQ,aAAA,GAAgB,kBAAkB,cAAA,oCAAA;wBAE1C,KAAK,KAAA,CAAM;wBAIjB;oBACF,OAAA,IAAW,OAAO,qBAAqB,UAAU;wBAE/C,WAAW,OAAA,CACT,YAAY,MAAA,CAAOA,CAAAA,GAAAA,4KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;wBAE9C,oCAAoC;wBACpC;oBACF;oBAOA,MAAM,oBAA2C;wBAC/C,GAAG,SAAA;wBACH,SAAS,KAAA;oBACX;oBAEA,UAAU,OAAA,GAAU,KAAA;oBAEpB,MAAM,eAAe,aAAa,kBAAkB;wBAClD,GAAG,iBAAA;wBACH,CAAC,mCAAkC,EAAG;oBACxC;oBAEA,MAAM,SAAS,aAAa,SAAA;oBAE5B,MAAO,KAAM;wBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;wBACrC,IAAI,MAAM;4BACR;wBACF;wBACA,WAAW,OAAA,CAAQ;oBACrB;gBACF;YACF,SAAE;gBACA,IAAI,UAAU,OAAA,IAAW,mCAAmC;oBAC1D,MAAM,UAAU,OAAA,CAAQ;gBAC1B;YACF;QACF;IACF;AACF;;AC9pBA,eAAsB,gBACpB,GAAA,EACA,EAAA,EACA,OAAA;IAlDF,IAAA;IAsDE,MAAM,MAAA,CAAM,KAAA,IAAI,IAAA,KAAJ,OAAA,KAAA,IAAA,GAAU,MAAA;IAEtB,IAAI,CAAC,KAAK;QACR,IAAI,IAAI,KAAA,EAAO,MAAM,IAAI,MAAM,IAAI,KAAK;aACnC,MAAM,IAAI,MAAM;IACvB;IAEA,MAAM,cAAc,MAAM,MAAM,KAAK;QACnC,QAAQ;QACR,SAAS;YACP,QAAQ;YACR,GAAG,WAAA,OAAA,KAAA,IAAA,QAAS,OAAA;QACd;IACF;IAEA,OAAO,SAAS,aAAa,KAAA,GAAW,IAAI,WAAA,CAC1C;AAEJ;;AC3DO,SAAS,aACd,OAAA,EACA,OAAA;IAEA,MAAM,UAAU,QAAQ,SAAA;IACxB,MAAM,UAAU,QAAQ,SAAA;IAExB,IAAI,YACF,KAAA;IACF,IAAI,YACF,KAAA;IAEF,IAAI,cAAc;IAClB,IAAI,cAAc;IAGlB,eAAe,YACb,UAAA;QAEA,IAAI;YACF,IAAI,aAAa,MAAM;gBACrB,YAAY,QAAQ,IAAA;YACtB;YAEA,MAAM,SAAS,MAAM;YACrB,YAAY,KAAA;YAEZ,IAAI,CAAC,OAAO,IAAA,EAAM;gBAChB,WAAW,OAAA,CAAQ,OAAO,KAAK;YACjC,OAAO;gBACL,WAAW,KAAA;YACb;QACF,EAAA,OAAS,OAAO;YACd,WAAW,KAAA,CAAM;QACnB;IACF;IAGA,eAAe,YACb,UAAA;QAEA,IAAI;YACF,IAAI,aAAa,MAAM;gBACrB,YAAY,QAAQ,IAAA;YACtB;YAEA,MAAM,SAAS,MAAM;YACrB,YAAY,KAAA;YAEZ,IAAI,CAAC,OAAO,IAAA,EAAM;gBAChB,WAAW,OAAA,CAAQ,OAAO,KAAK;YACjC,OAAO;gBACL,WAAW,KAAA;YACb;QACF,EAAA,OAAS,OAAO;YACd,WAAW,KAAA,CAAM;QACnB;IACF;IAEA,OAAO,IAAI,eAAgC;QACzC,MAAM,MAAK,UAAA;YACT,IAAI;gBAEF,IAAI,aAAa;oBACf,YAAY;oBACZ;gBACF;gBAGA,IAAI,aAAa;oBACf,YAAY;oBACZ;gBACF;gBAGA,IAAI,aAAa,MAAM;oBACrB,YAAY,QAAQ,IAAA;gBACtB;gBACA,IAAI,aAAa,MAAM;oBACrB,YAAY,QAAQ,IAAA;gBACtB;gBAKA,MAAM,EAAE,MAAA,EAAQ,MAAA,EAAO,GAAI,MAAM,QAAQ,IAAA,CAAK;oBAC5C,UAAU,IAAA,CAAK,CAAAH,UAAAA,CAAW;4BAAE,QAAAA;4BAAQ,QAAQ;wBAAQ,CAAA;oBACpD,UAAU,IAAA,CAAK,CAAAA,UAAAA,CAAW;4BAAE,QAAAA;4BAAQ,QAAQ;wBAAQ,CAAA;iBACrD;gBAED,IAAI,CAAC,OAAO,IAAA,EAAM;oBAChB,WAAW,OAAA,CAAQ,OAAO,KAAK;gBACjC;gBAEA,IAAI,WAAW,SAAS;oBACtB,YAAY,KAAA;oBACZ,IAAI,OAAO,IAAA,EAAM;wBAEf,YAAY;wBACZ,cAAc;oBAChB;gBACF,OAAO;oBACL,YAAY,KAAA;oBAEZ,IAAI,OAAO,IAAA,EAAM;wBACf,cAAc;wBACd,YAAY;oBACd;gBACF;YACF,EAAA,OAAS,OAAO;gBACd,WAAW,KAAA,CAAM;YACnB;QACF;QACA;YACE,QAAQ,MAAA;YACR,QAAQ,MAAA;QACV;IACF;AACF;;AC5HO,SAAS,iBACd,GAAA,EACA,QAAA,EACA,IAAA,EACA,IAAA;IAXF,IAAA;IAaE,SAAS,SAAA,CAAA,CAAU,KAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAA,KAAgB,KAAK;QACtC,gBAAgB;QAChB,GAAG,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA;IACX;IAEA,IAAI,kBAAkB;IAEtB,IAAI,MAAM;QACR,kBAAkB,aAAa,KAAK,MAAA,EAAQ;IAC9C;IAEA,MAAM,SAAS,gBAAgB,SAAA;IAC/B,SAAS;QACP,OAAO,IAAA,GAAO,IAAA,CAAK,CAAC,EAAE,IAAA,EAAM,KAAA,EAAM;YAChC,IAAI,MAAM;gBACR,SAAS,GAAA;gBACT;YACF;YACA,SAAS,KAAA,CAAM;YACf;QACF;IACF;IACA;AACF;;AC7BO,IAAM,wBAAN,cAAoC;IACzC,YAAY,GAAA,EAAqB,IAAA,EAAqB,IAAA,CAAmB;QACvE,IAAI,kBAAkB;QAEtB,IAAI,MAAM;YACR,kBAAkB,aAAa,KAAK,MAAA,EAAQ;QAC9C;QAEA,KAAA,CAAM,iBAAwB;YAC5B,GAAG,IAAA;YACH,QAAQ;YACR,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;;ArDQO,IAAMQ,cAAa,wLAAA,CAAA,aAAA;AAMnB,IAAM,SAAS,wLAAA,CAAA,aAAA"}},
    {"offset": {"line": 5050, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}},
    {"offset": {"line": 5054, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/.next-internal/server/app/search/[id]/page/actions.js"],"sourcesContent":["__turbopack_export_value__({\n  '06ff66d2cf5009eb575f572135e256000e6d69d7': (...args) => Promise.resolve(require('ACTIONS_MODULE0')).then(mod => (0, mod['getChat'])(...args)),\n  '26ff88276f1422ccb7d07d1b5b4a23d7ddeb872e': (...args) => Promise.resolve(require('ACTIONS_MODULE0')).then(mod => (0, mod['shareChat'])(...args)),\n  '498a2835364340fad66e4902c390c4ea01ad1bd8': (...args) => Promise.resolve(require('ACTIONS_MODULE0')).then(mod => (0, mod['getChats'])(...args)),\n  '5a7522b313dd544f8b930e49b6dba159bab50d07': (...args) => Promise.resolve(require('ACTIONS_MODULE1')).then(mod => (0, mod['$$ACTION_0'])(...args)),\n  '7a8dc6bee7cd0b9e920403b38265c786fd2e15cd': (...args) => Promise.resolve(require('ACTIONS_MODULE0')).then(mod => (0, mod['getSharedChat'])(...args)),\n  '833db4948b1b74ab764aeed32d37f096cf6b5498': (...args) => Promise.resolve(require('ACTIONS_MODULE0')).then(mod => (0, mod['clearChats'])(...args)),\n  'd6fec9fa00168e3bc03dd53f61a0be98da52d105': (...args) => Promise.resolve(require('ACTIONS_MODULE2')).then(mod => (0, mod['$$ACTION_1'])(...args)),\n  'daab9428748002bb5d7cf7dc3684e4b0f08ef85e': (...args) => Promise.resolve(require('ACTIONS_MODULE0')).then(mod => (0, mod['saveChat'])(...args)),\n  'ee0ea4b2c3481a96aef88a72b0e4b5e7f8e224a4': (...args) => Promise.resolve(require('ACTIONS_MODULE2')).then(mod => (0, mod['$$ACTION_2'])(...args)),\n  'fcb1bed4c9ae3c596bdc26846d7db0b90c54e58e': (...args) => Promise.resolve(require('ACTIONS_MODULE2')).then(mod => (0, mod['$$ACTION_0'])(...args)),\n});"],"names":[],"mappings":"AAAA,2BAA2B;IACzB,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,gFAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,UAAU,KAAK;IACxI,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,gFAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,YAAY,KAAK;IAC1I,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,gFAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,WAAW,KAAK;IACzI,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,oGAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,aAAa,KAAK;IAC3I,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,gFAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,gBAAgB,KAAK;IAC9I,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,gFAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,aAAa,KAAK;IAC3I,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,4EAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,aAAa,KAAK;IAC3I,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,gFAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,WAAW,KAAK;IACzI,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,4EAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,aAAa,KAAK;IAC3I,4CAA4C,CAAC,GAAG,OAAS,QAAQ,OAAO,4EAA6B,IAAI,CAAC,CAAA,MAAO,CAAC,GAAG,GAAG,CAAC,aAAa,KAAK;AAC7I"}},
    {"offset": {"line": 5066, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}}]
}